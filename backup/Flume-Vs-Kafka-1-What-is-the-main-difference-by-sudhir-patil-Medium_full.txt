Flume Vs Kafka. 1> What is the main difference? | by sudhir patil | MediumGet unlimited accessOpen in appHomeNotificationsListsStoriesWritesudhir patilFollowJan 12, 2018·2 min readFlume Vs Kafka1> What is the main difference?Use Kafka when you require messaging system, to send messages to multiple channels. But you have to write your own publishers and consumers. It’s alternative for ActiveMQ, Solace etcUse Flume when you require data ingestion and simple event processing framework. E.g EMS(could be Kafka) as source and process events and store it in HDFS. Flume provides built in collectors and sinks. So you can save to HDFS, HBASE etc without any coding.2> Push Or Pull and pro’s Cons?Kafka Pull Model, i.e Consumers pull data. Pro’s number of consumers doesn’t affect performance. Handles back pressure in case of slow consumers as each consumer events are persisted in durable disk. Each consumer has and manages it’s own read pointer. This allows a large number of consumers of each Kafka queue, that pull data at their own pace. With this, you could deliver your event streams to HBASE, Cassandra, Storm, Hadoop, RDBMS all in parallel.Flume is Push Model, i.e messages are pushed to consumer by Flume. Cons Back pressure is not handled well as flume pushes messages to consumers, channels give some buffer but consumer may still get flooded with messages.To get data out of Flume, you use a sink, which writes to your target store (HDFS, HBase, Cassandra etc). Flume will re-try connections to your sinks if they are offline. Because Flume pushes data, you have to do some interesting work to sink data to two data stores3 - Fault ToleranceKafka can be configured to replicate to multiple brokers, If you lose a broker node replicated node will start serving the data.With Flume & FlumeNG, and a File channel, if you loose a broker node you will lose access to those events until you recover that disk. The database channel with Flume is reported too slow for any production use cases at volume.4 - When to Use What?Flume’s main use-case is to ingest data into Hadoop. It is tightly integrated with Hadoop’s monitoring system, file system, file formats, and utilities such a Morphlines. A lot of the Flume development effort goes into maintaining compatibility with Hadoop. Sure, Flume’s design of sources, sinks and channels mean that it can be used to move data between other systems flexibly, but the important feature is its Hadoop integration.Kafka’s main use-case is a distributed publish-subscribe messaging system. Most of the development effort is involved with allowing subscribers to read exactly the messages they are interested in, and in making sure the distributed system is scalable and reliable under many different conditions. It was not written to stream data specifically for Hadoop, and using it to read and write data to Hadoop is significantly more challenging than it is in Flume.------More from sudhir patilFollowLove podcasts or audiobooks? Learn on the go with our new app.Try KnowableAboutHelpTermsPrivacyGet the Medium appGet startedsudhir patil3 FollowersFollowMore from MediumAll About CodeThe 5 Major APIs in Apache KafkaW SimpsonForm Fits Function: A Case Study on Integrating MongoDB and Redis with Apache SparkMahfooz AhamedIntroduction Apache KafkaMinoli de SilvaIntroduction to Data pipelines with Apache KafkaHelpStatusWritersBlogCareersPrivacyTermsAboutKnowable






































