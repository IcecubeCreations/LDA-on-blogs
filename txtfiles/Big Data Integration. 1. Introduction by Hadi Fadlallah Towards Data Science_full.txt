Big Data Integration. 1. Introduction | by Hadi Fadlallah | Towards Data ScienceGet unlimited accessOpen in appHomeNotificationsListsStoriesWritePublished inTowards Data ScienceHadi FadlallahFollowDec 6, 2018·16 min readBig Data Integration1. IntroductionData integration is a set of processes used to retrieve and combine data from disparate sources into meaningful and valuable information. A complete data integration solution delivers trusted data from a variety of sources [5]. Traditional data integration techniques was mainly based on ETL (extract, transform and Load) process to ingest and clean data then load it into a data warehouse.Nowadays, huge volume of data are collected from many heterogeneous data sources which are generating data in real-time with different qualities — which is called Big Data. The big data integration is very challenging especially after the traditional data integration techniques failed to handles it.We can say that big data integration differs from traditional data integration in many dimensions: Volume, Velocity, Variety and Veracity, which are the big data main characteristics:Volume: Which is the original attribute of big data. Nowadays the number of connected devices and peoples is higher than before, which highly influenced the number of data sources and the amount of data worldwide.Velocity: As the number of data sources increased, the rate of data generation over time highly increased, especially after the appearance of social media and the use of IOT.Variety: More data sources implies that we have more variety in the formats in which data are stored. We have structured and unstructured data in high level. In each type, we have a huge number of formats: text, images, sounds, xml, documents, spatial data and others.Veracity: The characteristics listed above, caused that we have different data quality, so we can find uncertain or imprecise data especially that social media and blogs allows users to spread this kind of data.In this article, we are trying to give an overview of the big data integration techniques and challenges, and to show some of the latest researches made in this domain.This article is mainly based on the amazing book “Big Data Integration” [2] written by X. L. Dong and D. Srivastava.2. Traditional Data integrationTo integrate data across mixed application environments, you need to get data from one data environment (source) to another data environment (destination). Extract, Transform and Load (ETL) technologies have been used to accomplish this in traditional data warehouse environments. [1]ETL tools combine three important functions required to get data from one data environment and put it into another data environment.Extract: Read data from the source database.Transform: Convert the format of the extracted data so that it conforms to the requirements of the target database. (Transformation is done by using rules or merging data with other data.)Load: Write data to the target databaseTraditionally, ETL has been used with batch processing (data on the rest) in data warehouse environments.Data warehouses provide business users with a way to consolidate information across disparate sources to analyze and report on data relevant to their specific business focus. ETL tools are used to transform the data into the format required by the data warehouse. The transformation is actually done in an intermediate location before the data is loaded into the data warehouse.Many software vendor including Oracle, Microsoft, IBM, Informatica, Talend, and Pentaho provided Traditional ETL software tools.Image Source: www.neiltortorella.com/hadoop-data-warehouse/impressive-hadoop-data-warehouse-5-traditional-data-warehouse-systems-data-integration-or-etl-systemsAfter the appearance of Big Data, the traditional data warehousing systems failed to handle it, which increase the need for improvement and to use more efficient and powerful technologies.3. Big Data IntegrationThe elements of the big data platform manage data in new ways as compared to the traditional relational database. Due to the need of scalability and high performance for managing both structured and unstructured data. All Components of the big data ecosystem from Hadoop to NoSQL Databases, each one of them have its own approach for extracting, transforming and loading data.In addition, the traditional ETL tools are evolving to handle the new big data characteristics. While traditional forms of integration take on new meanings in a big data world, the integration technologies need a common platform that supports data quality and profiling.As we mentioned in the previous section, traditional data integration was performed using batch processing (data on the rest), while big data integration can be done in Real-time or with batch processing. Which make the ETL phases reordered to become ELT in some cases, so the data is extracted, loaded into distributed file systems, and then transformed before being used.In goal of making good business decisions based on big data analysis, the data needs to be trusted and understood at all levels of the organization. It needs to be delivered to the business in a trusted, controlled, consistent, and flexible way across the enterprise. To accomplish this goal, three basic techniques are used:Schema MappingRecord LinkageData FusionSchema Mapping and record linkage was used in traditional data integration, but they was included in the ETL process, and they was not challenging as they are when integrating big data. In the next sections, we will describe each one of these techniques, and we will show what are the latest researches made to adopt this techniques to the big data integration.4. Schema Mapping4.1. Schema mapping basicsAt the initial stages of your big data analysis, you are not likely to have the same level of control over data definitions as you do with your operational data. However, once you have identified the patterns that are most relevant to your business, you need the capability to map data elements to a common definition. That common definition is then carried forward into operational data, data warehouses, reporting, and business processes.Schema Mapping can be considered as a process composed of two phases. First, creating a mediated (global) schema, then identifying the mappings between the mediated schema and the local schema of the data sources to determine which (sets of) attributes contain the same informations. [2]As example, consider that we are collecting data about “Cricket” players. [17]We have three data sources: S1, S2, and S3. Each data sources contains different attribute as shown in Table 1.First, we identify four attribute that we want in the mediated schema, which are the name, the number of games played, the Total score and the Team. Then we do the mapping between the mediated schema and the data sources schemes as shown in Table 2.After the mapping is done, we can query all data sources, as they are one logical data source. Figure. 3 shows an example of how can we collect data when searching for a player named “Allan Border”.4.2. Literature review4.2.1. Probabilistic Schema AlignmentTo handle the ambiguity of attributes meaning, adding probability to attributes matching and to schema mapping was proposed by Das Sarma and al. [19] and evaluated on web tables crawled from five domains, each domain contains about 50–800 web tables. In addition, it gives better results than deterministic method.4.2.2. Integrating Deep Web Data“Deep web” term is used to describe the web pages that are unreachable by search engines. In goal to offer access to deep web, an algorithm was proposed by Madhavan and al. [20] and tested on a sample of 500,000 HTML forms, and achieved a good coverage of the underlying database.4.2.3. Integrating Web Tables“Web Tables” are heterogeneous tabular form of data stored in HTML; it does not have a clear and precise schema. A Keyword search method on Web Tables was proposed by Cafarella and al. [21] based on two Ranking approaches Feature Rank and Schema Rank.Das Sarma and al. [22] proposed a framework to retrieve web tables that are related to a given table. They experimented the framework on Wikipedia tables and showed good results.In order to extract knowledge from Web Tables, Limaye and al. [23] proposed a graphical-model based solution to annotate Web Tables, and their experiments showed that graphical model obtains a higher accuracy than the traditional model.5. Record Linkage5.1. Record Linkage basicsRecord Linkage is a task in which we identify records that refer to the same logical entity across different data sources, especially when common identifier are not shared between multiple data sources (like the SSN for persons). In traditional data integration, it is working only on linking structured data.In Big data integration, data sources are heterogeneous in their structures and are collected from many sources (social media, sensor logs, …) which provides unstructured text data, and data sources are dynamic and continuously evolving [2 which make this technique very challenging.As a very simple example of what Record Linkage means, if we are collecting data about patients from different hospital. Assuming that we are collecting data about a patient named “Mohammad Hassan” and he was born in year 1953.We found two records from two hospital A and B as in Table 3 and 4.Comparing after that schema attribute are mapped, and some comparison we can find the probability of these rows are belonging to the same patient. Figure. 4 shows an assumption of comparison.Figure 4. Shows that first comparing both name give a probability of 50%, then comparing the year of the birth raise this probability to 80%. In addition, we can find from entry dates that the rows are not conflicting. Therefore, we can assume with a probability of 80% that these rows belong to the same patient.In Record linkage, many techniques are used:Pairwise matching: this technique is used for comparing a pair of records to check if they belong to the same logical entity.Clustering: This technique is used to reach a globally consistent decision of the appropriate records partitioning to make sure that each partition belong to a distinct entity.Blocking: This technique is used to partition the input records into multiple blocks in order to only allow pairwise matching to records in a same block.5.2. Literature review5.2.1. Using MapReduce to parallelize blocking5.2.1.1. MapReduce brief descriptionHadoop MapReduce [14] is a programming model for distributed computing, it is based on two important tasks Map and Reduce. As described by IBM analytics [25], Map task takes a set of data and converts it into another set of data, where individual elements are broken down into tuples (key/value pairs). Reduce task takes the output from a map as input and combines those data tuples into a smaller set of tuples. The reduce job is always performed after the map job.Mappers are the responsible of using Map() function and reducers are the responsible of Reduce().Image Source: www.tutorialspoint.com/hadoop/hadoop_mapreduce5.2.1.2. The proposalThe Record linkage basic approach using MapReduce is susceptible to serve load imbalance due to skewed block sizes, which decrease the performance.Kolb and al. [26] proposed two strategies (BlockSplit and PairRange) for load balancing among reducers. In addition, made experiments to evaluate various load-balancing strategies on real-world data sets, and finally made a comparison with the basic approach, which showed that the proposal has more robustness and efficiency.5.2.2. Meta-Blocking: pruning pairwise matching’sPapadakis and al. [27] proposed meta-blocking instead of multiple blocking keysapproach, to address some inefficiency showed when working with large-scale heterogeneous data. In addition, he made experiments to evaluate pruning schemes, to show that meta-blocking improves blocking efficiency.5.2.3. Incremental record linkageWhang and Garcia-Molina has focused on the evolution of pairwise matching rules over time, and identified a general incremental condition which incremental record-linkage can be performed [28] [29]. Gruenheid and al. [30] proposed some incremental techniques that gives more efficiency than the batch linkage and the old incremental record linkage algorithms.5.2.4. Linking Text Snippets to structured dataCortez and da Silva [31] proposed using information extraction techniques over unstructured data to obtain structured data before using linkage techniques.In order to link Hundreds of thousands products offers with the structured product information, Kannan and al. [32] proposed a novel approach to link text snippets to structured data, which is mainly based on:Semantic parsing of text snippet using tagging, plausible parse and optimal parse techniquesA matching function that return a probabilistic score of matching between records.5.2.5. Temporal record linkageLi and al. [33] proposed a technique to identify out of date attribute values using a model of entity evolution over time, which enable linkage over temporal record.Chiang and al. [34] developed detailed probabilistic models for better capturing entity evolution, and proposed faster temporal record linkage algorithms. In addition, they made experiments on real world data sets, including DBLP (computer science bibliography data set) data set to evaluate their work and they obtains good results on hard cases in DBLP.5.2.6. Record linkage with uniqueness constraintsGuo and al. [35] proposed a record linkage technique, which showed promising results in the presence of both erroneous data and multiple representations of the same attribute value.They proposed a combination of record linkage and data fusion to identify false attribute values and differentiate it from alternate representations of the correct value.6. Data Fusion6.1. Data Fusion basicsWhen unstructured and big data sources are integrated with structured operational data, you need to be confident that the results will be meaningful.Data fusion is a combination of techniques that aims to resolve conflicts from a collection of sources and to find the truth that reflects the real world. It is a new field that has emerged recently. Its motivation is exactly the veracity of data: the Web has made it easy to publish and spread false information across multiple sources, which make the separation of the wheat from the chaff very critical to present a high quality data. [2]There are three techniques used in Data Fusion: Copy detection, Voting and Source quality.Copy Detection: detect copier sources and reduces their weightVoting: detecting the most common value for each attribute.Source Quality: after voting, we gives more weight to knowledgeable sources (that have the highest number of common attributes)As example, consider that we have five data sources S1 … S5 having the same five attribute Att1 … Att5 as shown in Table. 5As shown in Figure. 5, first we search for copier sources, after looking to S3, S4 and S5, we find that S3 and S4 are identical. In addition, there is only one difference with S5. Therefore, the weight of S4 and S5 are reduced.After that, it is the Voting phase in which we search for the most common values for each attribute like shown in Figure. 6 (the values in red are the most common values)In the last phase, the Source quality we find that S1 has the highest number of common attribute so we gives it more weight.6.2. Literature review6.2.1. Truth discoveryMany efforts was made for measuring the trustworthiness of sources, Dong an al. [36] proposed a process for data fusion that is composed of 3 phases: Truth discovery, Trustworthiness evaluation and Copy detection. After that, many researchers proposed some extensions for each phase. [37] [38] [39]6.2.2. Online Data FusionIn many domains, online data changes over time and in many cases, it need to be evaluated in real-time. To address this problem Liu and al. [40] proposed an online data fusion technique in which source accuracy and copying relationship are evaluated in offline mode, and at query answering time the truth discovery is evaluated. This technique was evaluated by experimenting on a book data set and shows high efficiency.6.2.3. Dynamic Data FusionOnline data sets are often dynamic, in many cases data changes in a high speed, which create an issues to the static data fusion techniques. To address this problem, Dong and al. [41] proposed and evaluated a dynamic data fusion algorithm.7. DiscussionAs we saw in the previous sections, there are many proposed algorithms and techniques to address the big data integration challenges, but there are many things that are not taken into consideration in these proposals. First, all proposal are considering that the data is well formed, and the data pre-processing (Extract, Transform) is done, but when talking about the online data we are talking about a data generated by multi-linguistic data sources, even if the language is well recognized it may be slang.In addition, the only type of unstructured data that is taken into consideration is the text, but other types are frequently implemented, as example: posts on social media can contains videos, audios, images, maps and other types. In addition, when integrating data from social media (blogs, tweets, posts …) data has mostly a very low quality, because it is provided by normal users, which may do not have the basics skills of computing and writing. Therefore, it is very challenging to extract useful information.Also, all proposal require first to build the model offline, and then populated online. Which cannot always be done in real world. When it is about the veracity and variety of the data, we cannot always analyze the data sources offline. Sometimes we need online-analytics to be done which is very challenging especially if we take into consideration the issues listed above.In addition, when talking about data fusion, in Truth discovery data sources are evaluated based on which contains values with highest vote (the value provided by the largest number of sources). However, in many cases, especially when talking about Cyber-Propaganda, false informations are spreaded widely with a high speed to persuade, so it is not true that the value provided by largest number of sources is always the truth.8. Data Integration ToolsThere are many tools used in big data integration. Some of them was used in traditional integration processes and were enhanced and evolved to fit the big data needs. In addition, they can be classified as Commercial and Open Source. The following table contains some of these tools.9. SummaryIn this report, we gave a brief description of the traditional data integration techniques. Then we talked about Big Data integration challenges and we described three techniques used in this domain: the Schema mapping, Record linkage and the Data fusion.In addition, we listed some of the latest research topics that were made in this domain. Finally, we showed some of the tools that can be used.10. References[1] J. Hurwitz and A. Nugent and F. Halper and M. Kaufman, “Big Data for Dummies”, Date Published: 2013–04–5[2] X. L. Dong and D. Srivastava, “Big Data Integration”, Published date: 2017–08–26[3] What is, “Data Integration Definition”, URL: http://whatis.techtarget.com/definition/data-integration, Date Retrieved: 2017–12–30[4] Pentaho, “What is Data Integration”, URL: http://www.pentaho.com/faq/what-is-data-integration, Date Retrieved: 2017–12–30[5] IBM, “Data Integration”, URL: https://www.ibm.com/analytics/data-integration, Date Retrieved: 2017–12–30[6] Pentaho, “Pentaho Data Integration (Kettle) Tutorial”, URL: https://wiki.pentaho.com/display/EAI/Pentaho+Data+Integration+(Kettle)+Tutorial, Date Retrieved: 2017–12–30[7] Teradata, “Teradata products”, URL: https://www.teradata.com/Products, Date Retrieved: 2017–12–30[8] Matillion, “Matillion ETL Product Overview”, URL: https://redshiftsupport.matillion.com/customer/en/portal/articles/1975061-matillion-etl-product-overview, Date Retrieved: 2017–12–30[9] Oracle, “Oracle Data Integrator”, URL: http://www.oracle.com/technetwork/middleware/data-integrator/overview/index.html, Date Retrieved: 2017–12–30[10] Microsoft, “SQL Server Integration Services”, URL: https://docs.microsoft.com/en-us/sql/integration-services/sql-server-integration-services, Date Retrieved: 2017–12–30[11] Guru99, “What is Informatica? Complete Introduction”, URL: https://www.guru99.com/introduction-informatica.html, Date Retrieved: 2017–12–30[12] Apache Pig, “Welcome to Apache Pig!”, URL: https://pig.apache.org/, Date Retrieved: 2017–12–30[13] Apache sqoop, “Apache Sqoop”, URL: http://sqoop.apache.org/, Date Retrieved: 2017–12–30[14] Hadoop Mapreduce, “MapReduce Tutorial”, URL: https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html, Date Retrieved: 2017–12–30[15] Data Bricks, “What is Apache Spark™?” , https://databricks.com/spark/about, Date Retrieved: 2017–12–30[16] Talend, “Talend Open Studio for Data Integration”, URL: https://www.talend.com/products/data-integration/data-integration-open-studio, Date Retrieved: 2017–12–30[17] X. L. Dong and D. Srivastava, “A Small Tutorial on Big Data Integration”, URL: http://www.research.att.com/~divesh/papers/bdi-icde2013.pptx, Date Retrieved: 2017–12–15[18] IBM, “IBM Information server for data integration”, URL: https://www.ibm.com/us-en/marketplace/information-server-for-data-integration, Date Retrieved: 2018–01–10[19] Anish Das Sarma, Xin Luna Dong, and Alon Y. Halevy, “Bootstrapping pay-as-you-go data integration systems”, pages 861–874, Date Published: 2008.[20] Jayant Madhavan, David Ko, Lucja Kot, Vignesh Ganapathy, Alex Rasmussen, and Alon Y. Halevy., “Google’s deep web crawl”, Published Date: 2008[21] Michael J. Cafarella, Alon Y. Halevy, Daisy ZheWang, EugeneWu, and Yang Zhang, ”Webtables: exploring the power of tables on the web”, Published Date: 2008.[22] Anish Das Sarma, Lujun Fang, Nitin Gupta, Alon Y. Halevy, Hongrae Lee, Fei Wu, Reynold Xin, and Cong Yu, “Finding related tables”., pages 817–828, Published Date: 2012[23] Girija Limaye, Sunita Sarawagi, and Soumen Chakrabarti. “Annotating and searching web tables using entities, types and relationships”, Published Date: 2010[24] Apache Storm, “Apache Storm” , URL: http://storm.apache.org/, Date Retrieved: 2018–01–12[25] IBM analytics, “What is MapReduce?”, URL: https://www.ibm.com/analytics/hadoop/mapreduce, Date retrieved: 2018–01–13[26] Lars Kolb, Andreas Thor, and Erhard Rahm. “Load balancing for mapreduce-based entity resolution”, pages 618–629, Published Date: 2012.[27] George Papadakis, Georgia Koutrika, Themis Palpanas, and Wolfgang Nejdl. “Meta-blocking: Taking entity resolution to the next level”, Published date: 2014[28] Steven Euijong Whang and Hector Garcia-Molina. “Entity resolution with evolving rules”, Published date: 2010[29] Steven Euijong Whang and Hector Garcia-Molina. “Incremental entity resolution on rules and data”, Published date: 2014[30] Anja Gruenheid, Xin Luna Dong, and Divesh Srivastava. “Incremental record linkage”, Published date: 2014[31] Eli Cortez and Altigran Soares da Silva. “Unsupervised Information Extraction by Text Segmentation”, Published date: 2013[32] Anitha Kannan, Inmar E. Givoni, Rakesh Agrawal, and Ariel Fuxman. “Matching unstructured product offers to structured product specifications”, Published date: 2011.[33] Pei Li, Xin Luna Dong, Andrea Maurino, and Divesh Srivastava. “Linking temporal records”, Published date: 2011[34] Yueh-Hsuan Chiang, AnHai Doan, and Jeffrey F. Naughton. “Modeling entity evolution for temporal record matching”, Published date: 2014[35] Songtao Guo, Xin Luna Dong, Divesh Srivastava, and Remi Zajac. “Record linkage with uniqueness constraints and erroneous values”, Published date: 2010[36] XinLuna Dong, Laure Berti-Equille, and Divesh Srivastava. “Integrating conflicting data”, Published date: 2009[37] Alban Galland, Serge Abiteboul,Am´elie Marian, and Pierre Senellart. “Corroborating information from disagreeing views”, Published date: 2010[38] Jeff Pasternack and Dan Roth. “Knowing what to believe (when you already know something)”, Published date: 2010[39] Xiaoxin Yin, Jiawei Han, and Philip S. Yu., “Truth discovery with multiple conflicting information providers on the web”, Published date: 2007[40] Xuan Liu, Xin Luna Dong, Beng Chin Ooi, and Divesh Srivastava. “Online data fusion”, Published date: 2011[41] Xin Luna Dong, Laure Berti-Equille, and Divesh Srivastava. “Truth discovery and copying detection in a dynamic world”, Published date: 2009--1----1More from Towards Data ScienceYour home for data science. A Medium publication sharing concepts, ideas and codes.Read more from Towards Data ScienceRecommended from MediumSonaliindata-surgeConnecting to Databricks from PowerBIBen CreamerIdentifying use cases for artificial intelligence and robotic process automation in your businessErin Dawn TrochiminGeospatial Processing at ScaleTeaching Google Earth EngineGanesh ChandrasekaraninAnalytics VidhyaBase 36 — Why & How its important ?MagnimindinBecoming Human: Artificial Intelligence MagazineWhat advice do you give someone beginning to learn data science?Bikash PokharelApproaches for Handling Missing Data(NaN or NA) With Code and ExampleCaitlin MoyinRutgers WiCSWhat do the 109 Google Translate Languages Have in Common?Kurt KlingensmithinTowards Data SciencePreparing DNS Data for Cyber Security-Focused Data ScienceAboutHelpTermsPrivacyGet the Medium appGet startedHadi Fadlallah152 FollowersData Engineer, Doctoral ResearcherFollowMore from MediumMarie LefevreinTowards Data ScienceModern or Not: What Is a Data Stack?Stefan GrafFactless Fact table — not so absurd it may sound at firstB EYEExtracting Data from Anaplan with Qlik SenseVivek ParateData Travel from Raw to WisdomHelpStatusWritersBlogCareersPrivacyTermsAboutKnowable






































