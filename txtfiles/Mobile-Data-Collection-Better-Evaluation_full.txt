





















Mobile Data Collection | Better Evaluation






















































 


Jump to navigation









Menu
HomeSearchOverviewStart hereWhat is evaluation?Manage evaluationChoose methods and processesStrengthen evaluation capacityAbout BetterEvaluationOur principlesAbout usOur Theory of ChangeOur capacity strengthening approachOur research and innovationNewsEngage with BetterEvaluationJoin our communityContribute contentMethods and processesFind evaluation options through the Rainbow Framework:Manage an evaluation or evaluation systemUnderstand and engage stakeholdersEstablish decision making processesDecide who will conduct the evaluationDetermine and secure resourcesDefine ethical and quality evaluation standardsDocument management processes and agreementsDevelop planning documents for the evaluation or M&E systemReview evaluation (do meta-evaluation)Strengthen Evaluation CapacityDefine what is to be evaluatedDevelop initial descriptionDevelop programme theory / theory of changeIdentify potential unintended resultsFrame the boundaries for an evaluationIdentify primary intended usersDecide purposeSpecify key evaluation questionsDetermine what 'success' looks likeDescribe activities, outcomes, impacts and contextSampleUse measures, indicators or metricsCollect and/or retrieve dataManage dataCombine qualitative and quantitative dataAnalyse dataVisualise dataUnderstand Causes of outcomes and impactsCheck the results are consistent with causal contributionCompare results to the counterfactualInvestigate possible alternative explanationsSynthesise data from one or more evaluationsSynthesise data from a single evaluationSynthesise data across evaluationsExtrapolate findingsReport & Support Use of findingsIdentify reporting requirementsDevelop reporting mediaEnsure accessibilityDevelop recommendationsSupport useApproachesAppreciative InquiryBeneficiary AssessmentCase StudyCausal Link MonitoringCollaborative Outcomes ReportingContribution AnalysisCritical System HeuristicsDemocratic EvaluationDevelopmental EvaluationEmpowerment EvaluationHorizontal EvaluationInnovation HistoryInstitutional HistoriesMost Significant ChangeOutcome HarvestingOutcome MappingParticipatory EvaluationParticipatory Rural AppraisalPositive DevianceQualitative Impact Assessment ProtocolRandomized Controlled TrialRapid EvaluationRealist EvaluationSocial Return on InvestmentSuccess Case MethodUtilization-Focused EvaluationWhat are approaches?
Approaches (on this site) refer to an integrated package of options (methods or processes). For example, 'Randomized Controlled Trials' (RCTs) use a combination of the options random sampling, control group and standardised indicators and measures.
Appreciative enquiry
A strengths-based approach designed to support ongoing learning and adaptation by identifying and investigating outlier examples of good practice and ways of increasing their frequency.
Click an approach on the left to navigate to it

Beneficiary Assessment
An approach that focuses on assessing the value of an intervention as perceived by the (intended) beneficiaries, thereby aiming to give voice to their priorities and concerns.
Click an approach on the left to navigate to it
Case study
A research design that focuses on understanding a unit (person, site or project) in its context, which can use a combination of qualitative and quantitative data.
Click an approach on the left to navigate to itCausal Link Monitoring 
An approach designed to support ongoing learning and adaptation, which identifies the processes required to achieve desired results, and then observes whether those processes take place, and how.
Click an approach on the left to navigate to it
Collaborative Outcomes Reporting
An impact evaluation approach based on contribution analysis, with the addition of processes for expert review and community review of evidence and conclusions.
Click an approach on the left to navigate to itContribution Analysis
An impact evaluation approach that iteratively maps available evidence against a theory of change, then identifies and addresses challenges to causal inference.
Click an approach on the left to navigate to itCritical System Heuristics
An approach used to surface, elaborate, and critically consider the options and implications of boundary judgments, that is, the ways in which people/groups decide what is relevant to what is being evaluated.
Click an approach on the left to navigate to itDemocratic Evaluation
Various ways of doing evaluation in ways that support democratic decision making, accountability and/or capacity.
Click an approach on the left to navigate to itDevelopmental Evaluation
An approach designed to support ongoing learning and adaptation, through iterative, embedded evaluation.
Click an approach on the left to navigate to itEmpowerment Evaluation
A stakeholder involvement approach designed to provide groups with the tools and knowledge they need to monitor and evaluate their own performance and accomplish their goals.
Click an approach on the left to navigate to it
Horizontal Evaluation
A particular type of case study used to jointly develop an agreed narrative of how an innovation was developed, including key contributors and processes, to inform future innovation efforts.
Click an approach on the left to navigate to itInnovation History
A way to jointly develop an agreed narrative of how an innovation was developed, including key contributors and processes, to inform future innovation efforts.
Click an approach on the left to navigate to it
Institutional Histories
A particular type of case study used  to create a narrative of how institutional arrangements have evolved over time and have created and contributed to more effective ways to achieve project or program goals.
Click an approach on the left to navigate to itMost Significant Change
Approach primarily intended to clarify differences in values among stakeholders by collecting and collectively analysing personal accounts of change.
Click an approach on the left to navigate to itOutcome Harvesting
An impact evaluation approach suitable for retrospectively identifying emergent impacts by collecting evidence of what has changed  and, then, working backwards, determining whether and how an intervention has contributed to these changes.
Click an approach on the left to navigate to itOutcome Mapping
An impact evaluation approach which unpacks  an initiative’s theory of change, provides a framework to collect data on immediate, basic changes that lead to longer, more transformative change, and allows for the plausible assessment of the initiative’s contribution to results via ‘boundary partners’.
Click an approach on the left to navigate to it
Participatory Evaluation
A range of approaches that engage stakeholders (especially intended beneficiaries) in conducting the evaluation and/or making decisions about the evaluation​.
Click an approach on the left to navigate to itParticipatory Rural Appraisal
A participatory approach which enables  farmers to analyse their own situation and develop a common perspective on natural resource management and agriculture at village level. 
Click an approach on the left to navigate to itPositive Deviance
A strengths-based approach to learning and improvement that involves intended evaluation users in identifying ‘outliers’ – those with exceptionally good outcomes - and understanding how they have achieved these.
Click an approach on the left to navigate to itQualitative Impact Assessment Protocol  (QUIP)
An impact evaluation approach without a control group that uses narrative causal statements elicited directly from intended project beneficiaries.
Click an approach on the left to navigate to it
Randomised Controlled Trials (RCT)
An impact evaluation approach that compares results between a randomly assigned control group and experimental group or groups to produce an estimate of the mean net impact of an intervention.
Click an approach on the left to navigate to it
Rapid Evaluation
A Rapid Evaluation is an approach that uses multiple evaluation methods and techniques to quickly and systematically collect data when time or resources are limited.
Many terms are used to describe these approaches, including real time evaluations, rapid feedback evaluation, rapid evaluation methods, rapid-cycle evaluation and rapid appraisal.  The common feature of these different models is the expedited implementation timeframes which generally range from 10 days to 6 months.
 

Realist Evaluation
An approach especially to impact evaluation which examines what works for whom in what circumstances through what causal mechanisms, including changes in the reasoning and resources of participants.
Click an approach on the left to navigate to itSocial Return on Investment (SROI)
An participatory approach to value-for-money evaluation that identifies a broad range of social outcomes, not only the direct outcomes for the intended beneficiaries of an intervention.
Click an approach on the left to navigate to it
Success Case Method




The Success Case Method (SCM) involves identifying the most and least successful cases in a program and examining them in detail. This approach was developed by Robert Brinkerhoff to assess the impact of organisational interventions, such as training and coaching, though the use of SCM is not limited to this context. It is a useful approach to document stories of impact and to develop an understanding of the factors that enhance or impede impact.




Click an approach on the left to navigate to it
Utilisation-Focused Evaluation
An approach to decision-making in evaluation that involves identifying the primary intended users and uses of an evaluation and then making all decisions in terms of the evaluation design and plan with reference to these.
Click an approach on the left to navigate to itThemesWhat's a theme?Types of evaluationEvaluability assessmentImpact evaluationMonitoringSustained and emerging impacts evaluation (SEIE)Cross-cutting themesComplexityEvaluation and childrenFeminist evaluationFootprint evaluationGender analysisIndigenous* evaluationTechnology and evaluation in insecure settingsSectorsAgricultural projects and programsHumanitarian actionTypes of interventionAdaptive managementCapacity development resultsImpact investingNetwork evaluationOrganisational performancePolicy influence and advocacyResource libraryAbout the resource librarySearch resourcesUsing our resource libraryNew materialCommunityEvents calendarResource collections and projectsAEA Coffee Break webinarsBetterEval: WorldDownload the Rainbow FrameworkEqual access participatory M&E toolkitEvaluating C4D Resource HubGuidance for managers and commissionersImpact Evaluation SeriesGeneraTORManager's Guide to EvaluationNSW Government Evaluation ToolkitWriteshops casesEvaluation practice in Aboriginal and Torres Straight Islander settingsMethods LabBlogContributeRegisterLogin 





BetterEval:World






Join usLoginContribute content









Search form

Search this site 



-Any-ApproachBlog entryDevelopment ThemeEvaluation OptionEventFAQForum topicLanguage Landing PagePagePage One ColumnResourcesTask





Filter


      Select a content type to filter search results:    
Approach
Blog entry
Development Theme
Evaluation Option
Event
FAQ
Forum topic
Language Landing Page
Page
Page One Column
Resources
Task
 




HomeOverviewStart hereWhat is evaluation?Manage evaluationChoose methods and processesStrengthen evaluation capacityAbout BetterEvaluationOur principlesAbout usOur Theory of ChangeOur capacity strengthening approachOur research and innovationNewsEngage with BetterEvaluationJoin our communityContribute contentMethods and processesFind evaluation options through the Rainbow Framework:Manage an evaluation or evaluation systemUnderstand and engage stakeholdersEstablish decision making processesDecide who will conduct the evaluationDetermine and secure resourcesDefine ethical and quality evaluation standardsDocument management processes and agreementsDevelop planning documents for the evaluation or M&E systemReview evaluation (do meta-evaluation)Strengthen Evaluation CapacityDefine what is to be evaluatedDevelop initial descriptionDevelop programme theory / theory of changeIdentify potential unintended resultsFrame the boundaries for an evaluationIdentify primary intended usersDecide purposeSpecify key evaluation questionsDetermine what 'success' looks likeDescribe activities, outcomes, impacts and contextSampleUse measures, indicators or metricsCollect and/or retrieve dataManage dataCombine qualitative and quantitative dataAnalyse dataVisualise dataUnderstand Causes of outcomes and impactsCheck the results are consistent with causal contributionCompare results to the counterfactualInvestigate possible alternative explanationsSynthesise data from one or more evaluationsSynthesise data from a single evaluationSynthesise data across evaluationsExtrapolate findingsReport & Support Use of findingsIdentify reporting requirementsDevelop reporting mediaEnsure accessibilityDevelop recommendationsSupport useApproachesAppreciative InquiryBeneficiary AssessmentCase StudyCausal Link MonitoringCollaborative Outcomes ReportingContribution AnalysisCritical System HeuristicsDemocratic EvaluationDevelopmental EvaluationEmpowerment EvaluationHorizontal EvaluationInnovation HistoryInstitutional HistoriesMost Significant ChangeOutcome HarvestingOutcome MappingParticipatory EvaluationParticipatory Rural AppraisalPositive DevianceQualitative Impact Assessment ProtocolRandomized Controlled TrialRapid EvaluationRealist EvaluationSocial Return on InvestmentSuccess Case MethodUtilization-Focused EvaluationWhat are approaches?
Approaches (on this site) refer to an integrated package of options (methods or processes). For example, 'Randomized Controlled Trials' (RCTs) use a combination of the options random sampling, control group and standardised indicators and measures.
Appreciative enquiry
A strengths-based approach designed to support ongoing learning and adaptation by identifying and investigating outlier examples of good practice and ways of increasing their frequency.
Click an approach on the left to navigate to it

Beneficiary Assessment
An approach that focuses on assessing the value of an intervention as perceived by the (intended) beneficiaries, thereby aiming to give voice to their priorities and concerns.
Click an approach on the left to navigate to it
Case study
A research design that focuses on understanding a unit (person, site or project) in its context, which can use a combination of qualitative and quantitative data.
Click an approach on the left to navigate to itCausal Link Monitoring 
An approach designed to support ongoing learning and adaptation, which identifies the processes required to achieve desired results, and then observes whether those processes take place, and how.
Click an approach on the left to navigate to it
Collaborative Outcomes Reporting
An impact evaluation approach based on contribution analysis, with the addition of processes for expert review and community review of evidence and conclusions.
Click an approach on the left to navigate to itContribution Analysis
An impact evaluation approach that iteratively maps available evidence against a theory of change, then identifies and addresses challenges to causal inference.
Click an approach on the left to navigate to itCritical System Heuristics
An approach used to surface, elaborate, and critically consider the options and implications of boundary judgments, that is, the ways in which people/groups decide what is relevant to what is being evaluated.
Click an approach on the left to navigate to itDemocratic Evaluation
Various ways of doing evaluation in ways that support democratic decision making, accountability and/or capacity.
Click an approach on the left to navigate to itDevelopmental Evaluation
An approach designed to support ongoing learning and adaptation, through iterative, embedded evaluation.
Click an approach on the left to navigate to itEmpowerment Evaluation
A stakeholder involvement approach designed to provide groups with the tools and knowledge they need to monitor and evaluate their own performance and accomplish their goals.
Click an approach on the left to navigate to it
Horizontal Evaluation
A particular type of case study used to jointly develop an agreed narrative of how an innovation was developed, including key contributors and processes, to inform future innovation efforts.
Click an approach on the left to navigate to itInnovation History
A way to jointly develop an agreed narrative of how an innovation was developed, including key contributors and processes, to inform future innovation efforts.
Click an approach on the left to navigate to it
Institutional Histories
A particular type of case study used  to create a narrative of how institutional arrangements have evolved over time and have created and contributed to more effective ways to achieve project or program goals.
Click an approach on the left to navigate to itMost Significant Change
Approach primarily intended to clarify differences in values among stakeholders by collecting and collectively analysing personal accounts of change.
Click an approach on the left to navigate to itOutcome Harvesting
An impact evaluation approach suitable for retrospectively identifying emergent impacts by collecting evidence of what has changed  and, then, working backwards, determining whether and how an intervention has contributed to these changes.
Click an approach on the left to navigate to itOutcome Mapping
An impact evaluation approach which unpacks  an initiative’s theory of change, provides a framework to collect data on immediate, basic changes that lead to longer, more transformative change, and allows for the plausible assessment of the initiative’s contribution to results via ‘boundary partners’.
Click an approach on the left to navigate to it
Participatory Evaluation
A range of approaches that engage stakeholders (especially intended beneficiaries) in conducting the evaluation and/or making decisions about the evaluation​.
Click an approach on the left to navigate to itParticipatory Rural Appraisal
A participatory approach which enables  farmers to analyse their own situation and develop a common perspective on natural resource management and agriculture at village level. 
Click an approach on the left to navigate to itPositive Deviance
A strengths-based approach to learning and improvement that involves intended evaluation users in identifying ‘outliers’ – those with exceptionally good outcomes - and understanding how they have achieved these.
Click an approach on the left to navigate to itQualitative Impact Assessment Protocol  (QUIP)
An impact evaluation approach without a control group that uses narrative causal statements elicited directly from intended project beneficiaries.
Click an approach on the left to navigate to it
Randomised Controlled Trials (RCT)
An impact evaluation approach that compares results between a randomly assigned control group and experimental group or groups to produce an estimate of the mean net impact of an intervention.
Click an approach on the left to navigate to it
Rapid Evaluation
A Rapid Evaluation is an approach that uses multiple evaluation methods and techniques to quickly and systematically collect data when time or resources are limited.
Many terms are used to describe these approaches, including real time evaluations, rapid feedback evaluation, rapid evaluation methods, rapid-cycle evaluation and rapid appraisal.  The common feature of these different models is the expedited implementation timeframes which generally range from 10 days to 6 months.
 

Realist Evaluation
An approach especially to impact evaluation which examines what works for whom in what circumstances through what causal mechanisms, including changes in the reasoning and resources of participants.
Click an approach on the left to navigate to itSocial Return on Investment (SROI)
An participatory approach to value-for-money evaluation that identifies a broad range of social outcomes, not only the direct outcomes for the intended beneficiaries of an intervention.
Click an approach on the left to navigate to it
Success Case Method




The Success Case Method (SCM) involves identifying the most and least successful cases in a program and examining them in detail. This approach was developed by Robert Brinkerhoff to assess the impact of organisational interventions, such as training and coaching, though the use of SCM is not limited to this context. It is a useful approach to document stories of impact and to develop an understanding of the factors that enhance or impede impact.




Click an approach on the left to navigate to it
Utilisation-Focused Evaluation
An approach to decision-making in evaluation that involves identifying the primary intended users and uses of an evaluation and then making all decisions in terms of the evaluation design and plan with reference to these.
Click an approach on the left to navigate to itThemesWhat's a theme?Types of evaluationEvaluability assessmentImpact evaluationMonitoringSustained and emerging impacts evaluation (SEIE)Cross-cutting themesComplexityEvaluation and childrenFeminist evaluationFootprint evaluationGender analysisIndigenous* evaluationTechnology and evaluation in insecure settingsSectorsAgricultural projects and programsHumanitarian actionTypes of interventionAdaptive managementCapacity development resultsImpact investingNetwork evaluationOrganisational performancePolicy influence and advocacyResource libraryAbout the resource librarySearch resourcesUsing our resource libraryNew materialCommunityEvents calendarResource collections and projectsAEA Coffee Break webinarsBetterEval: WorldDownload the Rainbow FrameworkEqual access participatory M&E toolkitEvaluating C4D Resource HubGuidance for managers and commissionersImpact Evaluation SeriesGeneraTORManager's Guide to EvaluationNSW Government Evaluation ToolkitWriteshops casesEvaluation practice in Aboriginal and Torres Straight Islander settingsMethods LabBlog











You are hereHome › Mobile Data Collection 
Mobile Data Collection





Synonyms: MDC, Mobile Phone Logging, Mobile Technology



 

Mobile Data Collection (MDC) is the use of mobile phones, tablets or PDAs for programming or data collection. MDC can be very useful to the evaluator who is collecting quantitative data for their evaluation or abstracting data for an evaluation.
 

There are many mobile phone applications (referred to as platforms) which will allow you to build a mobile data collection survey. These platforms will allow you to customise the survey to collect specific data as required, such as photographs, information from a list selection, voice recordings, GPS coordinates, etc. Platforms vary in ease of use, cost, and features.
Brief Example
Pact in Namibia wanted to conduct a retrospective evaluation of capacity development work with partners during their project. They needed to collect data from 17 partners, Pact staff themselves and the donor. Data collection had to be accomplished within a one week period.
Mobile technology facilitated the collection of the evaluation data by ensuring that a survey with skip patterns and variable checks was designed and that the data were directly uploaded to the platform server by the evaluators. The data were available the next week for the analysis and report writing team, a process which aided the creation of a stakeholders presentation within one week of the end of data collection. The team learned a number of key lessons from this, and another use of mobile technology in Tanzania, which were presented at the American Evaluation Association meeting in 2012.
Advice
Advice for CHOOSING this option (tips and traps)
An important first step is figuring out whether or not MDC is appropriate for your evaluation. Most evaluations use surveys, which MDC is well-suited for, and generally a subset of available platforms cater best to creating surveys (as opposed to crowd-sourcing information, sending information out, or providing information on demand).  If you are collecting a lot of quantitative information and very little qualitative information, MDC is likely to work well.  However, if you are collecting many long, open-ended responses, MDC is unlikely to be the most efficient method; you could still record answers on mobile phones or tablets and either transcribe answers manually or use voice to text software, but typing in long answers on phones is likely to be disruptive to the interview process.


Consider your data needs. MDC is still not the best option for qualitative evaluation, but advances in this field are rapid, so this may change. 


MDC makes managing the data collection process easier.  Data are submitted in real time, allowing managers to see the pace of data collection, the coverage, and which data collectors are submitting data when and from where.


MDC makes entering and aggregating data easier.  Because the data are entered into the main database at the same time as they are collected, the long process of transcribing and double-entering responses is eliminated.  Data can be analyzed as soon as collection is done.


MDC offers instant visualization of data, including maps (if GPS coordinates are taken) and basic breakdowns of answers to each question.  This can be used for managing data collection or to aid later analysis.


MDC can lead to cleaner data.  Because platforms allow the form designer to put limits and skip logic on questions, answers that don’t make sense can be disallowed.


MDC can be less expensive than traditional paper surveys, because it reduces the costs associated with printing and form transportation, double entry, and data cleaning.


MDC allows the easy capture of other forms of data, such as images, video, and GPS coordinates.


What is the network coverage in the area where you’ll be collecting data?  While you don’t need network coverage to collect data, lack of coverage could affect when your data become available online.  If you don’t have network coverage, you can upload data through an Internet link, and you may want to invest in a portable WiFi hub.  Also, a few platforms will not work if there is no network coverage. 


What power sources are available to evaluators?  Will phones need to have a long battery life? Will you be able to recharge phones in vehicles?


What is your policy on phone use, ownership and replacement? Are data collectors allowed to use phones for personal purposes? Who pays for airtime? What happens if a phone is lost or stolen?


Consider the lifespan of the technology that you invest in. Will you be using the phones only once for a data collection effort, or will you repurpose the phones for other data collection or programming and use them throughout the life of the project? If you plan to re-use the phones, take this into consideration; you may want to invest more in phones that won’t be obsolete in a year, or ensure that they have certain capabilities that the survey may not require.


Ensure you have stakeholder buy-in for MDC. Getting buy-in from project stakeholders for the use of MDC is sometimes challenging. It’s important to clearly see and articulate the advantages of MDC, and to understand common challenges and solutions to those challenges so that they can be realistically addressed—and to understand when MDC might not be advantageous.


Be aware of security risks of sending data. If you send data through a data network, the platform encryption will be very strong and secure (256-bit). This is not true of data sent by SMS.


Advice for USING this option (tips and traps)
Once you’ve determined to use MDC, you’ll need to go through the usual steps of evaluation design and implementation, but with some special considerations added.


Create the evaluation tool: Your evaluation tool will have a big impact on everything else you choose. Once you have a tool in place, you’ll know what characteristics to look for in phones and platforms.


Evaluate the platforms: Which has the features that you need for your survey? Which are most cost effective?


Choose your technology: Which phones or tablets are compatible with the platform you’ve chosen? Which features (such as GPS capabilities, taking pictures or video, keyboards) do you need? What can the evaluation afford? What is locally available?


Create a budget: This should take into account the usual evaluation costs, such as data collectors, training workshops, and travel costs, but also include the cost of phones, airtime, the platform you’re using, and associated technology such as phone or solar chargers. The budget will not require paper copies and transport or double data entry, which sometimes means that the overall budget of mobile phone data collection will be lower.


Set up the mobile technology: Depending on the number of phones or tablets you are setting up, it may be easiest to set all of them up in advance for the data collectors, or to have the data collectors set up their own phones with the platform during the workshop.


Conduct the training: You will need an additional half to full training day to incorporate mobile technology training into your orientation for data collectors, depending on their level of familiarity with the technology.


Manage data collection: Data collection can be monitored in real time from a central online portal, making it easier to see if there are any problems with the questionnaire, if the survey is taking longer than planned, or if data collectors are covering their assigned areas properly.


Data aggregation: Your data are already aggregated. There is no need to re-enter the data, and you can export the data into Excel or a CSV format. All platforms will also provide some very basic instantaneous visualisation of data, which you can customise as necessary.


Data cleaning: Generally speaking, data will be much cleaner with mobile data entry than with transcribing paper forms, but data should still be checked over and cleaned.


Have back up options available: It’s wise for data collectors to have a couple of paper copies of the form when they go out into the field. The paper version may be easier for them to read questions off of and facilitates keeping their place in the survey, plus is a good back up in case the phone battery dies or something else goes wrong.


Considerations when choosing a platform
The platforms have very different characteristics, and there is currently no one platform that covers every base. The table below outlines common points of departure for phones and where the platforms tend to vary. However, note that all platforms are changing rapidly and that most are very willing to incorporate user feedback, especially if an organisation is planning on using the platform heavily; you may be able to negotiate different prices or services, or to suggest changes.
Table 1. Common points of departure for phones and where the platforms tend to vary



Characteristic
Variation


Phone compatibility for forms
Nearly all platforms work with Android phones, but few work with Apple, Blackberry, and Windows devices.  Most will also work with Java phones that use apps.


Skip logic
The majority of platforms offer skip logic, though not all.  Even among platforms with skip logic, platforms vary in the ease of use; some platforms require coding or complex logic branches, others have quite simple mechanisms for both branch and prerequisite logic.


Setup
Some platforms have setup support available for a fee.  Others may have free support contacts that can help you troubleshoot, but won’t set up your survey for you.  Typically, platforms without survey setup support will be more user-friendly to set up yourself, while those with support will be more difficult to do yourself but may be more flexible in their capabilities.


Language
Most platforms will let you ask questions in whatever language you have installed on your computer/phone, but some are coded in a way that makes this difficult and only offer certain alphabets. Additionally, some platforms are available in common languages such as Spanish, French, and Kiswahili.


SMS
Platforms differ in whether or not they accept and send SMSes.


Web portal entry
A useful feature is being able to enter data through a web portal rather than through a phone, but not all platforms provide this.


Extras
Check whether your platform can collect images, GPS, or video.



While comparing platforms, you may also find that there are seemingly unimportant intangibles about a certain platform that make it more friendly to your intended users.  For example, one platform might prompt the data collector to move forward with a “next” button, while another may require that the question swipe to move forward; some data collectors may find one method more intuitive than another.
Costs of the different platforms vary widely.  What is most cost effective will depend on the nature of your data collection.  Pricing models include:

Completely open source
Freemium: a basic service is freely available, but extras such as SMS or technical support set up require either a subscription (SMS) or a one-time fee (for set up) which is usually somewhat steep
Pay as you go: you pay for the amount of data collected
Subscription: after a trial period, you pay a monthly or yearly fee to use the service

Most platforms will allow at least a free trial, so if you find several that appear to meet your needs, it is wise to test them out before committing. 
Table 2. Platforms to consider using when collecting mobile data




Platform


Website




ChildCount+


http://www.childcount.org/resources/




CommCare


http://www.commcarehq.org/home/




*DataWinners


https://www.datawinners.com/en/home/




Doforms.com


http://www.doforms.com/




FieldCenter (cortex software)


http://www.cortexsoftware.com/




Ona (formerly Formhub)


https://ona.io/home/




Frontline SMS


http://www.frontlinesms.com/




Iformbuilder


View resource (rate and add comments)




Jana


View resource (rate and add comments)




*Magpi (formerly Episurveyor)


View resource (rate and add comments)




*Mobenzi Researcher


View resource (rate and add comments)




TTC Mobile


View resource (rate and add comments)




Viewworld


View resource (rate and add comments)




Resources
Course

TechChange: A series of online courses are available on mobile technology and mobile data collection.

Example

Digital Diversity: National Geographic reports on different examples of how innovative technologies and mobile phones are being used throughout the world to improve, enrich, and empower lives, including:. 
FLOW: Field Level Operations Watch - supporting communities to monitor their own water and sanitation for improved accountability.

Guide

Mobile Data Collection Systems:  A review of the current state of the field: This research by NOMAD, reviews existing mobile data collection software systems, projects, and initiatives.
Using Mobile Data for Development: This guide, written by Ed Naef et al., provides an examination of the use of mobile data collection in developing countries including how it can be leveraged for a range of development goals and potential obstacles that may be faced.

Tool

Crowdmap: allows you to set up your own map of Ushahidi without having to install it on your own web server.
NOMAD: provide a free tool for determining which MDC platform to choose

Sources
Jung, C. NOMAD, (2011). Mobile data collection systems a review of the current state of the field. Retrieved from website: http://humanitarian-nomad.org/wp-content/themes/nomad/presentations/NOMA...
Kugler, K. (2012, January 24). The cellphone that keeps the water, and data, flowing. Retrieved from http://newswatch.nationalgeographic.com/2012/01/24/the-cellphone-that-ke...
 

Updated:
      18th August 2020 - 5:27pm    

This Option is useful for:






 Describe 
 

>


  
 Collect and/or retrieve data 
 
 
 
  




 


 


A special thanks to this page's contributors



Author


Kerry Bruce
Director of Results and Measurement, Pact.
Madagascar.




Author


Jade Lamb






Author


Andrea Selva








Contributor


Nick Herft
UX Designer / Information Architect, BetterEvaluation.
Melbourne, Australia.








Reviewer


Patricia Rogers
Founder and former-CEO,  BetterEvaluation.
Melbourne.










Share




RSS
Print version
 


Comments




 



Wouter Rijneveld 22:19 26th July 2013


Nice article and nice overview of available options. I think some are missing. 
1. Ethnocorder (www.ethnocorder.com) can be used for MDC using IPod or IPhone. Strong point: easy for quant information mixed with multimedia. Tag overlay over video possible. I used this in Malawi, see report here: https://docs.google.com/file/d/0B0C42UxSyIcadjc4OVRJb050SDg/edit (download original wordfile to be able to use the multimedia).
2. Opendatakit (www.opendatakit.org or www.kobotoolbox.org). Completely open source. Easy to get it working. Full cycle: forms, collecting app for android, offline work, streaming of results directly into visualisations (concretely: online graph or table updates directly with each survey submitted). All free. Personally I think this could be a major option for resource low organisations. I am currently using this in Zambia.
 








 



Kerry Bruce 2:26 28th July 2013


Dear Wouter -
These are great additions and content.  Thank you for adding and enriching this page.  Great to hear about your work with mobile technology in Malawi and Zambia.
 








 



Richard Rawson 0:43 7th September 2013


Thank you for your review of this rapidly evolving set of technologies and applications. Recent additions to the NOMAD resources include COMMANDmobile, the flagship mobile data collection and workforce management solution from Tracen Technologies. Their data collection software has numerous for mobile surveys, field inspections, inventory management, workforce management, and monitoring and evaluation projects. More information: http://commandmobile.com/mobile-data-collection-software-features/. We blog weekly on mobile data collection for various industries, including Education, and would enjoy hearing your thoughts.
 








 



Kerry Bruce 0:15 9th December 2014


Dear Tuomas -
Thank you for the kind introduction to your technology.  It is on the list.  All the best - KB
 








 



Diego Menchaca 19:34 17th October 2019


Great resource on mobile data collection.
A characteristic that you may consider adding is Query Management. 
Query management is the ability of data collection systems to identify data entries with issues and isolate them into a report. For every out of range or inconsistent value, the data capture tool generates a data query. Each data issue becomes an entity in itself and thus can be tracked over time to see if it is still present, or if it has been resolved by someone in the team.
A query management system substantially minimizes and potentially eliminates the risk of invalid data remaining unnoticed. 
Also, it would be great if you could add Teamscope (https://www.teamscopeapp.com) to the list of platforms. Teamscope is data collection app (Android & iOS) for field and clinical research. 
 




Add new comment

Login Login and comment as BetterEvaluation member or simply fill out the fields below.    

 *



 *

The content of this field is kept private and will not be shown publicly.


Comment *


Switch to plain text editor




 Terms Acceptance *
I have read and I accept the terms of BetterEvaluation’s Privacy Policy and Terms of Use.

CAPTCHAThis question is for testing whether you are a human visitor and to prevent automated spam submissions.




 Notify me when new comments are posted 


 All comments 


 Replies to my comment 


Leave this field blank 


 






Share




RSS
Print version






Rainbow Framework
Rainbow Framework overview and downloads


Manage

Manage overview


1. Understand and engage stakeholders


2. Establish decision making processes


3. Decide who will conduct the evaluation


4. Determine and secure resources


5. Define ethical and quality evaluation standards


6. Document management processes and agreements


7. Develop planning documents for the evaluation or M&E system


8. Review evaluation (do meta-evaluation)


9. Strengthen evaluation capacity



Define

Define overview


1. Develop initial description


2. Develop programme theory/theory of change


3. Identify potential unintended results



Frame

Frame overview


1. Identify primary intended users


2. Decide purposes


3. Specify the key evaluation questions


4. Determine what 'success' looks like



Describe

Describe overview


1. Sample


2. Use measures, indicators or metrics


3. Collect and/or retrieve data


4. Manage data


5. Combining qualitative and quantitative data


6. Analyse data


7. Visualise data



Understand Causes

Understand Causes overview


1. Check the results are consistent with causal contribution


2. Compare results to the counterfactual


3. Investigate possible alternative explanations



Synthesise

Synthesise overview


1. Synthesise data from a single evaluation


2. Synthesise data across evaluations


3. Extrapolate findings



Report & Support Use

Report & Support Use overview


1. Identify reporting requirements


2. Develop reporting media


3. Ensure accessibility


4. Develop recommendations


5. Support use







Resources


 
 

  
 
 Open Data Kit (ODK)

 

 
 Using Mobile Data for Development

 

 
 DataWinners

 

 
 Mobile-Based Technology for Monitoring and Evaluation

 

 
 TTC Mobile

 
 
Pages1
2
3
4
5
next ›
last »

 
 



Suggest a Resource
















What is BetterEvaluation?
We are a global collaboration aimed at improving evaluation practice and theory through co-creation, curation, and sharing information.
More about us

Get in touch
Recommend content, collaborate, share, ask, tell us what you like, suggest an improvement, or just say hi! We’d love to hear from you.
Contact us
          
Help Sitemap Privacy policy Terms of use


 

Tweets by @BetterEval 

 





















