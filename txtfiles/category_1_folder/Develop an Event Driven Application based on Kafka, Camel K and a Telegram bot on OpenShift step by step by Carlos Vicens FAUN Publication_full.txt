develop
an
drive
component_1
base
on
technology_1
technology_2
k
and
a
telegram
requirement_1
on
technology_3
step
by
step
|
by
carlos
vicens
|
faun
publicationopen
in
apphomenotificationslistsstorieswritepublished
infaun
publicationcarlos
vicensfollowjan
2020·21
min
readsavehttps
common
wikimedia
wiki
knickerbocker_hospital
jpgdevelop
an
drive
component_1
base
on
technology_1
technology_2
k
and
a
telegram
requirement_1
on
technology_3
step
by
stepintrosome
month
ago
i
start
play
with
technology_2
k
i
instantly
fell
in
love
with
it
and
i
didn’t
have
any
previous
experience
with
technology_2
easy
to
quality_attribute_1
fast
development
ton
of
component_2
you
name
it
a
it
usually
happen
not
to
me
right
we
tech
people
try
a
quality_attribute_2
thing
and
if
we
love
it…
we
tend
to
use
it
everywhere…
ok
that
be
my
requirement_2
i
start
to
think
of
crazy
scenario
but
one
of
them
stand
out
and
after
some
refinement
end
up
be
this
project
note
ideally
you
will
connector_1
and
do
and
see
results…
and
hopefully
some
basic
to
create
your
own
eda
drive
component_1
this
lab
be
not
really
about
create
everything
from
scratch
but
more
about
have
an
example
project
up
and
run
and
then
explore
possibility
while
you
can
find
the
and
this
guide
herethe
ideathe
idea
come
to
me
after
remember
how
many
time
i
have
to
go
with
my
young
daughter
to
the
emergency
room
nothing
serious
but
recurrent
and
all
those
time
wait
for
connector_data_1
move
to
the
next
stage
and
inform
my
wife
who
be
with
my
other
daughter
wouldn’t
if
be
nice
to
connector_2
a
connector_data_2
to
my
telegram
component_3
every
time
the
state
of
the
patient
connector_3
well
that’s
the
aim
of
this
project
serve
a
an
example
eda
drive
component_1
run
on
technology_4
technology_3
that
connector_4
out
connector_data_3
via
a
telegram
requirement_1
whenever
the
state
status
of
a
patient
connector_3
in
order
to
give
our
example
eda
a
bit
more
of
fictitious
component_4
let’s
imagine
there’s
a
health
connector_data_4
component_5
his
at
a
hospital
connector_5
black
mountain
hospital
this
component_1
have
a
ui
where
you
can
connector_3
the
status
of
a
patient
and
every
time
a
connector_3
happen
a
proper
hl7
connector_data_2
be
connector_6
note
this
be
an
over
simplify
his
please
health
relate
professional
don’t
connector_7
too
mad
at
me
i
how
complex
a
real
his
be
the
scenario
portray
by
this
example
component_1
of
the
fictitious
black
mountain
hospital
comprise
these
element
his
frontend
angular
j
where
you
can
connector_3
the
status
of
a
patienthis
backend
technology_5
pattern_1
technology_6
connector_8
the
patient
info
technology_6
connector_data_5
be
persist
in
a
technology_7
component_6
this
piece
connector_4
out
hl7
connector_data_6
to
a
technology_1
topicintegration
pattern_2
technology_2
k
that
pattern_3
hl7
to
plain
you
can
connector_6
to
a
humantelegram
requirement_1
technology_8
j
where
you
can
signup
with
your
again
connector_data_5
be
persist
in
a
technology_7
databaseprerequisitesyou
need
connector_9
to
an
technology_3
2+
cluster
and
be
cluster
admin
or
connector_data_7
your
administrator
to
install
a
couple
of
element
for
you
you
can
also
run
your
own
local
x
cluster
use
codeready
container
scopein
this
guide
we’ll
cover
the
deployment
of
infrastructure
element
in
technology_3
component_6
technology_1
cluster
etc
local
development
of
ui
component_7
and
also
the
requirement_3
layerthe
deployment
of
component_7
on
openshiftget
yourself
readygit
clone
this
pattern_4
and
connector_3
dir…
the
usual
deployment
of
infrastructurewe
need
to
quality_attribute_1
a
couple
of
component_6
technology_1
and
also
technology_2
k…
let’s
connector_7
to
it
quality_attribute_1
kafkain
order
to
ease
the
deployment
of
technology_1
we’re
go
to
use
the
amq
operator
go
here
to
more
about
operator
warn
this
connector_data_8
should
be
run
by
a
cluster
administratorlog
in
a
a
cluster
admin
to
your
cluster
and
go
to
operator
operator
hub
in
the
search
start
type
amq
then
click
on
amq
connector_10
click
on
install
leave
the
default
requirement_4
and
click
subscribe
a
you
can
see
we
re
go
to
install
the
operator
so
that
it
s
quality_attribute_3
to
all
namespaces
this
will
allow
to
use
the
operator
a
a
normal
component_8
in
any
namespace
wait
until
status
connector_11
to
installsucceeded
if
status
be
installsucceeded
you
have
instal
the
operator
successfully
in
namespace
technology_3
operator
now
you
could
start
create
custom
resource
manage
by
the
amq
connector_12
operator
such
a
technology_1
technology_1
connector_13
etc
quality_attribute_1
the
technology_2
k
operatoras
we
have
explain
before
we
need
a
couple
of
technology_2
requirement_3
to
pattern_3
hl7
connector_data_6
come
in
to
a
technology_1
topic
and
another
one
to
connector_6
those
pattern_3
connector_data_6
to
a
telegram
requirement_1
well
in
order
to
run
this
technology_2
requirement_3
connector_14
we
can
do
it
manually
in
a
technology_9
project
or
use
technology_2
k
for
all
the
reason
mention
before
and
more
we’re
go
to
use
an
operator
the
technology_2
k
operator
warn
this
connector_data_8
should
be
run
by
a
cluster
administratorlog
in
a
to
your
cluster
a
cluster
admin
and
go
to
operator
operator
hub
in
the
search
start
type
technology_2
then
click
on
technology_2
k
click
on
continue
click
on
install
leave
the
default
requirement_4
and
click
subscribe
a
you
can
see
we
re
go
to
install
the
operator
so
that
it
s
quality_attribute_3
to
all
namespaces
this
will
allow
to
use
the
operator
a
a
normal
component_8
in
any
namespace
wait
until
status
connector_11
to
installsucceeded
if
that
be
the
requirement_2
you
have
instal
the
operator
successfully
in
namespace
technology_3
operator
now
you
could
start
create
custom
resource
manage
by
the
technology_2
k
operator
such
a
requirement_3
build
etc
quality_attribute_1
technology_1
use
the
amq
connector_12
operatoras
we
mention
before
we
need
a
couple
of
topic
one
for
hl7
and
another
one
for
pattern_3
events…
and
of
we
need
a
technology_1
cluster
to
support
them
note
this
connector_data_8
and
the
next
one
don’t
require
special
permission
apart
from
be
able
to
create
namespaces
deployment
pod
etc
we
have
prepare
a
set
of
number
shell
script
please
have
a
look
to
the
one
number
where
some
base
environment
variable
be
set
you
need
to
connector_3
the
project
name
to
be
sure
it
s
unique
in
your
cluster
set
the
environment
any
time
by
do
this
please
run
the
next
command
we’ll
need
to
use
$project_name
environment
variable
late
in
this
step
we
will
run
quality_attribute_1
technology_1
sh
please
have
a
look
to
this
script
there
be
a
couple
of
important
bit
there
first
it
create
a
project
to
hold
all
the
element
next
be
the
excerptoc
project
${project_name}important
if
you
run
the
script
in
order
and
you
don’t
create
another
project
in
between
the
default
project
will
be
automatically
set
to
the
project
create
that
be
the
one
set
by
$project_name
environment
variable
see
environment
sh
if
you
have
create
another
project
or
want
to
be
sure
the
default
project
be
set
correctly
please
use
oc
project
if
you
need
to
set
the
default
project
back
to
$project_name
do
this
environment
sh
&&
oc
project
$project_namesecond
it
also
create
a
custom
resource
cr
of
type
technology_1
that
define
a
technology_1
cluster
with
replica
three
component_9
plain
quality_attribute_4
and
technology_10
base
external
and
a
couple
of
cr
of
type
kafkatopic
for
each
of
the
technology_1
topic
we
need
note
the
amq
connector_12
operator
technology_11
to
the
creation
update
delete
of
a
set
of
custom
resource
definition
go
here
for
further
detailsnote
the
external
component_9
be
need
only
while
run
the
backend
component_10
locally
because
the
technology_1
cluster
be
run
in
technology_3
in
general
this
external
component_9
be
not
need
when
the
component_10
run
in
the
same
cluster
a
the
technology_1
cluster
now
please
run
the
script
warn
be
sure
you’re
requirement_5
in
if
unsure
run
oc
whoamito
pattern_5
the
status
of
the
deployment
you
can
run
the
next
command
a
you
can
see
there
be
replica
both
for
the
technology_1
cluster
and
for
the
technology_12
cluster
all
of
them
be
run
$
oc
connector_7
pod
n
$project_name
|
grep
state
component_11
clusterstate
component_11
cluster
component_12
operator
55d6f79ccf
dckht
run
5d15hstate
component_11
cluster
technology_1
run
5d15h
state
component_11
cluster
technology_1
run
5d15h
state
component_11
cluster
technology_1
run
5d15hstate
component_11
cluster
technology_12
run
5d15hstate
component_11
cluster
technology_12
run
5d15hstate
component_11
cluster
technology_12
run
5d15hanother
test
you
can
run
this
one
to
connector_15
if
our
topic
be
create
properly
$
oc
rsh
n
$project_name
state
component_11
cluster
technology_1
bin
technology_1
topic
sh
connector_data_9
bootstrap
component_13
localhost
9092defaulting
container
name
to
technology_1
use
oc
describe
pod
state
component_11
cluster
technology_1
n
state
component_11
assistant
to
see
all
of
the
container
in
this
pod
openjdk
bit
component_13
vm
warn
if
the
number
of
processor
be
expect
to
increase
from
one
then
you
should
configure
the
number
of
parallel
gc
component_14
appropriately
use
xx
parallelgcthreads=n__consumer_offsetsevents
topichl7
topicinstall
technology_2
k
clithis
step
be
quite
easy…
it
only
require
to
download
the
technology_2
k
cli
binary
kamel
please
run
this
script
note
open
the
script
environment
sh
if
you
want
to
connector_3
the
cli
version
install
technology_2
k
cli
shthis
script
leave
the
kamel
binary
in
the
same
directory
where
the
script
be
run
so
it
s
not
in
your
path
so
be
sure
to
connector_16
it
this
kamel
or
it
to
you
path
quality_attribute_1
the
his
backend
and
telegram
requirement_1
databaseswe’re
go
to
use
oc
component_3
command
to
quality_attribute_1
two
single
technology_8
technology_7
component_6
one
for
the
his
backend
and
another
one
for
the
telegram
requirement_1
if
you
have
a
look
to
script
you
ll
find
the
next
couple
of
command
these
be
normal
oc
component_3
command
that
create
the
need
descriptor
to
run
an
oci
image
centos
technology_7
centos7
pay
attention
to
the
follow
we
be
quality_attribute_1
the
same
image
twice
because
we
need
two
databasesthe
name
give
to
the
component_1
be
different
backend
component_6
vs
telegram
requirement_1
databasefinally
there
be
a
couple
of
environment
variable
that
set
component_8
and
password
with
the
same
requirement_4
for
both
component_6
yes
not
very
secure…
but
remember
this
be
for
development
purposesoc
component_3
e
postgresql_user=luke
\
epostgresql_password=secret
epostgresql_database=my_data
\
centos
technology_7
centos7
name=backend
databaseoc
component_3
e
postgresql_user=luke
\
epostgresql_password=secret
epostgresql_database=my_data
\
centos
technology_7
centos7
name=telegram
requirement_1
databasenow
run
the
script
quality_attribute_1
component_6
shcheck
the
status
of
the
deployment
with
this
command
eventually
you
should
see
something
this
where
the
status
of
our
component_6
pod
be
run
backend
component_6
ttq2b
in
this
example
output
$
oc
connector_7
pod
n
$project_name
|
grep
databasebackend
component_6
quality_attribute_1
complete
5mbackend
component_6
ttq2b
run
4mtelegram
requirement_1
component_6
7qv4d
run
4mtelegram
requirement_1
component_6
quality_attribute_1
complete
5mpreparing
the
development
environmentone
of
the
goal
of
the
lab
be
to
help
you
with
the
local
development
stage
for
instance
regard
the
his
backend
our
technology_5
component_15
technology_9
need
to
be
able
to
connector_13
to
an
technology_13
component_9
hence
you
need
the
root
ca
certificate
relate
to
that
component_9
a
a
trust
ca
to
a
keystore
to
do
so
we
have
prepare
this
script
prepare
development
env
sh
it
basically
do
the
follow
extract
the
root
ca
cert
from
a
secretoc
extract
secret
${cluster_name}
cluster
ca
cert
n
${project_name}
keys=ca
crt
to=
src
resource
ca
crtadds
it
to
a
technology_9
keystore$
keytool
delete
alias
root
keystore
src
resource
keystore
jks
storepass
password
noprompt$
keytool
trustcacerts
alias
root
src
resource
ca
crt
keystore
src
resource
keystore
jks
storepass
password
noprompthow
do
the
technology_5
component_1
which
keystore
use
to
connector_13
to
technology_1
well
there’s
a
connector_5
kafkaconfig
that
prepare
the
configuration
to
connector_13
to
technology_1
open
backend
src
technology_9
technology_14
redhat
his
component_15
kafkaconfig
technology_9
to
see
how
we
use
none
to
exclude
some
property
when
run
in
technology_3
then
we
have
to
different
profile
default
and
technology_3
so
when
develop
we
use
default
component_1
property
and
when
run
in
technology_3
we
use
technology_3
component_1
opeshift
property
next
you
can
find
the
relevant
property
in
the
default
profile
#
technology_1
bootstrap
component_16
=
run
backend
sh
set
kafka_service_host
automatically
for
local
dev
technology_1
bootstrap
servers=${kafka_service_host}
443#
component_17
and
groupkafka
clientid
=
kafkaclienthisbackendkafka
=
kafkahisbackendconsumergroup#
topicskafka
topic
=
hl7
topickafka
quality_attribute_5
technology_15
=
sslkafka
technology_16
truststore
location
=
src
resource
keystore
jkskafka
technology_16
truststore
password
=
passwordkafka
technology_16
keystore
location
=
src
resource
keystore
jkskafka
technology_16
keystore
password
=
passwordand
this
be
the
technology_3
profile
cuonterpart
#
technology_1
bootstrap
serverskafka
bootstrap
servers=state
component_11
cluster
technology_1
pattern_6
9092#
component_17
and
groupkafka
clientid
=
kafkaclienthisbackendkafka
=
kafkahisbackendconsumergroup#
topicskafka
topic
=
hl7
topickafka
quality_attribute_5
technology_15
=
nonekafka
technology_16
truststore
location
=
nonekafka
technology_16
truststore
password
=
nonekafka
technology_16
keystore
location
=
nonekafka
technology_16
keystore
password
=
nonethe
other
thing
do
by
prepare
development
env
sh
be
to
install
all
connector_17
need
by
the
telegram
requirement_1
and
the
his
component_18
requirement_1
technology_17
technology_18
component_1
now
please
run
this
command
prepare
development
env
shdeveloping
our
eda
locallyso
there
be
piece
we
need
to
be
able
to
run
locally
first
and
then
move
to
our
technology_3
cluster
namely
telegram
bothis
backendintegrations
hl7
to
and
to
telegram
bothis
frontendlet’s
run
them
test
them
one
by
onetelegram
botfirst
you
need
to
use
botfather
in
order
to
create
you
telegram
botgo
to
your
telegram
component_3
here
we’ll
show
you
haow
to
do
it
with
the
desktop
component_3
but
it
should
work
in
any
support
component_19
look
for
botfather
now
create
a
requirement_1
with
newbot
follow
the
step
when
do
you
should
connector_2
a
connector_data_10
please
copy
it
time
to
paste
this
connector_data_10
open
05a
run
telegram
requirement_1
sh
and
paste
it
when
require
run
this
script
in
an
terminal
window
be
aware
that
it
will
use
oc
port
connector_18
to
open
a
tunnel
with
the
component_6
run
in
technology_3
by
the
way
this
script
could
also
work
properly
completely
local
if
you
uncomment
the
require
line
use
technology_19
to
run
a
component_6
$
environment
sh$
05a
run
telegram
requirement_1
shpaste
connector_data_10
your_tokenusing
connector_data_10
your_token
telegram
bot@1
start
component_8
cvicensa
project
technology_3
tap
state
component_11
assistant
telegram
requirement_1
technology_8
component_3
jsnode
telegram
requirement_1
technology_6
deprecate
automatic
enabling
of
cancellation
of
promise
be
deprecate
in
the
future
you
will
have
to
enable
it
yourself
see
technology_13
technology_20
technology_14
yagop
technology_8
telegram
requirement_1
technology_6
issue
internal
cjs
loader
j
30dev
truebody
requirement_6
deprecate
bodyparser
use
individual
technology_21
urlencoded
middlewares
component_3
j
9body
requirement_6
deprecate
undefined
extend
provide
extend
option
node_modules
body
requirement_6
index
j
29health
assistant
requirement_1
have
start
start
conversation
in
your
telegram
telegram
requirement_1
start
at
tue
jan
gmt+0100
central
european
technology_22
time
on
port
9090error
connector_13
econnrefused
5432at
tcpconnectwrap
afterconnect
a
oncomplete
net
j
{errno
econnrefused
econnrefused
syscall
connector_13
connector_19
port
5432}forwarding
from
5432forwarding
from
5432hopefully
your
telegram
requirement_1
be
run
in
localhost
and
be
connector_20
to
technology_7
local
or
remote
let’s
test
it
first
open
your
telegram
component_3
and
look
for
your
requirement_1
in
our
requirement_2
state
machin
find
our
requirement_1
second
click
on
startas
you
can
see
some
help
be
display
create
a
component_8
in
our
requirement_2
component_8
already
exist
finally
connector_6
a
connector_data_2
and
see
the
connector_data_1
in
your
telegram
component_3
something
patient
john
smith
with
patid1234
have
be
admit
zzz
telegram
requirement_1
connector_6
connector_data_2
sh
9876543210w
technology_13
localhost
9090deploy
the
telegram
botbefore
we
can
run
the
requirement_3
pattern_2
we
need
to
quality_attribute_1
the
telegram
requirement_1
to
technology_3
info
this
be
so
because
the
requirement_3
pattern_2
run
in
the
cluster
so
it
would
be
require
for
your
local
telegram
requirement_1
to
be
listen
in
an
external
ip
reachable
from
the
clusterwe’re
go
to
quality_attribute_1
our
component_1
use
nodeshift
nodeshift
help
u
quality_attribute_1
our
nodejs
component_1
from
the
command
line
use
component_20
to
image
behind
scene
in
order
to
do
that
you
have
to
provide
minimal
connector_data_4
in
the
shape
of
technology_23
descriptor
in
a
folder
name
and
be
requirement_5
in
to
an
technology_3
cluster
here’s
a
connector_data_9
of
descriptor
already
prepare
for
quality_attribute_1
the
telegram
requirement_1
credential
secret
yml
be
a
proper
k8s
secret
connector_data_11
contain
the
component_6
credentialsroute
yml
be
a
fragment
of
a
connector_21
connector_data_11
that
connector_14
to
the
component_15
objectdeployment
yml
be
a
fragment
of
a
deploymentconfig
connector_data_11
you’ll
notice
that
there
be
some
environment
variable
there
for
component_6
component_21
literal
hardcoded
credential
requirement_4
come
from
the
secret
mention
above
and
the
telegram
connector_data_10
requirement_4
refer
to
a
configmap
hmm
be
you’ve
notice
some
thing
be
miss
here
where’s
the
configmap
to
connector_7
the
connector_data_10
from
it’ll
be
create
when
you
run
05b
quality_attribute_1
telegram
requirement_1
shwhere
be
the
component_15
imagestream
buildconfig
connector_data_11
those
be
infer
by
nodeshift
along
with
the
miss
piece
when
the
connector_data_11
be
not
complete
but
a
fragment
a
in
the
requirement_2
of
deployment
ymlnow
please
run
this
command
and
provide
the
telegram
connector_data_10
you
obtain
before
info
if
you
happen
to
forget
the
connector_data_10
you
can
always
go
to
botfater
and
ask
him
with
mybots
$
05b
quality_attribute_1
telegram
requirement_1
shpaste
connector_data_10
your_tokenusing
connector_data_10
your_token
telegram
bot@1
technology_3
component_8
cvicensa
project
technology_3
tap
state
component_11
assistant
telegram
requirement_1
nodeshift
strictssl=false
dockerimage=registry
connector_9
redhat
technology_14
rhoar
nodejs
nodejs
102020–01–15t11
945z
info
loading
configuration2020–01–15t11
952z
info
use
namespace
state
component_11
assistant
at
technology_13
technology_6
cluster
kharon
be2a
kharon
be2a
example
opentlc
technology_14
6443…2020–01–15t11
398z
trace
npm
info
ok2020–01–15t11
398z
trace…2020–01–15t11
393z
trace
connector_22
image
component_20
signatures2020–01–15t11
716z
trace
copy
blob
sha256
04dbaef9d44294aedd58690d85eb37e5b57dd98d9e922be36e4ae4e019b21619…2020–01–15t11
214z
trace
copy
config
sha256
1e16c354e3774215a388a4a0b94c1376c6f042f3e0965d1c7ac16972de8b7a1c2020–01–15t11
556z
trace
connector_23
manifest
to
image
destination2020–01–15t11
559z
trace
connector_24
signatures2020–01–15t11
957z
trace
1e16c354e3774215a388a4a0b94c1376c6f042f3e0965d1c7ac16972de8b7a1c2020–01–15t11
130z
trace2020–01–15t11
131z
trace
connector_25
image
image
registry
technology_3
image
registry
svc
state
component_11
assistant
telegram
requirement_1
late
…2020–01–15t11
132z
trace
connector_22
image
component_20
signatures2020–01–15t11
223z
trace
copy
blob
sha256
5af42566e7d1943de0196a7d22dc5abb18d916ae5cdb762dffd28d305a11ad41…2020–01–15t11
572z
trace
copy
config
sha256
1e16c354e3774215a388a4a0b94c1376c6f042f3e0965d1c7ac16972de8b7a1c2020–01–15t11
615z
trace
connector_23
manifest
to
image
destination2020–01–15t11
708z
trace
connector_24
signatures2020–01–15t11
710z
trace
successfully
connector_26
image
registry
technology_3
image
registry
svc
state
component_11
assistant
telegram
bot@sha256
f49ed9fbd0baf545ccbba03cf6246ba1ef29c647012d049fb3768e817599c9b32020–01–15t11
719z
trace
connector_26
successful2020–01–15t11
345z
info
build
telegram
requirement_1
s2i
complete2020–01–15t11
392z
info
technology_3
technology_23
and
technology_3
technology_21
connector_27
to
component_8
cvicensa
project
technology_3
tap
state
component_11
assistant
telegram
requirement_1
tmp
nodeshift
resource
2020–01–15t11
808z
info
create
configmap
telegram
bot2020–01–15t11
846z
info
create
secret
telegram
requirement_1
component_6
secret2020–01–15t11
849z
info
create
component_15
telegram
bot2020–01–15t11
855z
info
create
connector_21
telegram
bot2020–01–15t11
857z
info
create
deployment
configuration
telegram
bot2020–01–15t11
999z
info
connector_21
component_21
connector_data_12
telegram
requirement_1
state
component_11
assistant
component_22
cluster
kharon
be2a
kharon
be2a
example
opentlc
com2020–01–15t11
003z
info
completehis
backendnow
it’s
time
to
run
locally
our
technology_5
his
technology_6
in
order
to
do
so
we
run
run
backend
sh
open
a
terminal
and
run
it
info
this
script
run
our
component_1
which
connector_28
to
the
technology_1
topic
$hl7_events_topic
if
you’re
wonder
why
it
connector_28
to
the
technology_1
cluster
the
answer
be
this
environment
variable
kafka_service_host
when
run
locally
it’s
fill
with
the
connector_data_1
of
run
this
command
=$
oc
n
${project_name}
connector_7
connector_14
${cluster_name}
technology_1
bootstrap
o=jsonpath=
{
status
ingres
host}{
\n
}
when
run
in
technology_3
the
requirement_4
be
predefined
in
component_1
openshifr
property
and
equal
to
state
component_11
cluster
technology_1
pattern_6
run
backend
shif
it
all
work
properly
you
should
connector_7
something
this
info
restartedmain
o
a
k
component_17
component_23
consumerconfig
consumerconfig
requirement_4
auto
connector_29
interval
m
=
5000auto
offset
reset
=
latestbootstrap
component_13
=
state
component_11
cluster
technology_1
bootstrap
state
component_11
assistant
component_22
cluster
kharon
be2a
kharon
be2a
example
opentlc
technology_14
connector_15
crcs
=
trueclient
=connections
max
idle
m
=
540000default
technology_6
timeout
m
=
60000enable
auto
connector_29
=
falseexclude
internal
topic
=
truefetch
max
byte
=
52428800fetch
max
wait
m
=
500fetch
min
byte
=
1group
=
kafkahisbackendconsumergroupheartbeat
interval
m
=
3000interceptor
=
internal
leave
group
on
close
=
trueisolation
level
=
read_uncommittedkey
deserializer
=
technology_24
technology_1
common
serialization
longdeserializermax
component_24
fetch
byte
=
1048576max
pattern_7
interval
m
=
300000max
pattern_7
component_25
=
500metadata
max
age
m
=
300000metric
reporter
=
metric
num
sample
=
2metrics
component_25
level
=
infometrics
sample
window
m
=
30000partition
assignment
strategy
=
technology_24
technology_1
component_17
component_23
rangeassignor
connector_2
buffer
byte
=
65536reconnect
backoff
max
m
=
1000reconnect
backoff
m
=
50request
timeout
m
=
30000retry
backoff
m
=
100sasl
component_17
pattern_8
pattern_9
=
nullsasl
jaas
config
=
nullsasl
technology_25
kinit
cmd
=
usr
bin
kinitsasl
technology_25
min
time
before
relogin
=
60000sasl
technology_25
component_15
name
=
nullsasl
technology_25
ticket
renew
jitter
=
05sasl
technology_25
ticket
renew
window
factor
=
8sasl
login
pattern_8
pattern_9
=
nullsasl
login
=
nullsasl
login
refresh
buffer
second
=
300sasl
login
refresh
min
period
second
=
60sasl
login
refresh
window
factor
=
8sasl
login
refresh
window
jitter
=
05sasl
mechanism
=
gssapisecurity
technology_15
=
sslsend
buffer
byte
=
131072session
timeout
m
=
10000ssl
cipher
suite
=
nullssl
enable
technology_15
=
tlsv1
tlsv1
tlsv1
technology_16
identification
algorithm
=
httpsssl
key
password
=
nullssl
keymanager
algorithm
=
sunx509ssl
keystore
location
=
src
resource
keystore
jksssl
keystore
password
=
hide
technology_16
keystore
type
=
jksssl
technology_15
=
tlsssl
technology_26
=
nullssl
quality_attribute_4
random
implementation
=
nullssl
trustmanager
algorithm
=
pkixssl
truststore
location
=
src
resource
keystore
jksssl
truststore
password
=
hide
technology_16
truststore
type
=
jksvalue
deserializer
=
technology_24
technology_1
common
serialization
stringdeserializer2020
info
restartedmain
o
a
technology_1
common
utils
appinfoparser
technology_1
version
info
restartedmain
o
a
technology_1
common
utils
appinfoparser
technology_1
commitid
fa14705e51bd2ce5
init
with
topic
=
hl7
topic2020
info
restartedmain
o
s
s
concurrent
threadpooltaskexecutor
initialize
executorservice
applicationtaskexecutor
warn
restartedmain
awebconfiguration$jpawebmvcconfiguration
technology_5
technology_27
open
in
pattern_10
be
enable
by
default
therefore
component_6
connector_30
be
perform
during
pattern_10
render
explicitly
configure
technology_5
technology_27
open
in
pattern_10
to
disable
this
warning2020
info
component_26
technology_24
technology_1
component_17
metadata
cluster
jictoayzthcdpbnyvymlka2020
info
component_26
o
a
k
technology_28
technology_28
internals
abstractcoordinator
component_23
clientid=consumer
groupid=kafkahisbackendconsumergroup
discover
group
coordinator
state
component_11
cluster
technology_1
state
component_11
assistant
component_22
cluster
kharon
be2a
kharon
be2a
example
opentlc
technology_14
rack
info
restartedmain
o
s
b
a
w
s
welcomepagehandlermapping
welcome
component_27
path
resource
index
technology_29
info
component_26
o
a
k
technology_28
technology_28
internals
consumercoordinator
component_23
clientid=consumer
groupid=kafkahisbackendconsumergroup
revoke
previously
assign
component_24
info
component_26
o
a
k
technology_28
technology_28
internals
abstractcoordinator
component_23
clientid=consumer
groupid=kafkahisbackendconsumergroup
re
join
group2020
info
restartedmain
o
s
b
a
e
web
endpointlinksresolver
connector_8
s
beneath
base
path
actuator
info
restartedmain
o
s
b
w
embed
technology_30
tomcatwebserver
technology_31
start
on
port
s
technology_13
with
component_4
path
info
restartedmain
technology_14
redhat
his
boosterapplication
start
boosterapplication
in
second
technology_32
run
for
info
component_26
o
a
k
technology_28
technology_28
internals
abstractcoordinator
component_23
clientid=consumer
groupid=kafkahisbackendconsumergroup
successfully
join
group
with
generation
info
component_26
o
a
k
technology_28
technology_28
internals
consumercoordinator
component_23
clientid=consumer
groupid=kafkahisbackendconsumergroup
set
newly
assign
component_24
hl7
topic
let’s
run
some
test
for
instance
let’s
connector_7
all
the
patient
in
the
dabase
let’s
remember
that
when
run
locally
default
profile
be
use
and
technology_33
be
the
component_6
not
technology_7
please
run
this
command
from
a
different
terminal
$
curl
technology_13
localhost
technology_6
patient
{
patientid
personalid
0123456789z
firstname
john
lastname
smith
stage
idle
}
{
patientid
personalid
9876543210w
firstname
peter
lastname
jones
stage
idle
}
the
requirement_3
layerthe
requirement_3
pattern_2
comprise
two
technology_9
connector_31
two
technology_2
connector_14
hl7
to
which
connector_32
hl7
connector_data_2
pattern_3
them
to
human
readable
text
and
connector_6
them
to
a
different
topicevents
to
telegram
requirement_1
which
connector_32
and
pass
them
through
to
the
telegram
bottheses
be
the
relevant
line
of
of
hl7toeventsthere
be
one
connector_21
hl7
to
patient
info
that1
start
from
a
technology_1
topic
from
technology_1
{{kafka
from
topic}}
then
set
some
catch
for
exception
onexception
late
it
pattern_3
from
hl7
connector_data_6
use
the
quality_attribute_2
old
hapi
technology_34
component_28
exchange
finally
after
some
convertion
the
human
readable
connector_data_2
be
connector_33
to
another
topic
to
technology_1
{{kafka
to
topic}}
next
you
can
find
the
relevant
line
of
of
the
second
part
of
the
requirement_3
pattern_2
eventstotelegrambot
this
requirement_3
comprise
two
connector_14
the
1st
one
to
requirement_1
be
even
quality_attribute_6
again
it
start
from
a
topic
from
technology_1
{{kafka
from
topic}}
then
some
exception
handle
onexception
pass
the
connector_data_2
to
the
other
connector_21
to
direct
connector_6
to
requirement_1
the
2nd
connector_21
connector_6
to
bot1
set
some
to
configure
the
technology_13
component_2
setheader
connector_6
the
connector_data_2
to
the
telegram
requirement_1
use
the
technology_13
component_2
to
technology_13
{{telegram
requirement_1
host}}
{{telegram
requirement_1
port}}
connector_data_2
let’s
run
our
requirement_3
in
two
different
additional
terminal
window
07a
run
requirement_3
sh
in
charge
of
pattern_3
from
hl7
to
human
readable
text
events07b
run
requirement_3
sh
in
charge
of
connector_34
those
to
the
telegram
botso
please
in
a
different
terminal
run
$
07a
run
requirement_3
shand
yet
in
another
terminal
window
run
$
07b
run
requirement_3
shlet’s
run
a
test
that
generate
a
connector_3
in
the
status
of
a
patient
note
in
order
for
this
test
to
work
you
should
have
sign
up
the
personalid
you
should
have
do
this
before
when
test
the
telegram
requirement_1
curl
h
content
type
component_1
technology_21
x
put
\
technology_35
{
patientid
personalid
9876543210w
firstname
peter
lastname
jones
stage
admission
}
\http
localhost
technology_6
patient
2if
it
all
work
properly
no
error
should
be
connector_35
and
a
connector_data_2
should
be
connector_33
to
the
telegram
component_3
if
in
the
desktop
component_3
you’ll
see
this
then
in
the
telegram
component_3
you’ll
see
the
connector_data_2
so
for
now
we
have
his
backend
run
locally
both
requirement_3
run
in
technology_3
although
we
see
requirement_5
locally
and
the
telegram
requirement_1
run
also
in
technology_3
next
step
be
to
run
the
frontend
locally
point
to
the
backend
run
also
locally
his
frontendthis
time
we
have
to
run
a
nodejs
component_1
that
contain
the
of
an
angular
technology_18
with
material
design
along
with
a
pattern_11
that
connector_4
all
technology_6
patient
connector_data_13
to
technology_13
localhost
technology_6
patient
info
the
trick
be
in
the
script
dev
in
frontend
package
technology_21
that
run
in
paralell
component_17
and
component_13
component_17
u
pattern_11
config
component_13
conf
j
to
set
the
pattern_11
rule
open
and
connector_15
the
line
npm
run
dev
run
frontend
sh
frontend@0
dev
u
state
component_11
assistant
frontend
npx
concurrently
kill
others
npm
run
component_17
npm
run
component_13
now
open
a
browser
and
point
to
technology_13
localhost
you
should
see
something
this
now
a
reason
and
click
on
next
for
patient
peter
jones
9876543210w
this
way
you
connector_3
the
status
from
adminssion
to
triageif
you
go
to
he
backend
terminal
you
would
see
something
component_25
connector_33
to
component_24
with
offset
74on
requirement_3
7a
info
technology_2
technology_2
k
component_26
#1
kafkaconsumer
hl7
topic
hl7
to
patient
info
body
tvniff5+xcz8qurumxxnq018tefcqurufe1dtxwxotg4mdgxodexmjz8u0vdvvjjvfl8quruxkewohxnu0cwmdawmxxqfdiuna1fvk58qta4fdiwmduwmtewmdq1ntayfhx8fhwnuelefhwyfdk4nzy1ndmymtbxfhxkt05fu15qrvrfunx8mtk2mta2mtv8txx8mjewni0zfdeymdagtibfte0gu1rsruvuxl5hukvftlnct1jpxk5dxji3ndaxltewmjb8r0x8kdkxoskznzktmtixmnwoote5kti3ms0zndm0fig5mtkpmjc3ltmxmtr8ffn8fdk4nzy1ndmymtbxntawmv4yxk0xmhwxmjm0nty3odl8os04nzy1nf5oqw1qvjf8mxxjfevsxl5exl5eqnxvfhx8mzdetufsvelorvpesk9itl5exl5exkfjy01ncl5exl5dsxx8fdaxfhx8fdf8fhwzn15nqvjusu5fwl5kt0hoxl5exl5eqwnjtwdyxl5exknjfdj8ndawmdc3mtzexl5by2nnz3jevk58nhx8fhx8fhx8fhx8fhx8fhx8fhwxfhxhfhx8mjawntaxmtawnduyntn8fhx8fhwnr1qxfdexmjj8mtuxoxxkt0hoxkdbvevtxkensu4xfdawmxxbmzu3fdeymzr8qknnrhx8fhx8mtmyotg3dulomnxjrde1ntewmdf8u1nomtizndu2nzgn
encode
connector_data_2
tvniff5+xcz8qurumxxnq018tefcqurufe1dtxwxotg4mdgxodexmjz8u0vdvvjjvfl8quruxkewohxnu0cwmdawmxxqfdiuna1fvk58qta4fdiwmduwmtewmdq1ntayfhx8fhwnuelefhwyfdk4nzy1ndmymtbxfhxkt05fu15qrvrfunx8mtk2mta2mtv8txx8mjewni0zfdeymdagtibfte0gu1rsruvuxl5hukvftlnct1jpxk5dxji3ndaxltewmjb8r0x8kdkxoskznzktmtixmnwoote5kti3ms0zndm0fig5mtkpmjc3ltmxmtr8ffn8fdk4nzy1ndmymtbxntawmv4yxk0xmhwxmjm0nty3odl8os04nzy1nf5oqw1qvjf8mxxjfevsxl5exl5eqnxvfhx8mzdetufsvelorvpesk9itl5exl5exkfjy01ncl5exl5dsxx8fdaxfhx8fdf8fhwzn15nqvjusu5fwl5kt0hoxl5exl5eqwnjtwdyxl5exknjfdj8ndawmdc3mtzexl5by2nnz3jevk58nhx8fhx8fhx8fhx8fhx8fhx8fhwxfhxhfhx8mjawntaxmtawnduyntn8fhx8fhwnr1qxfdexmjj8mtuxoxxkt0hoxkdbvevtxkensu4xfdawmxxbmzu3fdeymzr8qknnrhx8fhx8mtmyotg3dulomnxjrde1ntewmdf8u1nomtizndu2nzgnin2|id1551001|ssn12345678||132987^john^^^^^^accmgr^^^^ci|||01||||1|||37^martinez^john^^^^^^accmgr^^^^ci|2|40007716^^^accmgr^vn|4|||||||||||||||||||1||g|||20050110045253||||||89|9
87654^nc
sendingapplication
adt1
hl7
adt
a08
on
requirement_3
7b
info
technology_2
technology_2
k
component_26
#2
kafkaconsumer
topic
to
requirement_1
connector_21
start
from
technology_1
topic
topic
info
technology_2
technology_2
k
component_26
#2
kafkaconsumer
topic
to
requirement_1
body
{
personalid
9876543210w
patientid
connector_data_2
patient
peter
jones
with
9876543210w
have
be
update
a08
in
black
mountain
}
info
technology_2
technology_2
k
component_26
#2
kafkaconsumer
topic
to
requirement_1
connector_34
connector_data_2
to
telegram
requirement_1
technology_13
telegram
requirement_1
technology_36
connector_data_2
{
personalid
9876543210w
patientid
connector_data_2
patient
peter
jones
with
9876543210w
have
be
update
a08
in
black
mountain
}
and
hopefully
you’ll
see
a
connector_data_3
and
the
whole
connector_data_2
a
before
so
for
now
we
have
his
frontend
and
backend
run
locally
both
requirement_3
run
in
technology_3
altough
we
see
requirement_5
locally
and
the
telegram
requirement_1
run
also
in
technology_3
next
step
be
to
run
quality_attribute_1
his
frontend
and
backend
quality_attribute_1
the
remain
component_29
to
openshiftdeploying
his
frontend
and
backendto
quality_attribute_1
the
frontend
a
we
do
before
for
the
telegram
requirement_1
we’re
go
to
use
nodeshift
on
the
other
hand
for
the
backend
we
cannot
use
nodeshift
because
be
a
technology_9
technology_37
component_3
but
no
worry
we
have
a
similar
technology_38
connector_5
fabric8
technology_37
plugin
for
now
run
this
command
quality_attribute_1
frontend
and
backend
sheventually
you’ll
see
in
project
your_project
workload
something
this
quality_attribute_1
technology_2
integrationsif
previously
we
have
two
script
to
run
the
requirement_3
hl7
to
and
to
telegram
requirement_1
now
we
only
have
one
quality_attribute_1
requirement_3
sh
the
other
difference
be
that
we
t
want
to
see
the
requirement_5
locally
so
no
need
for
the
dev
flag
any
more
this
be
the
relevant
piece
in
our
script
kamel
run
configmap=hl7
to
\
technology_35
technology_2
technology_39
technology_35
mvn
ca
uhn
hapi
hapi
base
technology_35
mvn
ca
uhn
hapi
hapi
connector_data_14
v24
\
requirement_3
hl7toevents
technology_9
kamel
run
configmap=events
to
requirement_1
requirement_3
eventstotelegrambot
javanow
please
run
the
script
your
should
connector_2
an
output
this
quality_attribute_1
requirement_3
shintegration
hl7to
createdintegration
to
telegram
requirement_1
createdfinal
testsnow
that
we
have
everything
quality_attribute_1
in
technology_3
it’s
time
to
do
the
final
test
we
only
need
the
connector_21
of
our
his
frontend
component_1
you
can
connector_7
it
by
run
this
command
you
can
also
use
the
web
console
and
go
to
project
your_project
workload
click
on
frontend
then
look
for
resource
routesoc
connector_7
connector_21
frontend
n
$project_nameopen
a
browser
and
connector_3
the
status
of
peter
jones
at
will
you
should
expect
the
same
connector_data_15
a
before
follow
u
on
twitter
🐦
and
👥
and
technology_40
📷
and
join
our
and
linkedin
group
💬
to
join
our
slack
team
chat
🗣️
connector_1
our
weekly
faun
topic
🗞️
and
connector_13
with
the
📣
click
here⬇if
this
be
helpful
please
click
the
clap
👏
below
a
few
time
to
show
your
support
for
the
author
⬇
1more
from
faun
publicationfollowthe
must
connector_1
publication
for
creative
developer
join
the
www
faun
devread
more
from
faun
publicationabouthelptermsprivacyget
the
appget
startedcarlos
vicens111
followersmobile
tech
lover
programming
technology_41
collector
open_source
believer
redhatterfollowmore
from
mediumstefanie
laiincodexready
to
win
the
cka
certificatevighnesh
manjrekarindev
geniusgit
warn
lf
will
be
replace
by
crlfasad
faiziwhen
to
use
technology_4
ruchira
sahabanduimplementing
technology_42
login
for
technology_42
linux
vms
use
tf
deployment
helpstatuswritersblogcareersprivacytermsaboutknowable
