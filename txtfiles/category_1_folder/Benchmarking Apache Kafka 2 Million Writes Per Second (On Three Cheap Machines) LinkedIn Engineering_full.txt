benchmarking
technology_1
technology_2
million
connector_1
per
second
on
three
cheap
component_1
|
linkedin
engineering
linkedin
engineering
engineering
home
connector_data_1
open_source
trust
infrastructure
search
clear
search
input
cancel
dismiss
this
connector_data_2
benchmarking
technology_1
technology_2
million
connector_1
per
second
on
three
cheap
component_1
jay
kreps
connector_2
i
connector_3
a
about
how
linkedin
u
technology_1
technology_2
a
a
central
pattern_1
requirement_1
for
quality_attribute_1
connector_data_1
between
component_2
connector_4
component_3
and
technology_3
connector_data_1
ingestion
to
actually
make
this
work
though
this
universal
requirement_1
have
to
be
a
cheap
abstraction
if
you
want
to
use
a
component_4
a
a
central
connector_data_1
hub
it
have
to
be
fast
quality_attribute_2
and
easy
to
quality_attribute_3
so
you
can
connector_data_3
all
your
connector_data_1
onto
it
my
experience
have
be
that
component_5
that
be
fragile
or
expensive
inevitably
develop
a
wall
of
protective
component_3
to
prevent
people
from
use
them
a
component_4
that
quality_attribute_3
easily
often
end
up
a
a
key
architectural
build
block
because
use
it
be
the
easy
way
to
connector_5
thing
build
i
ve
always
the
benchmark
of
technology_4
that
show
it
do
a
million
connector_1
per
second
on
three
hundred
component_6
on
technology_5
and
compute
component_7
i
m
not
sure
why
maybe
it
be
a
dr
evil
thing
but
do
a
million
of
anything
per
second
be
fun
in
any
requirement_2
one
of
the
nice
thing
about
a
technology_2
requirement_1
be
that
a
we
ll
see
it
be
cheap
a
million
connector_1
per
second
isn
t
a
particularly
big
thing
this
be
because
a
requirement_1
be
a
much
quality_attribute_4
thing
than
a
component_8
or
key
requirement_3
component_9
indeed
our
production
cluster
take
ten
of
million
of
connector_6
and
connector_1
per
second
all
day
long
and
they
do
so
on
pretty
modest
hardware
but
s
do
some
benchmarking
and
take
a
look
technology_2
in
second
to
help
understand
the
benchmark
me
give
a
quick
review
of
what
technology_2
be
and
a
few
detail
about
how
it
work
technology_2
be
a
quality_attribute_5
pattern_2
component_4
originally
build
at
linkedin
and
now
part
of
the
technology_1
foundation
and
use
by
a
variety
of
requirement_4
the
general
setup
be
quite
quality_attribute_4
component_10
connector_7
component_11
to
the
cluster
which
hold
on
to
these
component_11
and
hand
them
out
to
component_12
the
key
abstraction
in
technology_2
be
the
topic
component_10
publish
their
component_11
to
a
topic
and
component_13
subscribe
to
one
or
more
topic
a
technology_2
topic
be
a
sharded
connector_3
ahead
requirement_1
component_10
append
component_11
to
these
requirement_1
and
component_13
subscribe
to
connector_8
each
component_14
be
a
key
requirement_3
pair
the
key
be
use
for
assign
the
component_14
to
a
requirement_1
component_15
unless
the
pattern_3
specify
the
component_15
directly
here
be
a
quality_attribute_4
example
of
a
single
component_16
and
component_12
connector_9
and
connector_10
from
a
two
component_15
topic
this
picture
show
a
component_16
component_3
append
to
the
requirement_1
for
the
two
component_15
and
a
component_12
connector_9
from
the
same
requirement_1
each
component_14
in
the
requirement_1
have
an
associate
entry
number
that
we
connector_data_4
the
offset
this
offset
be
use
by
the
component_12
to
describe
it
s
position
in
each
of
the
requirement_1
these
component_15
be
spread
across
a
cluster
of
component_1
allow
a
topic
to
hold
more
connector_data_1
than
can
fit
on
any
one
component_1
note
that
unlike
most
pattern_2
component_5
the
requirement_1
be
always
persistent
connector_data_5
be
immediately
connector_3
to
the
filesystem
when
they
be
connector_11
connector_data_5
be
not
delete
when
they
be
connector_12
but
retain
with
some
quality_attribute_6
sla
say
a
few
day
or
a
week
this
allow
usage
in
situation
where
the
component_12
of
connector_data_1
need
to
reload
connector_data_1
it
also
make
it
possible
to
support
space
quality_attribute_7
pattern_1
a
there
be
a
single
connector_13
requirement_1
no
matter
how
many
component_12
in
traditional
pattern_2
component_5
there
be
usually
a
component_17
per
component_12
so
a
component_12
double
your
connector_data_1
size
this
make
technology_2
a
quality_attribute_8
fit
for
thing
outside
the
bind
of
normal
pattern_2
component_5
such
a
act
a
a
pipeline
for
offline
connector_data_1
component_5
such
a
technology_3
these
offline
component_5
load
only
at
interval
a
part
of
a
periodic
technology_6
cycle
or
go
down
for
several
hour
for
quality_attribute_9
during
which
time
technology_2
be
able
to
buffer
even
tb
of
unconsumed
connector_data_1
if
need
technology_2
also
replicate
it
requirement_1
over
multiple
component_18
for
fault
tolerance
one
important
architectural
aspect
of
our
pattern_4
implementation
in
contrast
to
other
pattern_2
component_4
be
that
pattern_4
be
not
an
exotic
bolt
on
that
require
complex
configuration
only
to
be
use
in
very
specialize
requirement_2
instead
pattern_4
be
assume
to
be
the
default
we
treat
un
replicate
connector_data_1
a
a
special
requirement_2
where
the
pattern_4
factor
happen
to
be
one
component_10
connector_5
an
acknowledgement
back
when
they
publish
a
connector_data_2
contain
the
component_14
s
offset
the
first
component_14
publish
to
a
component_15
be
give
the
offset
the
second
component_14
and
so
on
in
an
ever
increasing
sequence
component_13
connector_14
connector_data_1
from
a
position
specify
by
an
offset
and
they
connector_15
their
position
in
a
requirement_1
by
connector_16
periodically
connector_17
this
offset
in
requirement_2
that
component_12
instance
crash
and
another
instance
need
to
resume
from
it
s
position
okay
hopefully
that
all
make
sense
if
not
you
can
connector_12
a
more
complete
introduction
to
technology_2
here
this
benchmark
this
test
be
against
trunk
a
i
make
some
improvement
to
the
requirement_5
test
for
this
benchmark
but
nothing
too
substantial
have
connector_8
since
the
last
full
release
so
you
should
see
similar
connector_data_6
with
i
be
also
use
our
newly
re
connector_3
technology_7
component_16
which
offer
much
improve
quality_attribute_10
over
the
previous
component_16
component_19
i
ve
follow
the
basic
template
of
this
very
nice
technology_8
benchmark
but
i
cover
scenario
and
option
that
be
more
relevant
to
technology_2
one
quick
philosophical
note
on
this
benchmark
for
benchmark
that
be
go
to
be
publicly
report
i
to
follow
a
style
i
connector_data_4
lazy
benchmarking
when
you
work
on
a
component_4
you
generally
have
the
how
to
tune
it
to
perfection
for
any
particular
use
requirement_2
this
lead
to
a
kind
of
benchmarketing
where
you
heavily
tune
your
configuration
to
your
benchmark
or
bad
have
a
different
tune
for
each
scenario
you
test
i
think
the
real
test
of
a
component_4
be
not
how
it
perform
when
perfectly
tune
but
rather
how
it
perform
off
the
shelf
this
be
particularly
true
for
component_5
that
run
in
a
pattern_5
setup
with
dozen
or
hundred
of
use
requirement_2
where
tune
for
each
use
requirement_2
would
be
not
only
impractical
but
impossible
a
a
connector_data_7
i
have
pretty
much
stick
with
default
set
both
for
the
component_20
and
the
component_19
i
will
point
out
area
where
i
suspect
the
connector_data_7
could
be
improve
with
a
little
tune
but
i
have
try
to
resist
the
temptation
to
do
any
fiddle
myself
to
improve
the
connector_data_7
i
have
my
exact
configuration
and
command
so
it
should
be
possible
to
replicate
connector_data_6
on
your
own
gear
if
you
be
interest
the
setup
for
these
test
i
have
six
component_6
each
have
the
follow
spec
intel
xeon
ghz
processor
with
six
core
six
rpm
sata
drive
32gb
of
ram
1gb
ethernet
the
technology_2
cluster
be
set
up
on
three
of
the
component_1
the
six
drive
be
directly
mount
with
no
raid
jbod
style
the
remain
three
component_6
i
use
for
technology_9
and
for
generate
load
a
three
component_1
cluster
isn
t
very
big
but
since
we
will
only
be
test
up
to
a
pattern_4
factor
of
three
it
be
all
we
need
a
should
be
obvious
we
can
always
more
component_15
and
spread
connector_data_1
onto
more
component_6
to
quality_attribute_3
our
cluster
horizontally
this
hardware
be
actually
not
linkedin
s
normal
technology_2
hardware
our
technology_2
component_6
be
more
closely
tune
to
run
technology_2
but
be
le
in
the
spirit
of
off
the
shelf
i
be
aim
for
with
these
test
instead
i
borrow
these
from
one
of
our
technology_3
cluster
which
run
on
probably
the
cheap
gear
of
any
of
our
persistent
component_4
technology_3
usage
pattern_6
be
pretty
similar
to
technology_2
s
so
this
be
a
reasonable
thing
to
do
okay
without
further
technology_10
the
connector_data_7
component_16
quality_attribute_10
these
test
will
stress
the
quality_attribute_10
of
the
component_16
no
component_13
be
run
during
these
test
so
all
connector_data_5
be
persist
but
not
connector_12
we
ll
test
requirement_2
with
both
component_16
and
component_12
in
a
bit
since
we
have
recently
rewrite
our
component_16
i
be
test
this
single
component_16
component_21
no
pattern_4
component_14
sec
connector_data_8
sec
for
this
first
test
i
create
a
topic
with
six
component_15
and
no
pattern_4
then
i
produce
million
small
byte
component_11
a
quickly
a
possible
from
a
single
component_21
the
reason
for
focus
on
small
component_11
in
these
test
be
that
it
be
the
hard
requirement_2
for
a
pattern_2
component_4
generally
it
be
easy
to
connector_5
quality_attribute_8
quality_attribute_10
in
connector_data_8
sec
if
the
connector_data_5
be
large
but
much
hard
to
connector_5
quality_attribute_8
quality_attribute_10
when
the
connector_data_5
be
small
a
the
overhead
of
component_3
each
connector_data_2
dominate
throughout
this
benchmark
when
i
be
report
connector_data_8
sec
i
be
report
the
requirement_3
size
of
the
component_14
time
the
connector_data_9
per
second
none
of
the
other
overhead
of
the
connector_data_9
be
include
so
the
actually
requirement_6
usage
be
high
than
what
be
report
for
example
with
a
byte
connector_data_2
we
would
also
connector_18
about
byte
of
overhead
per
connector_data_2
for
an
optional
key
size
delimit
a
connector_data_2
crc
the
component_14
offset
and
attribute
flag
a
well
a
some
overhead
for
the
connector_data_9
include
the
topic
component_15
require
acknowledgement
etc
this
make
it
a
little
hard
to
see
where
we
hit
the
limit
of
the
nic
but
this
seem
a
little
more
reasonable
then
include
our
own
overhead
byte
in
quality_attribute_10
number
so
in
the
above
connector_data_7
we
be
likely
saturate
the
gigabit
nic
on
the
component_19
component_1
one
immediate
observation
be
that
the
raw
number
here
be
much
high
than
people
expect
especially
for
a
persistent
storage
component_4
if
you
be
use
to
random
connector_19
connector_data_1
component_4
a
component_8
or
key
requirement_3
component_9
you
will
generally
expect
maximum
quality_attribute_10
around
to
query
per
second
a
this
be
close
to
the
quality_attribute_11
that
a
quality_attribute_8
pattern_7
pattern_8
can
do
remote
connector_data_9
we
exceed
this
due
to
two
key
design
principle
we
work
hard
to
ensure
we
do
linear
disk
i
o
the
six
cheap
disk
these
component_18
have
give
an
aggregate
quality_attribute_10
of
connector_data_8
sec
of
linear
disk
i
o
this
be
actually
well
beyond
what
we
can
make
use
of
with
only
a
gigabit
requirement_6
card
many
pattern_2
component_5
treat
persistence
a
an
expensive
on
that
decimate
requirement_5
and
should
be
use
only
sparingly
but
this
be
because
they
be
not
able
to
do
linear
i
o
at
each
stage
we
work
on
pattern_9
together
small
bit
of
connector_data_1
into
large
requirement_6
and
disk
i
o
for
example
in
the
component_16
we
use
a
group
connector_20
mechanism
to
ensure
that
any
component_14
connector_21
initiate
while
another
i
o
be
in
progress
connector_5
group
together
for
more
on
understand
the
importance
of
pattern_9
connector_22
out
this
presentation
by
david
patterson
on
why
quality_attribute_12
lag
bandwidth
if
you
be
interest
in
the
detail
you
can
connector_12
a
little
more
about
this
in
our
design
document
single
component_16
component_21
3x
pattern_10
pattern_4
component_14
sec
connector_data_8
sec
this
test
be
exactly
the
same
a
the
previous
one
except
that
now
each
component_15
have
three
replica
so
the
total
connector_data_1
connector_3
to
requirement_6
or
disk
be
three
time
high
each
component_20
be
do
both
connector_1
from
the
component_16
for
the
component_15
for
which
it
be
a
master
a
well
a
fetch
and
connector_10
connector_data_1
for
the
component_15
for
which
it
be
a
follower
pattern_4
in
this
test
be
pattern_10
that
be
the
component_20
acknowledge
the
connector_3
a
soon
a
it
have
connector_3
it
to
it
local
requirement_1
without
wait
for
the
other
replica
to
also
acknowledge
it
this
mean
if
the
master
be
to
crash
it
would
likely
lose
the
last
few
connector_data_5
that
have
be
connector_3
but
not
yet
replicate
this
make
the
connector_data_2
acknowledgement
quality_attribute_12
a
little
quality_attribute_8
at
the
cost
of
some
risk
in
the
requirement_2
of
component_20
failure
the
key
take
away
i
would
people
to
have
from
this
be
that
pattern_4
can
be
fast
the
total
cluster
connector_3
capacity
be
of
3x
le
with
3x
pattern_4
since
each
connector_3
be
do
three
time
but
the
quality_attribute_10
be
still
quite
quality_attribute_8
per
component_19
high
requirement_5
pattern_4
come
in
large
part
from
the
quality_attribute_13
of
our
component_12
the
replica
be
really
nothing
more
than
a
specialize
component_12
which
i
will
discus
in
the
component_12
section
single
component_16
component_21
3x
pattern_11
pattern_4
component_14
sec
connector_data_8
sec
this
test
be
the
same
a
above
except
that
now
the
master
for
a
component_15
wait
for
acknowledgement
from
the
full
set
of
in
pattern_12
replica
before
acknowledge
back
to
the
component_16
in
this
mode
we
guarantee
that
connector_data_5
will
not
be
lose
a
long
a
one
in
pattern_12
replica
remain
pattern_11
pattern_4
in
technology_2
be
not
fundamentally
very
different
from
pattern_10
pattern_4
the
leader
for
a
component_15
always
track
the
progress
of
the
follower
replica
to
pattern_13
their
liveness
and
we
never
give
out
connector_data_5
to
component_13
until
they
be
fully
acknowledge
by
replica
with
pattern_11
pattern_4
we
wait
to
respond
to
the
component_16
connector_data_9
until
the
follower
have
replicate
it
this
additional
quality_attribute_12
do
seem
to
affect
our
quality_attribute_10
since
the
path
on
the
component_20
be
very
similar
we
could
probably
ameliorate
this
impact
by
tune
the
pattern_9
to
be
a
bit
more
aggressive
and
allow
the
component_19
to
buffer
more
outstanding
connector_data_9
however
in
spirit
of
avoid
special
requirement_2
tune
i
have
avoid
this
three
component_16
3x
pattern_14
pattern_4
component_14
sec
connector_data_8
sec
our
single
component_16
component_3
be
clearly
not
stress
our
three
technology_11
cluster
to
a
little
more
load
i
ll
now
repeat
the
previous
pattern_14
pattern_4
test
but
now
use
three
component_16
load
generator
run
on
three
different
component_6
run
more
component_22
on
the
same
component_1
win
t
help
a
we
be
saturate
the
nic
then
we
can
look
at
the
aggregate
quality_attribute_10
across
these
three
component_10
to
connector_5
a
quality_attribute_8
feel
for
the
cluster
s
aggregate
capacity
component_16
quality_attribute_10
versus
component_9
connector_data_1
one
of
the
hide
danger
of
many
pattern_2
component_5
be
that
they
work
well
only
a
long
a
the
connector_data_1
they
retain
fit
in
memory
their
quality_attribute_10
fall
by
an
order
of
magnitude
or
more
when
connector_data_1
back
up
and
isn
t
connector_14
and
hence
need
to
be
component_9
on
disk
this
mean
thing
be
run
fine
a
long
a
your
component_13
keep
up
and
the
component_17
be
empty
but
a
soon
a
they
lag
the
whole
pattern_2
pattern_8
back
up
with
unconsumed
connector_data_1
the
backup
cause
connector_data_1
to
go
to
disk
which
in
turn
cause
requirement_5
to
drop
to
a
rate
that
mean
pattern_2
component_4
can
no
long
keep
up
with
incoming
connector_data_1
and
either
back
up
or
fall
over
this
be
pretty
terrible
a
in
many
requirement_2
the
whole
purpose
of
the
component_17
be
to
handle
such
a
requirement_2
gracefully
since
technology_2
always
persist
connector_data_5
the
requirement_5
be
o
with
respect
to
unconsumed
connector_data_1
volume
to
test
this
experimentally
s
run
our
quality_attribute_10
test
over
an
extend
period
of
time
and
graph
the
connector_data_6
a
the
component_9
dataset
grow
this
graph
actually
do
show
some
variance
in
requirement_5
but
no
impact
due
to
connector_data_1
size
we
perform
a
well
after
connector_10
a
tb
of
connector_data_1
a
we
do
for
the
first
few
hundred
connector_data_8
the
variance
seem
to
be
due
to
linux
s
i
o
requirement_7
facility
that
pattern_9
connector_data_1
and
then
flush
it
periodically
this
be
something
we
have
tune
for
a
little
quality_attribute_8
on
our
production
technology_2
setup
some
note
on
tune
i
o
be
quality_attribute_14
here
component_12
quality_attribute_10
okay
now
s
turn
our
attention
to
component_12
quality_attribute_10
note
that
the
pattern_4
factor
will
not
effect
the
outcome
of
this
test
a
the
component_12
only
connector_6
from
one
replica
regardless
of
the
pattern_4
factor
likewise
the
acknowledgement
level
of
the
component_16
also
doesn
t
matter
a
the
component_12
only
ever
connector_6
fully
acknowledge
connector_data_2
even
if
the
component_16
doesn
t
wait
for
full
acknowledgement
this
be
to
ensure
that
any
connector_data_2
the
component_12
see
will
always
be
present
after
a
leadership
handoff
if
the
current
leader
fail
single
component_12
component_14
sec
connector_data_8
sec
for
the
first
test
we
will
connector_14
million
connector_data_5
in
a
single
component_21
from
our
component_15
3x
replicate
topic
technology_2
s
component_12
be
very
quality_attribute_7
it
work
by
fetch
chunk
of
requirement_1
directly
from
the
filesystem
it
u
the
sendfile
component_23
to
transfer
this
directly
through
the
operate
component_4
without
the
overhead
of
copy
this
connector_data_1
through
the
component_2
this
test
actually
start
at
the
begin
of
the
requirement_1
so
it
be
do
real
connector_12
i
o
in
a
production
set
though
the
component_12
connector_6
almost
exclusively
out
of
the
o
pagecache
since
it
be
connector_9
connector_data_1
that
be
connector_3
by
some
component_16
so
it
be
still
pattern_15
in
fact
if
you
run
i
o
stat
on
a
production
component_20
you
actually
see
that
there
be
no
physical
connector_6
at
all
even
though
a
great
deal
of
connector_data_1
be
be
connector_14
make
component_13
cheap
be
important
for
what
we
want
technology_2
to
do
for
one
thing
the
replica
be
themselves
component_12
so
make
the
component_12
cheap
make
pattern_4
cheap
in
addition
this
make
handle
out
connector_data_1
an
inexpensive
and
hence
not
something
we
need
to
tightly
control
for
quality_attribute_15
reason
three
component_13
component_14
sec
connector_data_8
sec
s
repeat
the
same
test
but
run
three
parallel
component_12
component_3
each
on
a
different
component_1
and
all
connector_23
the
same
topic
a
expect
we
see
near
linear
quality_attribute_16
not
surprise
because
consumption
in
our
component_24
be
so
quality_attribute_4
component_16
and
component_12
component_14
sec
connector_data_8
sec
the
above
test
cover
the
component_16
and
the
component_12
run
in
isolation
now
s
do
the
natural
thing
and
run
them
together
actually
we
have
technically
already
be
do
this
since
our
pattern_4
work
by
have
the
component_18
themselves
act
a
component_12
all
the
same
s
run
the
test
for
this
test
we
ll
run
one
component_16
and
one
component_12
on
a
six
component_15
3x
replicate
topic
that
begin
empty
the
component_16
be
again
use
pattern_14
pattern_4
the
quality_attribute_10
report
be
the
component_12
quality_attribute_10
which
be
obviously
an
upper
bind
on
the
component_16
quality_attribute_10
a
we
would
expect
the
connector_data_6
we
connector_5
be
basically
the
same
a
we
saw
in
the
component_16
only
case—the
component_12
be
fairly
cheap
effect
of
connector_data_2
size
i
have
mostly
show
requirement_5
on
small
byte
connector_data_2
small
connector_data_5
be
the
hard
problem
for
a
pattern_2
component_4
a
they
magnify
the
overhead
of
the
bookkeeping
the
component_4
do
we
can
show
this
by
graph
quality_attribute_10
in
both
component_14
second
and
connector_data_8
second
a
we
vary
the
component_14
size
so
a
we
would
expect
this
graph
show
that
the
raw
count
of
component_11
we
can
connector_7
per
second
decrease
a
the
component_11
connector_5
big
but
if
we
look
at
connector_data_8
second
we
see
that
the
total
byte
quality_attribute_10
of
real
component_25
connector_data_1
increase
a
connector_data_5
connector_5
big
we
can
see
that
with
the
byte
connector_data_5
we
be
actually
cpu
bind
by
acquire
the
lock
and
enqueuing
the
connector_data_2
for
sending—we
be
not
able
to
actually
max
out
the
requirement_6
however
start
with
byte
we
be
actually
see
requirement_6
saturation
though
the
connector_data_8
sec
continue
to
increase
a
our
fix
size
bookkeeping
byte
become
an
increasingly
small
percentage
of
the
total
byte
connector_7
end
to
end
quality_attribute_12
m
median
m
99th
percentile
m
9th
percentile
we
have
talk
a
lot
about
quality_attribute_10
but
what
be
the
quality_attribute_12
of
connector_data_2
delivery
that
be
how
long
do
it
take
a
connector_data_2
we
connector_7
to
be
connector_24
to
the
component_12
for
this
test
we
will
create
component_16
and
component_12
and
repeatedly
time
how
long
it
take
for
a
component_16
to
connector_7
a
connector_data_2
to
the
technology_2
cluster
and
then
be
connector_25
by
our
component_12
note
that
technology_2
only
give
out
connector_data_5
to
component_13
when
they
be
acknowledge
by
the
full
in
pattern_12
set
of
replica
so
this
test
will
give
the
same
connector_data_6
regardless
of
whether
we
use
pattern_12
or
pattern_14
pattern_4
a
that
set
only
affect
the
acknowledgement
to
the
component_16
replicate
this
test
if
you
want
to
try
out
these
benchmark
on
your
own
component_1
you
can
a
i
say
i
mostly
use
our
pre
packaged
requirement_5
test
technology_12
that
ship
with
technology_2
and
mostly
stick
with
the
default
configs
both
for
the
component_20
and
for
the
component_19
however
you
can
see
more
detail
of
the
configuration
and
command
here
topic
requirement_5
technology_2
quality_attribute_5
component_5
relate
storybuilding
a
blazingly
fast
technology_13
component_26
part
1related
storygobblin
gobble
camus
look
towards
the
future
back
to
toplinkedin
technology_14
connector_data_1
open_source
trust
infrastructure
linkedin
corporation
©
about
requirement_8
privacy
requirement_8
component_25
agreement
quality_attribute_17
linkedin
twitter
youtube
technology_15
dismiss
