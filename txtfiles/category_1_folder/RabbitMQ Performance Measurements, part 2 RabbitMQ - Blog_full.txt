technology_1
requirement_1
measurement
part
|
technology_1
featuresget
startedsupportcommunitydocsblog«
london
realtime
hackweekendsome
pattern_1
theory
quality_attribute_1
quality_attribute_2
and
bandwidth
»rabbitmq
requirement_1
measurement
part
2tweet
follow
@rabbitmqapril
2012welcome
back
last
time
we
talk
about
flow
control
and
quality_attribute_2
today
let’s
talk
about
how
different
feature
affect
the
requirement_1
we
see
here
be
some
quality_attribute_3
scenario
a
before
they’re
all
variation
on
the
theme
of
one
pattern_2
and
one
component_1
publish
a
fast
a
they
can
some
quality_attribute_3
scenariosthis
first
scenario
be
the
quality_attribute_3
one
component_2
and
one
component_1
so
we
have
a
baseline
of
we
want
to
produce
impressive
figure
so
we
can
go
a
bit
fast
than
that
if
we
don’t
connector_1
anything
then
we
can
publish
fast
this
u
a
couple
of
the
core
on
our
component_3
but
not
all
of
them
so
for
the
best
headline
connector_2
rate
we
start
a
number
of
parallel
component_2
all
publish
into
nothing
of
connector_3
be
rather
important
so
for
the
headline
connector_3
rate
we
publish
to
a
large
number
of
component_4
in
parallel
of
to
some
extent
this
quest
for
large
number
be
a
bit
silly
we’re
more
interest
in
relative
requirement_1
so
let’s
revert
to
one
component_2
and
one
component_1
now
let’s
try
publish
with
the
mandatory
flag
set
we
drop
to
about
40%
of
the
non
mandatory
rate
the
reason
for
this
be
that
the
pattern_3
we’re
publish
to
can’t
asynchronously
connector_4
connector_data_1
at
component_5
any
more
it
synchronously
connector_5
with
the
component_5
to
make
sure
they’re
still
there
yes
we
could
probably
make
mandatory
publish
fast
but
it’s
not
very
heavily
use
the
immediate
flag
give
u
almost
exactly
the
same
drop
in
requirement_1
this
isn’t
hugely
surprise
it
have
to
make
the
same
pattern_4
connector_6
with
the
component_6
scrap
the
rarely
use
mandatory
and
immediate
flag
let’s
try
turn
on
acknowledgement
for
connector_7
connector_data_2
we
still
see
a
requirement_1
drop
compare
to
connector_8
without
acknowledgement
the
component_3
have
to
do
more
bookkeeping
after
all
but
it’s
le
noticeable
now
we
turn
on
publish
confirm
a
well
requirement_1
drop
a
little
more
but
we’re
still
at
over
60%
the
quality_attribute_4
of
neither
acks
nor
confirm
finally
we
enable
connector_data_2
persistence
the
rate
become
much
lower
since
we’re
throw
all
those
connector_data_1
at
the
disk
a
well
connector_data_2
sizesnotably
all
the
connector_data_1
we’ve
be
connector_9
until
now
have
only
be
a
few
byte
long
there
be
a
couple
of
reason
for
this
quite
a
lot
of
the
work
do
by
technology_1
be
per
connector_data_2
not
per
byte
of
connector_data_2
it’s
always
nice
to
look
at
big
number
but
in
the
real
world
we
will
often
want
to
connector_10
big
connector_data_2
so
let’s
look
at
the
next
requirement_2
connector_9
rate
connector_data_2
sizeshere
again
we’re
connector_9
unacked
unconfirmed
connector_data_1
a
fast
a
possible
but
this
time
we
vary
the
connector_data_2
size
we
can
see
that
of
the
connector_data_2
rate
drop
further
a
the
size
increase
but
the
actual
number
of
byte
connector_11
increase
a
we
have
le
and
le
connector_12
overhead
so
how
do
the
connector_data_2
size
affect
horizontal
quality_attribute_5
let’s
vary
the
number
of
component_7
with
different
connector_data_2
size
for
a
connector_13
in
this
test
we’re
not
go
to
have
any
component_4
ar
all
n
connector_9
msg
rate
vs
number
of
component_2
for
various
connector_data_2
sizesn
connector_9
byte
rate
vs
number
of
component_2
for
various
connector_data_2
sizesin
these
test
we
can
see
that
for
small
connector_data_1
it
only
take
a
couple
of
component_7
to
reach
an
upper
bind
on
how
many
connector_data_1
we
can
publish
but
that
for
large
connector_data_1
we
need
more
component_7
to
use
the
quality_attribute_6
bandwidth
another
frequently
confuse
issue
be
requirement_1
around
component_4
with
a
prefetch
count
technology_1
well
technology_2
default
to
connector_9
all
the
connector_data_1
it
can
to
any
component_1
that
look
ready
to
connector_14
them
the
maximum
number
of
these
unacknowledged
connector_data_1
per
pattern_3
can
be
limit
by
set
the
prefetch
count
however
small
prefetch
count
can
hurt
requirement_1
since
we
can
be
wait
for
acks
to
arrive
before
connector_9
out
more
connector_data_2
so
let’s
have
a
look
at
prefetch
count
and
while
we’re
there
also
consider
the
number
of
component_4
connector_3
from
a
single
component_6
this
requirement_2
contain
some
deliberately
absurd
extreme
n
connector_15
rate
vs
component_1
count
prefetch
countthe
first
thing
to
notice
be
that
tiny
prefetch
count
really
hurt
requirement_1
note
the
large
difference
in
requirement_1
between
prefetch
=
and
prefetch
=
but
we
also
connector_16
into
diminish
notice
that
the
difference
between
prefetch
=
and
prefetch
=
be
hard
to
see
and
the
difference
between
prefetch
=
and
prefetch
=
be
almost
invisible
of
this
be
because
for
our
particular
requirement_3
connector_17
prefetch
=
already
ensure
that
we
never
starve
the
component_1
while
wait
for
acks
of
this
test
be
run
over
a
low
quality_attribute_2
connector_17
more
latent
connector_18
will
benefit
from
a
high
prefetch
count
the
second
thing
to
notice
be
that
when
we
have
a
small
number
of
component_1
one
more
will
increase
requirement_1
we
connector_16
more
parallellism
and
with
a
tiny
prefetch
count
increasing
component_4
even
up
to
a
large
number
have
benefit
since
each
individual
component_1
spend
much
of
it
time
starve
but
when
we
have
a
large
prefetch
count
increasing
the
number
of
component_4
be
not
so
helpful
since
even
a
small
number
can
keep
busy
enough
to
max
out
our
component_6
but
the
more
component_4
we
have
the
more
work
technology_1
have
to
do
to
keep
track
of
all
of
them
large
queuesall
the
example
we’ve
look
at
so
far
have
one
thing
in
common
very
few
connector_data_1
actually
connector_16
component_6
in
general
we’ve
look
at
scenario
where
connector_data_1
connector_16
connector_1
a
quickly
a
they
connector_16
produce
and
thus
each
component_6
have
an
average
length
of
so
what
happen
whe
component_5
connector_16
big
when
component_5
be
small
ish
they
will
reside
entirely
within
memory
persistent
connector_data_1
will
also
connector_16
connector_19
to
disc
but
they
will
only
connector_16
connector_20
again
if
the
pattern_5
restart
but
when
component_5
connector_16
large
they
will
connector_16
component_8
to
disc
persistent
or
not
in
this
requirement_4
requirement_1
can
take
a
hit
a
suddenly
we
need
to
connector_21
the
disc
to
connector_10
connector_data_1
to
component_1
so
let’s
run
a
test
publish
a
lot
of
non
persistent
connector_data_1
to
a
component_6
and
then
connector_1
them
all
component_6
load
drain
500k
messagesin
this
small
requirement_4
we
can
see
fairly
consistent
requirement_1
the
connector_data_1
go
into
the
component_6
fairly
quickly
and
then
come
out
even
more
quickly
component_6
load
drain
10m
messagesbut
when
we
have
a
large
component_6
we
see
that
the
requirement_1
vary
a
lot
more
we
see
that
when
loading
the
component_6
we
initially
connector_16
a
very
high
quality_attribute_1
then
a
pause
while
some
of
the
component_6
be
component_8
out
to
disc
then
a
more
consistent
lower
quality_attribute_1
similarly
when
drain
the
component_6
we
see
a
much
lower
rate
when
connector_22
the
connector_data_1
from
disc
requirement_1
of
disc
bind
component_5
be
a
complex
topic
see
matthew’s
on
the
subject
for
some
more
talk
on
the
subject
morewebinar
what’s
in
technology_1
webpage
technology_1
best
practiceswebinar
thing
every
developer
use
technology_1
should
knowtags
requirement_2
flow
control
performancewritten
by
simon
macmullencategories
performancefeaturesget
startedsupportcommunitydocsblogcopyright
©
vmware
inc
or
it
affiliate
all
right
reserve
term
of
use
privacy
and
trademark
guidelinesthe
on
this
be
by
individual
member
of
the
technology_1
team
and
do
not
represent
vmware’s
position
strategy
or
opinion
