top
requirement_1
technology_1
technology_2
requirement_2
requirement_2
zone
thanks
for
visit
today
edit
profile
manage
subscription
how
to
to
submission
guideline
sign
out
pattern_1
profile
an
manage
my
draft
over
million
developer
have
join
requirement_3
in
join
refcardz
trend
report
webinars
zone
|
agile
requirement_4
requirement_2
requirement_5
component_1
devops
requirement_6
iot
technology_3
pattern_2
open_source
requirement_7
quality_attribute_1
web
dev
requirement_2
zone
top
requirement_1
technology_1
technology_2
top
requirement_1
technology_1
technology_2
if
you
re
look
to
adopt
a
technology_2
to
help
perform
technology_1
component_2
connector_1
out
this
connector_data_1
of
five
option
and
see
which
one
fit
your
need
by
vitaliy
samofal
·
mar
·
requirement_2
zone
·
analysis
connector_2
tweet
01k
pattern_1
join
the
and
connector_3
the
full
member
experience
join
for
free
with
the
ever
grow
amount
of
connector_data_2
requirement_1
create
an
increasing
demand
for
connector_data_2
warehousing
project
and
component_3
for
advance
requirement_8
technology_1
be
their
essential
element
it
ensure
successful
connector_data_2
requirement_6
within
various
component_1
and
component_4
in
this
technology_1
technology_2
comparison
we
will
look
at
technology_4
nifi
technology_4
streamsets
technology_4
airflow
technology_5
connector_data_2
pipeline
technology_5
glue
they
be
among
the
most
popular
technology_1
technology_2
s
compare
the
pro
and
con
to
find
out
the
best
solution
for
your
project
the
technology_1
mean
be
often
misunderstand
due
to
the
quality_attribute_2
interpretation
of
it
abbreviation
it
stand
for
three
connector_data_2
requirement_9
concept
extract
transform
load
thus
technology_1
component_5
include
extract
connector_data_2
from
different
external
component_6
transform
it
a
the
requirement_10
component_7
require
loading
connector_data_2
into
the
requirement_9
technology_1
be
only
a
subset
of
connector_data_2
movement
connector_data_3
ralph
s
kimball
book
connector_data_2
requirement_9
technology_1
technology_6
define
it
three
fundamental
feature
connector_data_2
be
download
in
a
suitable
technology_7
for
requirement_8
it
be
enrich
with
additional
connector_data_4
the
component_8
component_9
and
document
the
origin
of
the
connector_data_2
so
the
connector_data_2
shouldn
t
be
reload
from
one
place
to
another
—
it
should
be
improve
in
the
loading
component_2
for
example
an
technology_1
developer
can
calculate
or
technical
attribute
it
s
important
to
track
how
the
connector_data_2
appear
in
the
component_1
a
well
a
how
and
when
it
be
connector_4
technology_1
component_2
step
a
web
programmer
can
imagine
technology_1
architecture
a
a
set
of
three
area
a
connector_data_2
component_10
an
intermediate
area
a
connector_data_2
receiver
a
connector_data_2
connector_5
be
the
movement
of
connector_data_2
from
the
component_10
to
the
receiver
each
of
the
stage
can
be
quite
complicate
the
component_2
of
create
technology_1
include
different
challenge
the
variety
of
external
component_10
unification
of
connector_data_2
accord
to
requirement_10
rule
the
frequency
of
connector_data_5
and
other
specific
requirement
that
s
why
an
it
requirement_11
need
to
have
a
clear
picture
of
connector_data_6
of
the
component_10
and
destination
component_4
an
technology_1
example
the
common
technology_1
connector_data_3
be
to
transfer
connector_data_2
from
technology_8
to
another
component_1
that
work
quality_attribute_3
for
requirement_10
intelligence
technology_2
elt
be
divide
into
two
type
pattern_3
connector_5
the
requirement_12
with
take
connector_data_2
from
technology_8
be
an
example
of
a
connector_5
the
connector_data_2
be
transfer
separately
one
by
one
for
further
component_2
otherwise
we
can
talk
about
a
pattern_3
this
mean
you
can
take
a
whole
component_2
it
and
connector_2
it
to
a
large
various
technology_1
component_3
cope
with
these
connector_data_7
in
different
way
nowadays
the
pattern_3
only
approach
have
become
a
relic
the
grow
number
of
connector_6
connector_data_2
component_6
have
cause
technology_1
technology_2
to
be
use
mainly
for
connector_5
they
make
the
most
recent
connector_data_2
quality_attribute_4
a
quickly
a
possible
the
variety
of
common
and
requirement_5
base
connector_data_2
requirement_6
technology_2
make
the
choice
really
difficult
so
i
prepare
a
connector_data_1
of
five
technology_1
solution
that
be
quality_attribute_5
in
my
experience
technology_4
nifi
requirement_13
free
official
technology_9
nifi
technology_4
useful
resource
documentation
pro
perfect
implementation
of
dataflow
programming
concept
the
opportunity
to
handle
binary
connector_data_2
connector_data_2
provenance
con
simplistic
ui
lack
of
live
pattern_4
and
per
component_11
statistic
the
first
in
the
connector_data_1
of
the
best
technology_1
technology_2
be
an
open_source
project
technology_4
nifi
develop
by
the
technology_4
foundation
it
be
base
on
the
concept
of
dataflow
programming
this
mean
that
this
technology_1
technology_2
allow
u
to
visually
assemble
component_12
from
component_13
and
run
them
almost
without
cod
so
you
t
have
to
any
programming
technology_10
one
of
the
most
popular
open_source
technology_1
technology_2
nifi
be
capable
of
work
with
a
lot
different
component_10
for
example
technology_11
technology_12
query
technology_13
technology_14
technology_15
connector_data_8
etc
a
for
the
action
you
can
pattern_5
adjust
join
split
enhance
and
verify
connector_data_2
technology_4
nifi
be
connector_7
in
technology_3
and
quality_attribute_6
under
the
technology_4
license
it
run
on
a
technology_16
and
support
all
technology_16
technology_10
this
technology_1
technology_2
help
to
create
long
run
and
be
suit
for
component_2
both
connector_6
connector_data_2
and
periodic
pattern_3
a
for
manually
manage
they
be
also
possible
however
there
be
a
risk
to
face
difficulty
while
set
them
up
thanks
to
the
well
round
architecture
technology_4
nifi
be
consider
a
one
of
the
best
open_source
technology_1
technology_2
it
s
a
powerful
and
easy
to
use
solution
flowfile
include
meta
connector_data_4
so
the
technology_2
s
possibility
aren
t
limit
to
csv
you
can
work
with
photo
video
audio
or
binary
connector_data_2
the
processor
include
three
output
failure
mean
there
be
problem
with
appropriate
component_2
of
flowfile
original
show
that
an
incoming
flowfile
have
be
component_2
success
denote
that
the
component_2
of
flowfiles
be
finish
if
you
want
to
drop
terminate
output
you
can
use
special
checkboxes
you
should
pay
attention
to
the
component_2
group
they
be
necessary
for
combine
element
of
a
complex
dataflow
in
advance
technology_1
programming
another
great
feature
be
the
possibility
of
use
various
component_14
requirement_14
pattern_6
lifo
and
others
connector_data_2
provenance
be
a
connector_8
component_15
that
component_9
almost
everything
in
your
dataflows
it
s
very
convenient
because
you
can
see
how
the
connector_data_2
be
connector_2
or
perform
the
only
drawback
be
that
the
require
lot
of
disk
space
some
component_16
complain
about
the
technology_4
nifi
s
actually
it
isn
t
impressive
but
the
quality_attribute_7
be
quality_attribute_3
enough
the
ui
have
a
clear
minimalist
design
without
extra
element
the
only
exception
be
the
lack
of
automatic
adjustment
of
text
for
long
technology_17
query
you
should
do
them
manually
there
be
also
a
build
in
technology_18
cluster
you
can
pick
up
several
instance
and
it
will
connector_9
out
the
necessary
technology_1
connector_data_2
technology_4
nifi
include
back
pressure
it
be
need
for
quickly
connector_10
to
technology_19
connector_11
the
and
it
to
the
next
processor
to
sum
up
technology_4
nifi
be
a
nice
alternative
to
other
mainstream
technology_1
technology_2
it
advantage
be
more
than
different
embed
processor
they
provide
an
opportunity
to
download
via
technology_9
technology_20
or
connector_data_2
component_10
and
connector_12
them
to
technology_19
or
other
connector_data_2
receiver
you
need
to
configure
the
ui
press
the
run
and
if
everything
be
right
it
will
work
technology_4
streamsets
requirement_13
free
official
technology_9
streamsets
technology_21
useful
resource
documentation
developer
support
forum
component_10
pro
each
processor
have
individual
per
component_11
statistic
with
nice
visualization
for
quality_attribute_8
debug
attractive
component_17
quality_attribute_3
technology_2
for
connector_6
or
component_11
base
connector_data_2
con
the
absence
of
a
quality_attribute_9
technology_12
configuration
connector_4
a
set
of
one
processor
require
stop
the
whole
dataflow
technology_4
streamsets
be
a
strong
competitor
of
technology_4
nifi
it
s
difficult
to
say
which
on
these
free
technology_1
technology_2
be
quality_attribute_3
all
connector_data_2
that
you
put
into
streamsets
automatically
convert
into
exchangeable
component_11
the
common
technology_7
be
design
for
smooth
connector_5
unlike
technology_4
nifi
this
technology_1
technology_2
doesn
t
show
component_18
between
processor
if
you
want
to
use
different
technology_7
technology_4
nifi
require
turn
from
one
version
of
the
processor
to
another
streamsets
avoid
these
manipulation
instead
of
stop
only
one
processor
you
need
to
stop
the
whole
dataflow
to
connector_4
the
set
while
it
seem
that
fix
bug
be
more
difficult
in
streamsets
in
fact
it
s
easy
due
to
the
real
time
debug
technology_2
thanks
to
the
component_17
friendly
with
a
live
requirement_15
and
all
the
necessary
statistic
you
can
notice
and
fix
any
error
moreover
there
be
an
opportunity
to
put
component_11
pattern_5
on
the
connector_13
between
processor
to
connector_1
suspicious
component_11
there
be
four
variation
of
the
processor
origin
processor
connector_14
connector_data_4
from
connector_data_2
component_10
processor
that
connector_3
and
transform
the
connector_15
connector_data_2
destination
put
transform
connector_data_2
to
the
external
executor
component_2
action
complete
by
other
processor
streamsets
processor
can
generate
action
and
include
bug
in
order
to
track
and
fix
them
you
need
executor
some
component_16
prefer
technology_4
nifi
because
it
design
be
quality_attribute_2
all
you
need
be
the
processor
and
pattern_7
component_15
however
streamsets
also
have
well
think
architecture
which
isn
t
difficult
to
connector_3
use
to
and
the
ui
also
look
quality_attribute_3
i
felt
the
lack
of
pattern_7
component_15
which
be
quite
important
for
technology_12
set
adjust
all
technology_12
set
for
each
processor
be
really
annoying
streamsets
connector_16
all
processor
before
you
can
run
the
dataflow
this
feature
seem
quite
useful
in
my
experience
it
s
a
controversial
thing
streamsets
doesn
t
allow
you
to
leave
disconnect
processor
for
fix
bug
in
the
future
all
of
them
must
be
connector_8
before
the
dataflow
start
a
for
other
con
i
felt
a
lack
of
possibility
a
i
couldn
t
choose
more
than
processor
at
once
move
many
processor
and
reorganize
them
one
by
one
take
too
much
time
and
effort
all
in
all
it
s
a
mature
open_source
technology_1
technology_2
with
convenient
visual
connector_data_2
flow
and
a
modern
web
i
recommend
you
to
try
streamsets
and
technology_4
nifi
to
find
out
which
of
them
be
the
most
suitable
for
your
purpose
technology_4
airflow
requirement_13
free
official
technology_9
airflow
technology_4
useful
resource
pro
suit
for
different
type
of
connector_data_3
component_17
friendly
for
clear
visualization
quality_attribute_10
solution
con
isn
t
suitable
for
connector_6
require
additional
operator
this
modern
component_19
for
design
create
and
track
workflow
be
an
open_source
technology_1
it
can
be
use
with
requirement_5
component_15
include
gcp
technology_22
and
technology_23
there
be
an
opportunity
to
run
airflow
on
technology_24
use
astronomer
requirement_1
you
can
in
technology_25
but
not
have
to
worry
about
connector_data_9
or
drag
and
drop
gui
the
workflow
be
connector_7
in
technology_25
however
the
step
themselves
can
be
do
in
anything
you
want
airflow
be
create
a
a
perfectly
quality_attribute_11
connector_data_3
scheduler
one
of
the
top
technology_1
technology_2
be
suitable
for
lot
of
different
purpose
it
be
use
to
train
ml
component_7
connector_17
connector_data_10
track
component_8
and
power
within
various
apis
the
fact
about
the
component_19
airflow
a
a
component_15
be
quality_attribute_4
from
qubole
and
astronomer
io
it
be
create
by
airbnb
in
and
transition
to
technology_4
in
the
basis
for
s
requirement_5
composer
beta
summer
workflow
be
perform
a
direct
acyclic
graph
dag
technology_4
airflow
be
design
accord
to
four
fundamental
principle
the
developer
aim
to
create
a
dynamic
quality_attribute_12
elegant
and
quality_attribute_10
solution
so
it
provide
dynamic
pipeline
generation
through
cod
in
technology_25
you
can
also
define
your
own
operator
and
executor
and
extend
the
technology_26
accord
to
the
need
level
of
abstraction
the
pipeline
be
clear
and
quality_attribute_13
because
parameterizing
be
include
into
the
core
of
the
component_19
thanks
to
the
modular
design
with
a
connector_data_11
component_14
airflow
can
be
easily
quality_attribute_14
technology_4
airflow
be
suitable
for
most
of
the
everyday
connector_data_7
run
technology_1
and
ml
pipeline
connector_18
connector_data_2
and
complete
db
backup
however
it
s
a
bad
choice
for
connector_5
the
component_19
have
a
modern
ui
that
be
full
of
visualization
element
you
can
see
all
the
run
pipeline
track
progress
and
fix
bug
this
help
complete
complex
connector_data_7
on
dag
a
for
workflow
they
be
constant
and
quality_attribute_15
the
connector_data_12
be
a
little
bit
more
dynamic
than
an
technology_1
component_1
if
you
define
workflow
a
they
will
be
more
collaborative
versionable
quality_attribute_16
and
quality_attribute_17
the
component_19
run
on
a
private
technology_24
cluster
it
also
include
resource
requirement_16
technology_2
and
requirement_8
statsd
prometheus
grafana
what
about
the
technology_1
test
of
airflow
workflow
you
can
use
unit
test
requirement_6
test
end
to
end
test
in
some
requirement_12
the
first
type
be
suitable
for
connector_19
dag
loading
technology_25
operator
custom
operator
and
bash
emr
script
the
component_19
doesn
t
require
any
original
configuration
the
only
thing
that
should
be
connector_4
be
the
db
connector_13
you
need
to
create
an
empty
component_1
and
give
the
component_17
permission
to
create
alter
so
an
airflow
command
will
handle
all
the
rest
to
conclude
technology_4
airflow
be
a
free
independent
technology_27
connector_7
in
technology_25
it
s
a
quality_attribute_3
example
of
open_source
technology_1
technology_2
airflow
can
be
challenge
to
run
alone
so
you
should
use
different
operator
technology_5
connector_data_2
pipeline
requirement_13
variable
official
technology_9
technology_23
technology_21
datapipeline
useful
resource
documentation
forum
pro
easy
to
use
technology_1
technology_28
reasonable
requirement_13
nice
quality_attribute_18
con
doesn
t
have
many
build
in
the
web
component_15
ensure
component_2
and
move
connector_data_2
between
an
technology_5
compute
and
various
connector_data_2
component_10
it
provide
permanent
connector_20
to
the
component_20
connector_data_2
a
well
a
it
transformation
the
final
connector_data_13
can
be
transfer
to
technology_5
component_15
they
be
technology_29
rds
emr
and
technology_20
this
technology_1
technology_2
simplify
the
component_2
of
create
complex
connector_data_2
component_2
workload
it
help
to
achieve
a
quality_attribute_19
highly
quality_attribute_4
and
quality_attribute_5
requirement_12
load
technology_5
connector_data_2
pipeline
give
the
possibility
to
move
and
component_2
connector_data_2
that
be
previously
lock
up
in
on
premise
connector_data_2
silo
assert
that
it
technology_1
technology_2
have
six
advantage
quality_attribute_20
quality_attribute_21
quality_attribute_22
quality_attribute_3
requirement_13
quality_attribute_23
quality_attribute_24
technology_5
connector_data_2
pipeline
be
a
quality_attribute_5
component_15
that
automatically
retry
the
active
component_5
in
requirement_12
of
any
failure
you
will
also
connector_21
connector_data_10
via
sn
they
can
be
set
for
successful
run
delay
or
failure
the
drag
and
drop
console
allow
fast
and
quality_attribute_2
design
of
pipeline
the
build
in
precondition
prevent
you
from
connector_22
any
extra
component_21
to
use
them
the
web
developer
enjoy
various
popular
feature
i
mean
schedule
connector_23
track
and
issue
handle
the
component_15
s
quality_attribute_11
design
allow
for
the
smooth
component_2
of
numerous
this
technology_30
isn
t
expensive
compare
to
other
technology_1
technology_2
technology_5
connector_data_2
pipeline
be
a
serverless
pattern_8
component_15
and
you
pay
only
for
what
you
use
moreover
there
be
a
free
trial
version
for
component_17
it
s
a
quality_attribute_25
solution
the
component_17
connector_14
full
connector_data_4
on
the
pipeline
and
have
complete
control
over
the
computational
resource
finally
i
especially
recommend
this
technology_1
technology_2
for
perform
pattern_9
i
use
it
on
my
current
project
for
transfer
connector_data_2
although
technology_5
connector_data_2
pipeline
doesn
t
have
many
build
in
it
provide
a
convenient
ui
it
can
spawn
instance
and
ensure
cascade
requirement_16
i
this
quality_attribute_2
inexpensive
and
useful
technology_2
with
build
in
processor
that
allow
you
to
do
everything
via
the
ui
technology_5
glue
requirement_13
variable
official
technology_9
technology_23
technology_21
glue
useful
resource
pro
support
various
connector_data_2
component_10
quality_attribute_3
requirement_6
with
technology_5
component_15
con
a
lot
of
manual
work
poor
quality_attribute_18
the
base
serverless
technology_1
alternative
to
traditional
drag
and
drop
component_22
be
quality_attribute_8
but
an
ambitious
solution
technology_5
glue
allow
you
to
create
and
run
an
technology_1
in
the
technology_5
requirement_16
console
the
component_15
take
connector_data_2
and
metadata
from
technology_23
put
it
in
the
catalog
and
make
it
searchable
queryable
and
quality_attribute_4
for
technology_1
the
component_2
include
three
step
classify
connector_data_2
through
build
a
catalog
technology_31
csv
parquet
and
many
other
technology_7
be
quality_attribute_4
generate
technology_1
and
edit
transformation
connector_7
in
technology_32
or
technology_25
schedule
and
run
technology_1
point
out
three
benefit
of
this
technology_1
technology_2
convenience
have
tight
requirement_6
with
numerous
technology_5
component_23
and
component_24
this
technology_2
be
quality_attribute_2
for
those
who
already
use
technology_30
the
drawback
be
that
you
can
t
connector_24
it
on
premise
or
in
any
other
requirement_5
environment
profitable
the
serverless
solution
mean
you
t
need
to
provision
or
manage
infrastructure
so
the
cost
quality_attribute_26
on
the
measure
of
connector_data_2
component_2
unit
you
pay
only
for
the
that
be
run
powerful
the
automatization
of
create
maintain
and
run
technology_1
be
perfect
on
the
other
hand
the
component_15
require
a
lot
of
manual
work
too
technology_4
technology_33
be
use
a
the
base
for
technology_1
component_21
however
you
notice
significant
difference
from
ordinary
technology_33
the
component_15
have
a
dynamic
frame
with
specific
glue
while
technology_33
u
a
connector_data_2
frame
technology_5
glue
be
a
modern
and
strong
part
of
the
technology_5
ecosystem
but
you
should
be
mindful
of
it
nuance
the
component_15
provide
a
level
of
abstraction
in
which
you
must
identify
component_25
they
represent
your
csv
there
be
a
lot
of
manual
work
here
but
in
the
end
it
will
generate
the
for
technology_33
and
launch
it
you
can
download
this
in
technology_32
or
technology_25
and
connector_4
it
a
you
want
it
s
suitable
for
a
wide
range
of
connector_data_2
component_10
but
the
component_15
force
you
to
choose
a
specific
solution
if
you
want
to
try
another
way
you
not
be
able
to
do
that
how
to
select
the
right
technology_1
technology_2
infoworld
assert
that
technology_1
cause
the
large
cost
in
build
connector_data_2
warehousing
component_8
it
s
the
bottleneck
that
require
special
attention
a
correct
technology_1
implementation
be
your
chance
to
optimize
cost
and
quality_attribute_27
up
work
when
hoosing
an
technology_1
technology_2
consider
five
criterion
the
complexity
of
your
component_8
your
connector_data_2
requirement
developer
experience
cost
of
technology_1
technology_28
special
requirement_10
need
extract
transform
load
connector_data_2
component_2
technology_4
nifi
open_source
technology_5
web
component_15
component_1
career
it
technology_4
airflow
publish
at
with
permission
of
vitaliy
samofal
see
the
original
here
opinion
express
by
contributor
be
their
own
popular
on
best
practice
to
do
functional
test
on
the
requirement_5
refactoring
technology_3
component_4
connector_data_14
orient
and
functional
approach
how
to
design
a
crud
web
component_15
for
inheritable
component_26
why
requirement_7
project
fail
requirement_2
partner
resource
x
about
u
about
connector_17
feedback
career
sitemap
advertise
advertise
with
contribute
on
submission
guideline
mvb
component_27
become
a
contributor
visit
the
writer
zone
legal
term
of
component_15
privacy
requirement_14
u
park
office
drive
suite
durham
nc
support@dzone
technology_21
+1
s
be
friend
technology_21
be
powered
by
