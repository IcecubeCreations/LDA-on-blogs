benchmarking
technology_1
vs
technology_2
vs
technology_3
which
be
fast
register
for
demo
|
rbac
at
quality_attribute_1
technology_4
cdc
component_1
connector
and
more
within
our
q2
launch
for
confluent
cloudcontact
usproductschoose
your
deploymentconfluent
requirement_1
requirement_2
login
confluent
component_2
subscription
connector
technology_5
connector_1
governance
confluent
vs
technology_1
why
you
need
confluent
solutionsby
requirement_3
by
use
requirement_4
by
architecture
by
requirement_5
all
solution
hybrid
and
multicloud
modernization
pattern_1
pattern_2
connector_2
technology_6
use
requirement_4
showcase
connector_2
use
requirement_4
to
transform
your
requirement_6
learnblog
resource
train
professional
component_3
career
meetups
technology_1
summit
webinars
connector_2
technology_6
requirement_1
demo
master
technology_1
connector_3
and
technology_5
pattern_2
with
confluent
developersconfluent
developer
doc
technology_7
technology_1
quick
start
connector_2
audio
podcast
ask
the
connector_4
start
freeus
englishget
start
freeproductschoose
your
deploymentconfluent
requirement_1
requirement_2
login
confluent
component_2
subscription
connector
technology_5
connector_1
governance
confluent
vs
technology_1
why
you
need
confluent
solutionsby
requirement_3
by
use
requirement_4
by
architecture
by
requirement_5
all
solution
hybrid
and
multicloud
modernization
pattern_1
pattern_2
connector_2
technology_6
use
requirement_4
showcase
connector_2
use
requirement_4
to
transform
your
requirement_6
learnblog
resource
train
professional
component_3
career
meetups
technology_1
summit
webinars
connector_2
technology_6
requirement_1
demo
master
technology_1
connector_3
and
technology_5
pattern_2
with
confluent
developersconfluent
developer
doc
technology_7
technology_1
quick
start
connector_2
audio
podcast
ask
the
connector_4
start
freeapache
kafkabenchmarking
technology_7
technology_1
technology_7
technology_2
and
technology_3
which
be
the
fast
alok
nikhilvinoth
chandaraug
2020apache
kafka®
be
one
of
the
most
popular
connector_2
component_4
there
be
many
way
to
compare
component_5
in
this
space
but
one
thing
everyone
care
about
be
requirement_7
technology_1
have
be
to
be
fast
but
how
fast
be
it
today
and
how
do
it
technology_8
up
against
other
component_4
we
decide
to
test
kafka’s
requirement_7
on
the
late
requirement_1
hardware
to
find
out
for
comparison
we
choose
a
traditional
connector_data_1
pattern_3
technology_3
and
one
of
the
technology_7
bookkeeper™
base
connector_data_1
pattern_3
technology_7
technology_2
we
focus
on
component_4
quality_attribute_2
and
component_4
quality_attribute_3
a
these
be
the
primary
requirement_7
metric
for
connector_2
component_5
in
production
in
particular
the
quality_attribute_2
test
measure
how
quality_attribute_4
each
component_4
be
in
utilize
the
hardware
specifically
the
disk
and
the
cpu
the
quality_attribute_3
test
measure
how
close
each
component_4
be
to
connector_5
real
time
pattern_4
include
tail
quality_attribute_3
of
up
to
p99
9th
percentile
a
key
requirement
for
real
time
and
mission
critical
component_6
a
well
a
pattern_2
architecture
we
find
that
technology_1
connector_6
the
best
quality_attribute_2
while
provide
the
low
end
to
end
quality_attribute_3
up
to
the
p99
9th
percentile
at
lower
quality_attribute_2
technology_3
connector_6
connector_data_2
at
very
low
quality_attribute_3
technology_1
technology_2
technology_3
mirror
peak
quality_attribute_2
connector_data_3
s
connector_data_3
s
connector_data_3
s
connector_data_3
s
p99
quality_attribute_3
m
m
connector_data_3
s
load
m
connector_data_3
s
load
ms*
reduce
connector_data_3
s
load
*rabbitmq
quality_attribute_3
degrade
significantly
at
quality_attribute_2
high
than
the
connector_data_3
s
furthermore
the
impact
of
mirror
be
significant
at
high
quality_attribute_2
and
quality_attribute_5
quality_attribute_3
can
be
achieve
by
use
classic
component_7
without
mirror
this
be
pattern_5
to
first
walk
you
through
the
benchmarking
technology_9
we
use
follow
by
a
description
of
the
testbed
and
the
workload
it
will
finish
with
an
explanation
of
the
connector_data_4
use
the
various
component_4
and
component_8
metric
all
of
these
be
open_source
so
curious
reader
can
reproduce
the
connector_data_4
for
themselves
or
dig
deep
into
the
connector_7
prometheus
metric
a
with
most
benchmark
we
compare
requirement_7
on
a
setup
for
a
specific
workload
we
always
encourage
reader
to
compare
use
their
own
workload
setup
to
understand
how
these
pattern_6
to
production
deployment
for
a
deep
look
at
feature
architecture
ecosystem
and
more
connector_8
this
complete
guide
compare
technology_1
technology_2
and
technology_3
overview
background
quality_attribute_6
in
quality_attribute_7
component_5
benchmarking
technology_9
fix
to
the
omb
technology_9
fix
to
the
omb
technology_1
driver
fix
to
the
omb
technology_3
driver
fix
to
the
omb
technology_2
driver
testbed
disk
o
tune
memory
quality_attribute_2
test
effect
of
fsync
test
setup
quality_attribute_2
connector_data_4
quality_attribute_3
test
quality_attribute_3
connector_data_4
quality_attribute_3
requirement_8
off
summary
background
first
let’s
discus
each
of
the
component_5
briefly
to
understand
their
high
level
design
and
architecture
look
at
the
requirement_8
off
each
component_4
make
technology_1
be
an
open_source
quality_attribute_7
connector_2
component_2
and
one
of
the
five
most
active
project
of
the
technology_7
foundation
at
it
core
technology_1
be
design
a
a
replicate
quality_attribute_7
persistent
connector_9
requirement_9
that
be
use
to
power
pattern_1
pattern_2
or
large
quality_attribute_1
connector_1
component_9
component_8
component_10
produce
or
connector_10
directly
to
from
a
cluster
of
pattern_3
which
connector_8
connector_11
durably
to
the
underlie
component_4
and
also
automatically
replicate
the
synchronously
or
asynchronously
within
the
cluster
for
fault
tolerance
and
high
quality_attribute_8
technology_2
be
an
open
component_1
quality_attribute_7
pattern_7
pattern_4
component_4
originally
cater
towards
pattern_8
use
requirement_4
it
recently
connector_2
requirement_10
a
well
technology_2
be
design
a
a
tier
of
almost
stateless
pattern_3
instance
that
connector_12
to
a
separate
tier
of
bookkeeper
instance
which
actually
connector_8
connector_11
and
optionally
component_11
replicate
the
connector_data_2
durably
technology_2
be
not
the
only
component_4
of
it
kind
a
there
be
also
other
pattern_4
component_5
technology_7
distributedlog
and
pravega
which
have
be
create
on
top
of
bookkeeper
and
aim
to
also
provide
some
technology_1
connector_2
requirement_10
bookkeeper
be
an
open
component_1
quality_attribute_7
storage
component_12
that
be
originally
design
a
a
connector_11
ahead
requirement_9
for
apache™
hadoop®’s
namenode
it
provide
persistent
storage
of
connector_data_2
in
ledger
across
component_13
instance
connector_13
bookie
each
bookie
synchronously
connector_14
each
connector_data_1
to
a
local
journal
requirement_9
for
recovery
purpose
and
then
asynchronously
into
it
local
index
ledger
storage
unlike
technology_1
pattern_3
bookie
do
not
connector_15
with
each
other
and
it’s
the
bookkeeper
component_10
that
be
responsible
for
replicate
the
connector_data_2
across
bookie
use
a
quorum
style
technology_10
technology_3
be
an
open
component_1
traditional
pattern_4
technology_11
that
connector_16
the
technology_12
pattern_4
technology_13
cater
to
low
quality_attribute_3
pattern_8
use
requirement_4
technology_3
consist
of
a
set
of
pattern_3
component_14
that
component_15
“exchanges”
for
publish
connector_data_2
to
and
component_7
for
connector_17
connector_data_2
from
quality_attribute_8
and
quality_attribute_6
be
property
of
the
various
component_16
type
offer
classic
component_7
offer
the
least
quality_attribute_8
guarantee
classic
mirror
component_7
replicate
connector_data_2
to
other
pattern_3
and
improve
quality_attribute_8
strong
quality_attribute_6
be
provide
through
the
more
recently
introduce
quorum
component_7
but
at
the
cost
of
requirement_7
since
this
be
a
requirement_7
orient
we
restrict
our
evaluation
to
classic
and
mirror
component_16
quality_attribute_6
in
quality_attribute_7
component_5
single
technology_14
storage
component_5
e
g
technology_15
quality_attribute_9
on
fsyncing
connector_14
to
disk
to
ensure
maximal
quality_attribute_6
but
in
quality_attribute_7
component_4
quality_attribute_6
typically
come
from
pattern_9
with
multiple
copy
of
the
connector_data_5
that
fail
independently
fsyncing
connector_data_5
be
a
way
of
reduce
the
impact
of
the
failure
when
it
do
occur
e
g
fsyncing
more
often
could
lead
to
lower
recovery
time
conversely
if
enough
replica
fail
a
quality_attribute_7
component_4
be
unusable
regardless
of
fsync
or
not
hence
whether
we
fsync
or
not
be
a
matter
of
what
guarantee
each
component_4
choose
to
quality_attribute_9
on
for
it
pattern_9
design
while
some
quality_attribute_9
closely
on
never
lose
connector_data_5
connector_11
to
disk
thus
require
fsync
on
every
connector_11
others
handle
this
scenario
in
their
design
kafka’s
pattern_9
technology_10
be
carefully
design
to
ensure
consistency
and
quality_attribute_6
guarantee
without
the
need
for
pattern_10
fsync
by
track
what
have
be
fsynced
to
the
disk
and
what
hasn’t
by
assume
le
technology_1
can
handle
a
wide
range
of
failure
filesystem
level
corruption
or
accidental
disk
de
provision
and
do
not
take
for
grant
the
quality_attribute_10
of
connector_data_5
that
be
not
to
be
fsync’d
technology_1
be
also
able
to
leverage
the
o
for
pattern_11
connector_14
to
the
disk
for
quality_attribute_5
requirement_7
we
have
not
be
able
to
ascertain
categorically
whether
bookkeeper
offer
the
same
consistency
guarantee
without
fsyncing
each
write—specifically
whether
it
can
rely
on
pattern_9
for
fault
tolerance
in
the
absence
of
pattern_10
disk
persistence
this
isn’t
cover
in
the
documentation
or
a
connector_11
up
on
the
underlie
pattern_9
algorithm
base
on
our
inspection
and
the
fact
that
bookkeeper
connector_16
a
group
fsync
algorithm
we
believe
it
do
rely
on
fsyncing
on
each
connector_11
for
it
quality_attribute_10
but
we’d
love
to
hear
from
folk
in
the
who
might
quality_attribute_5
if
our
conclusion
be
correct
in
any
requirement_4
since
this
can
be
somewhat
of
a
controversial
topic
we’ve
give
connector_data_4
in
both
requirement_4
to
ensure
we
be
be
a
fair
and
complete
a
possible
though
run
technology_1
with
pattern_10
fsync
be
extremely
uncommon
and
also
unnecessary
benchmarking
technology_9
with
any
benchmark
one
wonder
what
technology_9
be
be
use
and
if
it’s
fair
to
that
end
we
want
to
use
the
openmessaging
benchmark
technology_9
omb
originally
author
in
large
part
by
technology_2
contributor
omb
be
a
quality_attribute_5
start
point
with
basic
workload
specification
metric
collection
report
for
the
test
connector_data_6
support
for
the
three
chosen
pattern_4
component_5
a
well
a
a
modular
requirement_1
deployment
workflow
quality_attribute_11
for
each
component_4
but
of
note
technology_1
and
technology_3
implementation
do
have
some
significant
shortcoming
that
affect
the
fairness
and
quality_attribute_12
of
these
test
the
connector_data_6
benchmarking
include
the
fix
describe
in
more
detail
below
be
quality_attribute_13
a
open_source
fix
to
the
omb
technology_9
we
upgrade
to
technology_16
and
technology_1
technology_3
and
technology_2
the
late
release
at
the
time
of
connector_11
we
significantly
enhance
the
pattern_12
capability
across
the
three
component_4
with
the
grafana
prometheus
pattern_12
technology_8
capture
metric
across
pattern_4
component_4
technology_17
linux
disk
cpu
and
requirement_11
this
be
critical
for
be
able
to
not
report
connector_data_4
but
explain
them
we
have
support
for
component_17
only
test
and
component_18
only
test
with
support
for
generate
drain
backlog
while
also
fix
an
important
bug
with
component_17
rate
calculation
when
the
number
of
topic
be
small
than
the
number
of
component_17
component_19
fix
to
the
omb
technology_1
driver
we
fix
a
critical
bug
in
the
technology_1
driver
that
starve
technology_1
component_20
of
technology_18
connector_18
bottleneck
on
a
single
connector_18
from
each
component_19
instance
the
fix
make
the
technology_1
number
fair
compare
to
other
systems—that
be
all
of
them
now
use
the
same
number
of
technology_18
connector_18
to
talk
to
their
respective
pattern_3
we
also
fix
a
critical
bug
in
the
technology_1
benchmark
component_18
driver
where
offset
be
be
connector_9
too
frequently
and
synchronously
cause
degradation
whereas
it
be
do
asynchronously
for
other
component_4
we
also
tune
the
technology_1
component_18
fetch
size
and
pattern_9
component_21
to
eliminate
bottleneck
in
connector_data_1
fetch
at
high
quality_attribute_2
and
to
configure
the
pattern_3
equivalent
to
the
other
component_4
fix
to
the
omb
technology_3
driver
we
enhance
technology_3
to
use
connector_19
key
and
quality_attribute_14
exchange
type
direct
and
topic
exchange
and
also
fix
a
bug
in
the
technology_3
cluster
setup
deployment
workflow
connector_19
key
be
introduce
to
mimic
the
concept
of
component_22
per
topic
equivalent
to
the
setup
on
technology_1
and
technology_2
we
a
timesync
workflow
for
the
technology_3
deployment
to
synchronize
time
across
component_23
instance
for
precise
end
to
end
quality_attribute_3
measurement
in
addition
we
fix
another
bug
in
the
technology_3
driver
to
ensure
quality_attribute_15
end
to
end
quality_attribute_3
measurement
fix
to
the
omb
technology_2
driver
for
the
omb
technology_2
driver
we
the
ability
to
specify
a
maximum
pattern_11
size
for
the
technology_2
component_17
and
turn
off
any
global
limit
that
could
artificially
limit
quality_attribute_2
at
high
target
rat
for
component_17
component_7
across
component_22
we
do
not
need
to
make
any
other
major
connector_20
to
technology_2
benchmark
driver
testbed
omb
contain
testbed
definition
instance
type
and
technology_17
configs
and
workload
driver
configuration
component_17
component_18
configs
and
component_13
side
configs
for
it
benchmark
which
we
use
a
the
basis
for
our
test
all
test
quality_attribute_16
four
component_19
instance
to
drive
workload
three
pattern_3
component_13
instance
one
pattern_12
instance
and
optionally
a
three
instance
technology_7
technology_19
cluster
for
technology_1
and
technology_2
after
experiment
with
several
instance
type
we
settle
on
the
requirement_11
storage
optimize
of
technology_20
instance
with
enough
cpu
core
and
requirement_11
bandwidth
to
support
disk
i
o
bind
workload
in
the
section
below
we
connector_data_7
out
any
connector_20
we
have
make
to
these
baseline
configuration
along
the
way
for
different
test
disk
specifically
we
go
with
the
i3en
2xlarge
with
vcores
gb
ram
x
gb
nvme
ssds
for
it
high
gbps
requirement_11
transfer
limit
that
ensure
that
the
test
setup
be
not
requirement_11
bind
this
mean
that
the
test
measure
the
respective
maximum
component_13
requirement_7
measure
not
simply
how
fast
the
requirement_11
be
i3en
2xlarge
instance
support
up
to
~655
connector_data_3
s
of
connector_11
quality_attribute_2
across
two
disk
which
be
plenty
to
stress
the
component_13
see
the
full
instance
type
definition
for
detail
per
the
general
recommendation
and
also
per
the
original
omb
setup
technology_2
u
one
of
the
disk
for
journaling
and
one
for
ledger
storage
no
connector_20
be
make
to
the
disk
setup
of
technology_1
and
technology_3
figure
establish
the
maximum
disk
bandwidth
of
i3en
2xlarge
instance
across
two
disk
test
use
the
dd
linux
command
to
serve
a
a
north
star
for
quality_attribute_2
test
disk
dd
if=
dev
zero
of=
mnt
connector_data_5
test
bs=1m
count=65536
oflag=direct
65536+0
component_24
in
65536+0
component_24
out
byte
gb
copy
s
connector_data_3
s
disk
dd
if=
dev
zero
of=
mnt
connector_data_5
test
bs=1m
count=65536
oflag=direct
65536+0
component_24
in
65536+0
component_24
out
byte
gb
copy
s
connector_data_3
s
o
tune
additionally
for
all
three
of
the
compare
component_4
we
tune
the
o
for
quality_attribute_5
quality_attribute_3
requirement_7
use
tune
adm’s
quality_attribute_3
requirement_7
profile
which
disable
any
dynamic
tune
mechanism
for
disk
and
requirement_11
scheduler
and
u
the
requirement_7
governor
for
cpu
frequency
tune
it
peg
the
p
state
at
the
high
possible
frequency
for
each
core
and
it
set
the
i
o
scheduler
to
the
deadline
to
offer
a
quality_attribute_17
upper
bind
on
disk
connector_data_8
quality_attribute_3
finally
it
also
tune
the
power
requirement_12
quality
of
component_12
qos
in
the
kernel
for
requirement_7
over
power
connector_21
memory
the
i3en
2xlarge
test
instance
have
almost
half
the
physical
memory
gb
vs
gb
compare
to
the
default
instance
in
omb
tune
technology_1
and
technology_3
to
be
quality_attribute_18
with
the
test
instance
be
quality_attribute_19
both
rely
primarily
on
the
operate
system’s
component_25
pattern_13
which
automatically
connector_22
quality_attribute_1
down
with
the
instance
however
technology_2
pattern_3
a
well
a
bookkeeper
bookie
rely
on
off
heap
direct
memory
for
pattern_13
and
we
size
the
technology_17
heap
maximum
direct
memory
for
these
two
separate
component_14
to
work
well
on
i3en
2xlarge
instance
specifically
we
halve
the
heap
size
from
gb
each
in
the
original
omb
configuration
to
gb
each
proportionately
divide
quality_attribute_13
physical
memory
amongst
the
two
component_14
and
the
o
in
our
test
we
encounter
technology_16
lang
outofmemoryerror
direct
buffer
memory
error
at
high
target
quality_attribute_2
cause
bookie
to
completely
crash
if
the
heap
size
be
any
lower
this
be
typical
of
memory
tune
problem
face
by
component_5
that
employ
off
heap
memory
while
direct
byte
buffer
be
an
attractive
choice
for
avoid
technology_16
gc
tame
them
at
a
high
quality_attribute_1
be
a
challenge
exercise
quality_attribute_2
test
the
first
thing
we
set
out
to
measure
be
the
peak
quality_attribute_20
quality_attribute_2
each
component_4
could
achieve
give
the
same
amount
of
requirement_11
disk
cpu
and
memory
resource
we
define
peak
quality_attribute_20
quality_attribute_2
a
the
high
average
component_17
quality_attribute_2
at
which
component_26
can
keep
up
without
an
ever
grow
backlog
effect
of
fsync
a
mention
early
the
default
recommend
configuration
for
technology_7
technology_1
be
to
flush
fsync
connector_data_2
to
disk
use
the
component_25
pattern_13
flush
requirement_13
dictate
by
the
underlie
o
instead
of
fsyncing
every
connector_data_1
synchronously
and
to
rely
on
pattern_9
for
quality_attribute_6
fundamentally
this
provide
a
quality_attribute_19
and
quality_attribute_21
way
to
amortize
the
cost
of
different
pattern_11
size
employ
by
technology_1
component_20
to
achieve
maximum
possible
quality_attribute_2
under
all
condition
if
technology_1
be
configure
to
fsync
on
each
connector_11
we
would
be
artificially
impede
the
requirement_7
by
force
fsync
component_4
connector_data_7
without
any
additional
gain
that
say
it
still
be
worthwhile
understand
the
impact
of
fsyncing
on
each
connector_11
in
technology_1
give
that
we
be
go
to
discus
connector_data_4
for
both
requirement_4
the
effect
of
the
various
component_17
pattern_11
size
on
technology_1
quality_attribute_2
be
show
below
quality_attribute_2
increase
with
increase
in
pattern_11
size
before
reach
a
“sweet
spot
”
where
the
pattern_11
size
be
high
enough
to
fully
saturate
the
underlie
disk
fsyncing
every
connector_data_1
to
disk
on
technology_1
orange
bar
in
figure
yield
comparable
connector_data_4
for
high
pattern_11
size
note
that
these
connector_data_4
have
be
verify
only
on
the
ssds
in
the
describe
testbed
technology_1
do
make
full
use
of
the
underlie
disk
across
all
pattern_11
size
either
maximize
iop
at
lower
pattern_11
size
or
disk
quality_attribute_2
at
high
pattern_11
size
even
when
force
to
fsync
every
connector_data_1
figure
effect
of
pattern_11
size
on
quality_attribute_2
in
connector_data_1
s
in
technology_1
with
green
bar
denote
fsync=off
default
and
orange
bar
denote
fsync
every
connector_data_1
respectively
that
say
from
the
requirement_14
above
it
be
evident
that
use
the
default
fsync
set
green
bar
allow
the
technology_1
pattern_3
to
quality_attribute_5
manage
the
component_25
flush
to
provide
a
quality_attribute_5
quality_attribute_2
overall
particularly
the
quality_attribute_2
with
the
default
pattern_14
set
for
lower
component_17
pattern_11
size
kb
and
kb
be
~3–5x
high
than
what
be
achieve
by
fsyncing
every
connector_data_1
with
large
pattern_11
kb
and
connector_data_3
however
the
cost
of
fsyncing
be
amortize
and
the
quality_attribute_2
be
comparable
to
the
default
fsync
set
technology_2
connector_16
similar
pattern_11
on
the
component_20
and
do
quorum
style
pattern_9
of
produce
connector_data_2
across
bookie
bookkeeper
bookie
connector_23
group
connector_9
pattern_14
to
disk
at
the
component_8
level
to
similarly
maximize
the
disk
quality_attribute_2
bookkeeper
by
default
control
by
the
journalsyncdata=true
bookie
config
fsyncs
connector_14
to
the
disk
to
cover
all
base
we
test
technology_2
with
journalsyncdata=false
configure
on
bookkeeper
for
an
apple
to
apple
comparison
with
kafka’s
default
and
recommend
set
of
not
fsyncing
on
every
individual
connector_data_1
however
we
encounter
large
quality_attribute_3
and
instability
on
the
bookkeeper
bookie
indicate
component_16
relate
to
flush
we
also
verify
the
same
behavior
use
the
technology_2
perf
technology_21
that
ship
with
technology_2
a
far
a
we
can
tell
after
consult
the
technology_2
this
appear
to
be
a
bug
so
we
choose
to
exclude
it
from
our
test
nonetheless
give
we
could
see
the
disk
be
maxed
out
on
quality_attribute_2
with
journalsyncdata=true
we
believe
it
will
not
affect
the
final
outcome
anyway
figure
illustration
of
pulsar’s
requirement_7
with
bookkeeper’s
journalsyncdata=false
show
quality_attribute_2
drop
and
quality_attribute_3
spike
figure
bookkeeper
journal
pattern_15
component_16
growth
with
journalsyncdata=false
technology_3
operate
with
a
quality_attribute_22
component_16
that
persist
connector_data_2
to
the
disk
if
and
only
if
the
connector_data_2
have
not
already
be
connector_10
unlike
technology_1
and
technology_2
however
technology_3
do
not
support
‘rewinding’
of
component_7
to
connector_8
old
connector_data_2
yet
again
from
a
quality_attribute_6
standpoint
our
benchmark
indicate
that
the
component_18
keep
up
with
the
component_17
and
thus
we
do
not
notice
any
connector_14
to
the
disk
we
also
set
up
technology_3
to
connector_24
the
same
quality_attribute_8
guarantee
a
technology_1
and
technology_2
by
use
mirror
component_7
in
a
cluster
of
three
pattern_3
test
setup
the
experiment
be
design
accord
to
the
follow
principle
and
expect
guarantee
connector_data_2
be
replicate
3x
for
fault
tolerance
see
below
for
specific
configs
we
enable
pattern_11
for
all
three
component_5
to
optimize
for
quality_attribute_2
we
pattern_11
up
to
connector_data_3
of
connector_data_5
for
a
maximum
of
m
technology_2
and
technology_1
be
configure
with
component_22
across
one
topic
technology_3
do
not
support
component_22
in
a
topic
to
match
the
technology_1
and
technology_2
setup
we
declare
a
single
direct
exchange
equivalent
to
a
topic
and
connector_25
component_7
equivalent
to
component_22
more
specific
on
this
setup
can
be
find
below
omb
u
an
auto
rate
discovery
algorithm
that
derive
the
target
component_17
quality_attribute_2
dynamically
by
probe
the
backlog
at
several
rat
we
saw
wild
technology_22
in
the
determine
rate
go
from
connector_data_1
s
to
connector_data_1
s
in
many
requirement_4
these
hurt
the
quality_attribute_23
and
quality_attribute_24
of
the
experiment
significantly
in
our
experiment
we
explicitly
configure
target
quality_attribute_2
without
use
this
feature
and
steadily
increase
the
target
quality_attribute_2
across
10k
50k
100k
200k
500k
and
million
component_17
connector_data_2
per
second
with
four
component_20
and
four
component_26
use
kb
connector_data_1
we
then
observe
the
maximum
rate
at
which
each
component_4
offer
quality_attribute_20
end
end
requirement_7
for
different
configuration
quality_attribute_2
connector_data_4
we
find
technology_1
connector_24
the
high
quality_attribute_2
of
the
component_5
we
compare
give
it
design
each
byte
produce
be
connector_11
once
onto
disk
on
a
path
that
have
be
optimize
for
almost
a
decade
by
thousand
of
organization
across
the
world
we
will
delve
into
these
connector_data_4
in
more
detail
for
each
component_4
below
figure
comparison
of
peak
quality_attribute_20
quality_attribute_2
for
all
three
component_4
topic
component_22
with
kb
connector_data_1
use
four
component_20
and
four
component_26
we
configure
technology_1
to
use
pattern_11
size=1mb
and
linger
ms=10
for
the
component_17
to
effectively
pattern_11
connector_14
connector_26
to
the
pattern_3
in
addition
we
configure
acks=all
in
the
component_17
along
with
min
insync
replicas=2
to
ensure
every
connector_data_1
be
replicate
to
at
least
two
pattern_3
before
acknowledge
it
back
to
the
component_17
we
observe
that
technology_1
be
able
to
efficiently
max
out
both
the
disk
on
each
of
the
brokers—the
ideal
outcome
for
a
storage
component_4
see
kafka’s
driver
configuration
for
detail
figure
technology_1
requirement_7
use
the
default
recommend
fsync
set
the
graph
show
i
o
utilization
on
technology_1
pattern_3
and
the
correspond
component_17
component_18
quality_attribute_2
component_1
prometheus
technology_14
metric
see
raw
connector_data_4
for
detail
we
also
benchmarked
technology_1
with
the
alternative
configuration
of
fsyncing
every
connector_data_1
to
disk
on
all
replica
use
flush
messages=1
and
flush
ms=0
before
acknowledge
the
connector_11
the
connector_data_4
be
show
in
the
follow
graph
and
be
quite
close
to
the
default
configuration
figure
prometheus
technology_14
metric
show
i
o
utilization
on
technology_1
pattern_3
and
the
correspond
component_17
component_18
quality_attribute_2
see
raw
connector_data_4
for
detail
pulsar’s
component_17
work
differently
from
technology_1
in
term
of
how
it
component_7
produce
connector_data_8
specifically
it
have
per
component_22
component_17
component_7
internally
a
well
a
limit
for
these
component_16
size
that
place
an
upper
bind
on
the
number
of
connector_data_2
across
all
component_22
from
a
give
component_17
to
avoid
the
technology_2
component_17
from
bottleneck
on
the
number
of
connector_data_2
be
connector_26
out
we
set
both
the
per
component_22
and
global
limit
to
infinity
while
match
with
a
connector_data_3
byte
base
pattern_11
limit
batchingmaxbytes
1mb
batchingmaxmessages
max_value
maxpendingmessagesacrosspartitions
max_value
we
also
give
technology_2
a
high
time
base
pattern_11
limit
batchingmaxpublishdelayms=50
to
ensure
the
pattern_11
kick
in
primarily
due
to
byte
limit
we
arrive
at
this
requirement_15
by
continuously
increasing
the
requirement_15
to
the
point
that
it
have
no
measurable
effect
on
the
peak
quality_attribute_20
quality_attribute_2
that
technology_2
ultimately
achieve
for
the
pattern_9
configuration
we
use
ensemblesize=3
writequorum=3
ackquorum=2
which
be
equivalent
to
how
technology_1
be
configure
see
the
technology_2
benchmark
driver
configs
for
detail
with
bookkeeper’s
design
where
bookie
connector_11
connector_data_5
locally
into
both
a
journal
and
a
ledger
we
notice
that
the
peak
quality_attribute_20
quality_attribute_2
be
effectively
half
of
what
technology_1
be
able
to
achieve
we
find
that
this
fundamental
design
choice
have
a
profound
negative
impact
on
quality_attribute_2
which
directly
affect
cost
once
the
journal
disk
be
fully
saturated
on
the
bookkeeper
bookie
the
component_17
rate
of
technology_2
be
cap
at
that
point
figure
prometheus
technology_14
metric
show
bookkeeper
journal
disk
maxed
out
for
technology_2
and
the
connector_data_6
quality_attribute_2
measure
at
the
bookkeeper
bookie
see
raw
connector_data_4
for
detail
to
further
validate
this
we
also
configure
bookkeeper
to
use
both
disk
in
a
raid
configuration
which
provide
bookkeeper
the
opportunity
to
stripe
journal
and
ledger
connector_14
across
both
disk
we
be
able
to
observe
that
technology_2
maxed
out
the
combine
quality_attribute_2
of
the
disk
~650
connector_data_3
s
but
be
still
limit
to
~340
connector_data_3
s
of
peak
quality_attribute_20
quality_attribute_2
figure
prometheus
technology_14
metric
show
bookkeeper
journal
disk
still
maxed
out
with
a
raid
configuration
figure
prometheus
technology_14
metric
show
raid
disk
connector_27
maxed
out
and
the
connector_data_6
quality_attribute_2
measure
at
the
technology_2
pattern_3
see
raw
connector_data_4
for
detail
technology_2
have
a
tiered
architecture
that
separate
the
bookkeeper
bookie
storage
from
the
technology_2
pattern_3
pattern_13
pattern_16
for
the
storage
for
completeness
we
therefore
also
run
the
quality_attribute_2
test
above
in
a
tiered
deployment
that
move
the
technology_2
pattern_3
to
three
additional
compute
optimize
c5n
2xlarge
with
vcores
gb
ram
upto
gbps
requirement_11
transfer
eb
back
storage
instance
the
bookkeeper
technology_14
remain
on
the
storage
optimize
i3en
2xlarge
instance
this
connector_data_6
in
a
total
of
six
instance
resource
for
technology_2
and
bookkeeper
in
this
special
setup
with
2x
extra
cpu
resource
and
33%
extra
memory
than
what
be
give
to
technology_1
and
technology_3
even
at
the
high
quality_attribute_2
the
component_4
be
mostly
i
o
bind
and
we
do
not
find
any
improvement
from
this
setup
see
the
component_27
below
for
the
full
connector_data_4
of
this
specific
run
in
fact
without
any
real
cpu
bottleneck
pulsar’s
two
tier
architecture
simply
seem
to
more
overhead—two
jvms
take
up
more
memory
twice
the
requirement_11
transfer
and
more
move
part
in
the
component_4
architecture
we
expect
that
when
requirement_11
constraint
unlike
our
test
which
offer
a
surplus
of
requirement_11
bandwidth
pulsar’s
two
tier
architecture
would
exhaust
the
requirement_11
resource
twice
a
quickly
and
thereby
reduce
requirement_7
technology_2
deployment
component_28
peak
produce
quality_attribute_2
connector_data_3
s
tiered
co
locate
technology_3
unlike
both
technology_1
and
technology_2
do
not
feature
the
concept
of
component_22
in
a
topic
instead
technology_3
u
an
exchange
to
connector_28
connector_data_2
to
connector_25
component_16
use
either
attribute
exchange
connector_19
key
direct
and
topic
exchange
or
bind
fanout
exchange
from
which
component_26
can
component_9
connector_data_1
to
match
the
setup
for
the
workload
we
declare
a
single
direct
exchange
equivalent
to
a
topic
and
connector_25
component_7
equivalent
to
component_22
each
dedicate
to
serve
a
specific
connector_19
key
end
to
end
we
have
all
component_20
generate
connector_data_2
with
all
connector_19
key
round
robin
and
component_26
dedicate
to
each
component_16
we
also
optimize
technology_3
with
best
practice
that
be
suggest
by
the
pattern_9
enable
component_16
be
replicate
to
all
technology_14
in
the
cluster
connector_data_1
persistence
disable
component_16
be
in
memory
only
component_18
auto
acking
enable
load
balance
component_7
across
pattern_3
component_7
since
technology_3
u
a
dedicate
core
per
component_16
vcpus
x
pattern_3
technology_3
do
not
fare
well
with
the
overhead
of
pattern_9
which
severely
reduce
the
quality_attribute_2
of
the
component_4
we
notice
all
the
technology_14
be
cpu
bind
during
this
workload
cf
the
green
line
with
the
right
y
technology_23
in
the
figure
below
leave
very
little
room
for
any
additional
connector_data_2
to
be
pattern_3
see
the
technology_3
driver
configuration
for
detail
figure
technology_3
quality_attribute_2
+
cpu
usage
see
the
raw
connector_data_4
connector_data_5
for
detail
quality_attribute_3
test
give
the
ever
grow
popularity
of
connector_1
component_9
and
pattern_1
architecture
another
key
aspect
of
pattern_4
component_5
be
end
to
end
quality_attribute_3
for
a
connector_data_1
to
traverse
the
pipeline
from
the
component_17
through
the
component_4
to
the
component_18
we
design
an
experiment
to
compare
this
on
all
three
component_5
at
the
high
quality_attribute_20
quality_attribute_2
that
each
component_4
could
sustain
without
show
any
sign
of
over
utilization
to
optimize
for
quality_attribute_3
we
connector_29
the
component_17
configuration
across
all
component_5
to
pattern_11
connector_data_2
up
to
a
maximum
of
m
only
versus
the
m
we
use
for
quality_attribute_2
test
and
to
also
leave
each
component_4
at
it
default
recommend
configuration
while
ensure
high
quality_attribute_8
technology_1
be
configure
to
use
it
default
fsync
set
i
e
fsync
off
and
technology_3
be
configure
to
not
persist
connector_data_2
while
still
mirror
the
component_16
base
on
repeat
run
we
choose
to
compare
technology_1
and
technology_2
at
200k
connector_data_1
s
or
200mb
s
which
be
below
the
single
disk
quality_attribute_2
limit
of
connector_data_3
s
on
this
testbed
we
observe
that
at
quality_attribute_2
high
than
30k
connector_data_1
s
technology_3
would
face
cpu
bottleneck
quality_attribute_3
connector_data_4
figure
end
to
end
quality_attribute_3
for
the
technology_13
mode
configure
for
high
quality_attribute_8
measure
at
200k
connector_data_1
s
kb
connector_data_1
size
on
technology_1
and
technology_2
and
at
only
30k
connector_data_1
s
on
technology_3
because
it
couldn’t
sustain
a
high
load
note
quality_attribute_3
m
—lower
be
quality_attribute_5
technology_1
consistently
connector_6
lower
quality_attribute_3
than
technology_2
technology_3
achieve
the
low
quality_attribute_3
among
the
three
component_4
but
only
at
a
much
lower
quality_attribute_2
give
it
limit
vertical
quality_attribute_25
since
the
experiment
be
deliberately
set
up
so
that
for
each
component_4
component_26
be
always
able
to
keep
up
with
the
component_17
almost
all
of
the
connector_30
be
serve
off
of
the
pattern_13
memory
for
all
three
component_4
much
of
kafka’s
requirement_7
can
be
attribute
to
a
heavily
optimize
connector_8
implementation
for
component_18
build
on
quality_attribute_4
connector_data_5
organization
without
any
additional
overhead
connector_data_5
skip
technology_1
deeply
leverage
the
linux
component_25
pattern_13
and
zero
copy
mechanism
to
avoid
copy
connector_data_5
into
component_29
space
typically
many
component_4
component_30
have
build
out
component_8
level
pattern_13
that
give
them
more
quality_attribute_26
to
support
random
connector_8
connector_11
workload
however
for
a
pattern_4
component_4
rely
on
the
component_25
pattern_13
be
a
great
choice
because
typical
workload
do
sequential
connector_8
connector_11
the
linux
kernel
have
be
optimize
for
many
year
to
be
smart
about
detecting
these
pattern_17
and
employ
technique
readahead
to
vastly
improve
connector_8
requirement_7
similarly
build
on
top
of
the
component_25
pattern_13
allow
technology_1
to
employ
sendfile
base
requirement_11
transfer
that
avoid
additional
connector_data_5
copy
stay
consistent
with
the
quality_attribute_2
test
we
also
run
the
same
test
by
configure
technology_1
to
fsync
every
connector_data_1
technology_2
take
a
very
different
approach
to
pattern_13
than
technology_1
and
some
of
it
stem
from
the
core
design
choice
in
bookkeeper
to
separate
the
journal
and
ledger
storage
technology_2
employ
multiple
pattern_18
of
pattern_13
in
addition
to
the
linux
component_25
pattern_13
namely
a
readahead
pattern_13
on
bookkeeper
bookie
retain
the
omb
default
of
dbstorage_readaheadcachemaxsizemb=1024
in
our
test
manage
ledger
managedledgercachesizemb
20%
of
quality_attribute_13
direct
memory
of
gb
=
gb
in
our
test
we
do
not
observe
any
benefit
to
this
multi
pattern_18
pattern_13
in
our
test
in
fact
pattern_13
multiple
time
could
increase
the
overall
cost
of
deployment
and
we
suspect
that
there
be
a
fair
bit
of
pad
in
the
gb
off
heap
usage
to
avoid
hit
technology_16
gc
issue
with
direct
byte
buffer
a
mention
early
rabbitmq’s
requirement_7
be
a
factor
of
both
the
exchange
at
the
component_17
side
and
the
component_7
bind
to
these
exchange
at
the
component_18
side
we
use
the
same
mirror
setup
from
the
quality_attribute_2
experiment
for
the
quality_attribute_3
experiment
specifically
direct
exchange
and
mirror
component_16
due
to
cpu
bottleneck
we
be
not
able
to
drive
a
quality_attribute_2
high
than
38k
connector_data_1
s
and
any
attempt
to
measure
quality_attribute_3
at
this
rate
show
significant
degradation
in
requirement_7
clocking
a
p99
quality_attribute_3
of
almost
two
second
gradually
lower
the
quality_attribute_2
down
from
38k
connector_data_1
s
down
to
30k
connector_data_1
s
allow
u
to
establish
a
quality_attribute_20
quality_attribute_2
at
which
point
the
component_4
do
not
appear
to
be
over
utilize
this
be
confirm
by
a
significantly
quality_attribute_5
p99
quality_attribute_3
of
m
we
believe
the
overhead
of
replicate
component_7
across
three
technology_14
seem
to
have
a
profound
negative
impact
on
end
to
end
quality_attribute_3
at
high
quality_attribute_2
while
quality_attribute_2
le
than
30k
connector_data_1
s
or
connector_data_3
s
solid
magenta
line
allow
technology_3
to
connector_24
significantly
lower
end
to
end
quality_attribute_3
than
the
other
two
component_4
in
general
follow
it
best
practice
allow
technology_3
to
provide
bind
quality_attribute_3
give
that
the
quality_attribute_3
experiment
be
deliberately
set
up
so
that
the
component_26
be
always
able
to
keep
up
with
the
component_17
the
quality_attribute_27
of
rabbitmq’s
pattern_4
pipeline
boil
down
to
the
number
of
component_31
switch
the
technology_24
technology_25
vm
and
thus
the
cpu
need
to
do
to
component_9
the
component_16
thus
limit
this
by
dedicate
a
component_16
per
cpu
core
connector_6
the
low
possible
quality_attribute_3
in
addition
use
direct
or
topic
exchange
allow
for
sophisticate
connector_19
similar
to
component_26
that
be
dedicate
to
a
component_22
on
technology_1
and
technology_2
to
specific
component_16
but
direct
exchange
offer
quality_attribute_5
requirement_7
owe
to
the
absence
of
wildcard
match
which
more
overhead
and
be
the
suitable
choice
for
this
test
figure
end
to
end
quality_attribute_3
for
technology_1
technology_2
and
technology_3
measure
at
200k
connector_data_1
s
kb
connector_data_1
size
on
technology_1
and
technology_2
and
30k
connector_data_1
s
on
technology_3
see
the
raw
connector_data_4
technology_1
technology_2
and
technology_3
for
detail
note
quality_attribute_3
m
—lower
be
quality_attribute_5
we
already
cover
the
quality_attribute_3
connector_data_4
of
technology_1
in
it
default
recommend
fsync
configuration
solid
green
line
at
the
begin
of
this
section
in
the
alternative
configuration
where
technology_1
fsync’s
every
connector_data_1
to
disk
dot
green
line
we
find
that
technology_1
still
have
lower
quality_attribute_3
than
technology_2
almost
up
until
the
p99
9th
percentile
while
technology_2
blue
line
fare
quality_attribute_5
on
the
high
tail
percentile
while
reason
about
tail
quality_attribute_3
accurately
at
the
p99
9th
percentile
and
above
be
difficult
we
believe
that
the
non
linear
quality_attribute_3
shoot
up
at
the
p99
9th
percentile
for
the
alternative
technology_1
fsync
configuration
dot
green
line
could
be
attribute
to
corner
requirement_4
involve
the
technology_1
component_17
give
that
the
component_17
quality_attribute_3
seem
to
follow
the
same
trend
quality_attribute_3
requirement_8
off
figure
end
to
end
quality_attribute_3
for
technology_3
mirror
component_7
configuration
use
in
the
test
versus
classic
component_7
no
pattern_9
at
10k
20k
30k
and
40k
connector_data_1
s
note
the
quality_attribute_1
on
the
y
technology_23
be
logarithmic
in
this
requirement_14
we
acknowledge
that
every
component_4
be
design
with
certain
requirement_8
off
though
unfair
to
technology_1
and
technology_2
we
find
it
interest
to
compare
technology_3
also
in
a
configuration
that
do
not
connector_24
high
quality_attribute_8
versus
technology_1
and
technology_2
both
of
which
requirement_8
lower
quality_attribute_3
for
provide
strong
quality_attribute_6
guarantee
along
with
3x
more
quality_attribute_8
than
technology_3
this
could
be
relevant
to
certain
use
requirement_4
e
g
component_32
location
track
where
it
can
be
acceptable
to
trade_off
quality_attribute_8
for
quality_attribute_5
requirement_7
especially
if
the
use
requirement_4
demand
real
time
pattern_4
and
be
not
sensitive
to
quality_attribute_8
issue
our
connector_data_4
indicate
that
technology_3
can
sustain
lower
quality_attribute_3
at
high
quality_attribute_2
quality_attribute_5
when
pattern_9
be
disable
albeit
even
the
improve
quality_attribute_2
100k
connector_data_1
s
be
still
significantly
lower
than
what
technology_1
and
technology_2
can
achieve
even
though
technology_1
and
technology_2
be
slow
clocking
~5
m
and
~25
m
respectively
at
p99
the
quality_attribute_6
high
quality_attribute_2
and
high
quality_attribute_8
they
provide
be
critical
for
large
quality_attribute_1
connector_2
use
requirement_4
component_9
financial
transaction
or
retail
inventory
requirement_12
for
use
requirement_4
that
demand
lower
quality_attribute_3
technology_3
can
achieve
a
p99
~1
m
quality_attribute_3
only
a
long
a
it’s
lightly
load
since
connector_data_2
be
component_16
in
memory
with
no
overhead
of
pattern_9
in
practice
the
operator
need
to
carefully
provision
technology_3
to
keep
the
rat
low
enough
to
sustain
these
low
quality_attribute_3
bar
which
the
quality_attribute_3
degrade
quickly
and
significantly
but
this
connector_data_9
be
difficult
and
even
practically
impossible
to
achieve
in
a
general
fashion
across
all
use
requirement_4
overall
a
quality_attribute_5
architectural
choice
with
lower
operational
overhead
and
cost
might
be
to
pick
a
single
quality_attribute_22
component_4
technology_1
for
all
use
requirement_4
where
the
component_4
can
provide
the
best
quality_attribute_2
with
low
quality_attribute_3
across
all
load
level
summary
in
this
we
have
present
a
thorough
balance
analysis
of
three
pattern_4
component_4
technology_1
technology_3
and
technology_2
which
lead
to
the
follow
conclusion
quality_attribute_2
technology_1
provide
the
high
quality_attribute_2
of
all
component_4
connector_31
15x
fast
than
technology_3
and
2x
fast
than
technology_2
quality_attribute_3
technology_1
provide
the
low
quality_attribute_3
at
high
quality_attribute_2
while
also
provide
strong
quality_attribute_6
and
high
quality_attribute_8
technology_1
in
it
default
configuration
be
fast
than
technology_2
in
all
quality_attribute_3
benchmark
and
it
be
fast
up
to
p99
when
set
to
fsync
on
every
connector_data_1
technology_3
can
achieve
lower
end
to
end
quality_attribute_3
than
technology_1
but
only
at
significantly
lower
quality_attribute_2
cost
complexity
cost
tend
to
be
an
inverse
of
requirement_7
technology_1
a
the
component_4
with
the
high
quality_attribute_20
quality_attribute_2
offer
the
best
requirement_15
i
e
cost
per
byte
connector_11
of
all
the
component_4
due
to
it
quality_attribute_4
design
in
fact
twitter’s
technology_1
journey
of
move
away
from
a
bookkeeper
base
architecture
technology_2
corroborate
our
observation
that
kafka’s
few
move
part
lower
it
cost
significantly
up
to
75%
in
twitter’s
requirement_4
additionally
the
work
on
remove
technology_19
from
technology_7
technology_1
see
kip
be
well
underway
and
further
simplify
kafka’s
architecture
while
this
have
focus
entirely
on
requirement_7
there
be
much
more
to
talk
about
when
compare
quality_attribute_7
component_4
if
you
be
interest
in
more
about
the
nuanced
requirement_8
off
in
quality_attribute_7
component_5
design
for
technology_1
rabbit
technology_2
and
similar
component_4
stay
tune
for
a
follow
up
and
connector_32
out
this
podcast
episode
in
the
meantime
if
you’re
look
to
connector_4
start
with
technology_1
the
fast
way
be
to
use
confluent
requirement_1
when
you
sign
up
you’ll
connector_33
$400
to
spend
within
confluent
requirement_1
during
your
first
day
and
you
can
use
the
promo
cl60blog
to
connector_33
an
additional
$60
of
free
usage
*
start
free
alok
nikhil
be
a
engineer
at
confluent
work
on
the
requirement_1
requirement_16
technology_1
team
prior
to
join
confluent
he
work
on
serverless
relational
component_30
at
web
component_3
and
high
requirement_7
vector
packet
processor
at
cisco
research
vinoth
chandar
drive
various
effort
around
connector_1
component_9
at
confluent
prior
to
join
confluent
vinoth
have
build
large
quality_attribute_1
mission
critical
infrastructure
component_5
at
requirement_17
uber
and
linkedin
vinoth
be
also
the
co
creator
of
the
technology_7
hudi
project
which
have
connector_29
the
face
of
connector_data_5
lake
architecture
over
the
past
few
year
vinoth
have
a
keen
interest
in
unify
connector_data_5
storage
component_9
architecture
do
you
this
connector_34
it
nowsubscribe
to
the
confluent
blogsubscribemore
thisi
nearly
technology_7
technology_1
expert
and
i
these
thingsmany
lead
light
of
the
technology_7
kafka®
have
appear
a
guest
on
connector_2
audio
at
one
time
or
another
in
the
past
three
year
but
some
of
it
episodesreadhow
to
survive
an
technology_7
technology_1
outagethere
be
a
of
component_6
that
cannot
afford
to
be
unavailable—for
example
external
face
entry
point
into
your
organization
typically
anything
your
requirement_5
connector_35
with
directly
cannot
go
down
asreadcontainerized
test
with
technology_26
and
sshkerberos
pattern_19
be
widely
use
in
today’s
pattern_20
component_8
however
connector_27
start
with
technology_26
be
a
daunt
connector_data_9
if
you
don’t
have
prior
experience
connector_data_10
on
set
up
kerberosreadproductconfluent
platformconnectorsksqldbstream
governanceconfluent
hubsubscriptionprofessional
servicestrainingcustomerscloudconfluent
cloudsupportsign
uplog
incloud
faqsolutionsfinancial
servicesinsuranceretail
and
ecommerceautomotivegovernmentgamingcommunication
component_12
providerstechnologymanufacturingfraud
detectioncustomer
360messaging
modernizationstreaming
etlevent
drive
microservicesmainframe
offloadsiem
optimizationhybrid
and
multicloudinternet
of
thingsdata
warehousedevelopersconfluent
developerwhat
be
technology_1
resourceseventsonline
talksmeetupskafka
summittutorialsdocsblogaboutinvestor
relationscompanycareerspartnersnewscontacttrust
and
securityalso
of
interestkafka
vs
pulsarimplementingmessageprioritizationinapachekafkaapachekafka®performanceterms
&
condition
|
privacy
requirement_13
|
do
not
sell
my
connector_data_10
|
modern
slavery
requirement_13
|
settingscopyright
©
confluent
inc
technology_7
technology_7
technology_1
technology_1
and
associate
open_source
project
name
be
trademark
of
the
technology_7
foundation
