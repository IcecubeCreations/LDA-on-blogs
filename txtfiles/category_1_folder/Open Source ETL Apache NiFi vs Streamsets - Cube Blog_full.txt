open_source
technology_1
technology_2
nifi
vs
streamsets
cube
blogdata
engineeringapplication
developmentall
thing
cubejoin
u
on
slackget
cubeopen
component_1
technology_1
technology_2
nifi
vs
streamsetschoosing
between
mainstream
open_source
technology_1
projectsapril

2018extract
load
transform
technology_1
dmitry
dorofeevtable
of
content
dataflow
programming
technology_2
nifi
vs
streamsets
architecture
and
feature
ui
conclusion
while
work
with
cube
j
technology_3
we
ve
see
a
lot
of
diffrent
technology_1
technology_4
use
by
connector_data_1
engineer
nowadays
most
of
them
require
connector_1

but
there
be
some
visual
technology_1
you
can
try
a
well
we
ask
dmitry
dorofeev
head
of
r&d
at
luxms
group
to
tell
u
about
his
experience
with
compare
technology_2
nifi
and
streamsets
our
team
at
luxms
inc
have
recently
face
a
boring
connector_data_1
requirement_1
problem
when
some
connector_data_1
be
component_2
in
technology_5
some
in
technology_6
and
a
little
bit
be
in

the
goal
be
to
technology_1
all
that
connector_data_1
into
greenplum
and
finally
provide
some
pattern_1
on
top
of
it
we
quickly
find

mainstream
open_source
technology_1
project
technology_2
nifi
and
streamsets
and
it
seem
an
easy
connector_data_2
to
choose
one
technology_7
out
of
the
two
in
no
way
be
it
easy
i
that
quality_attribute_1
than
anyone
since
i
be
responsible
for
the
technology_7
evaluation
and
the
final
choice
in
this

i’d
to
connector_2
my
experience
and
maybe
connector_3
day
of
your
life
spoiler
there
be
no
silver
bullet
technology_2
nifi
be
not
necessarily
quality_attribute_1
than
streamsets
nor
streamsets
quality_attribute_1
than
nifi
everything
have
it
pro
and
con
this
be
my
personal
experience
with
these
technology_4
a
a
novice
component_3
without
any
introductory
train
dataflow
programming
programmer
analyst
and
even
manager
often
draw
a
component_4
and
arrow
diagram
to
illustrate
some
flow
you
can
even
use
these
component_5
and
arrow
to
create
component_6
we
can
track
such
attempt
back
to
the
1960s
when
the
dataflow
programming
paradigm
be
bear
in
mit
technology_1
technology_4
connector_data_3
overview
&
requirement_2
today
we
have
ten
of
dataflow
programming
technology_4
where
you
can
visually
assemble
component_7
from
component_5
and
arrow
connector_1
zero
line
of

some
of
them
be
open_source
and
some
be
suitable
for
technology_1
yes
you
don’t
have
to
any
programming
technology_8
you
use
ready
make
“processors”
represent
with
component_4
connector_4
them
with
arrow
which
represent
exchange
of
connector_data_1
between
“processors
”
and
that’s
it
there
be
three
type
of
component_4
component_1
processor
and
connector_5
think
extract
for
component_1
transform
for
processor
and
load
for
connector_5
almost
anything
can
be
a
component_1
for
example
on
the
disk
or
technology_9
technology_10
query
technology_5
web
component_8
technology_11
technology_12
technology_13
twitter
or
technology_14
connector_data_4
a
processor
can
enhance
verify
pattern_2
join
split
or
adjust
connector_data_1
if
ready
make
processor
component_5
be
not
enough
you
can
on
technology_15
shell
technology_16
or
even
technology_17
for
connector_data_1
transformation
connector_5
be
basically
the
same
a
component_1
but
they
be
design
for
connector_1
connector_data_1
technology_2
nifi
vs
streamsets
when
we
face
yet
another
requirement_3
with
complicate
technology_1
requirement
i
decide
to
try
visual
dataflow
technology_4
visual
might
be
attractive
even
if
you
use
singer
connector_data_1
build
technology_4
or
other
handy
open_source
technology_1
technology_4
right
luckily
there
be
two
open_source
visual
technology_4
with
the
web

technology_2
nifi
and
streamsets
connector_data_1
collector
sdc
nifi
be
donate
by
the
nsa
to
the
technology_2
foundation
in

and
current
development
and
support
be
provide
mostly
by
hortonworks
sdc
be
start
by
a
california
base
startup
in

a
an
open_source
technology_1
project
quality_attribute_2
on
technology_18
the
first
release
be
publish
in

both
technology_7
be
connector_6
in
technology_19
and
quality_attribute_3
under
the
technology_2


license
here
be
some
stats
from
technology_20
for
early

metric
technology_2
nifi
streamsetsfirst
release
year

2015forks

914releases

113stars

405version






architecture
and
feature
both
technology_4
encourage
creation
of
long
run
which
work
with
either
connector_7
connector_data_1
or
regular
periodic
pattern_3
you
can
create
manually
manage

but
they
might
be
tricky
to
set
up
this
be
the
greatest
surprise
and
mind
shift
feature
i
personally
have
with
these
technology_4
technology_2
nifi
technology_2
nifi
have
a
well
think
out
architecture
once
connector_data_1
be
fetch
from
external
component_1
it
be
represent
a
flowfile
inside
technology_2
nifi
dataflows
flowfile
be
basically
original
connector_data_1
with
meta
connector_data_5
attach
to
it
you
can
easily
component_9
not
only
csv
or
other
component_10
base
connector_data_1
but
also
picture
video
audio
or
any
binary
connector_data_1
a
processor
usually
will
have

output
failure
if
a
flowfile
cannot
be
component_9
correctly
the
original
flowfile
will
be
connector_8
to
this
output
original
once
an
incoming
flowfile
have
be
component_9
the
original
flowfile
be
connector_8
to
this
output
success
flowfiles
that
be
successfully
component_9
will
be
connector_8
to
this
relationship
you
can
terminate
output
with
checkboxes
so
technology_2
nifi
will
ignore
terminate
output
and
will
not
connector_9
any
flowfiles
there
another
handy
feature
be
component_9
group
when
a
dataflow
become
complex
you
can
combine
your
dataflow
element
into
component_9
group
which
be
graphically
represent
in
the
ui
in
the
same
way
a
technology_21
processor
it
behave
a
a
processor
so
you
can
build
very
complex
dataflows
recursively
pattern_4
component_8
be
something
that
exist
outside
of
your
dataflows
but
provide
some
useful
connector_data_5
for
your
processor
it
might
be
technology_22
certificate
technology_10
connector_10
and
pool
setting
schema
definition
and
so
on
the
idea
be
that
rather
than
configure
this
connector_data_5
in
every
processor
that
might
need
it
the
pattern_4
component_8
provide
it
for
any
processor
to
use
processor
be
connector_11
with
well
connector_10
usually
connector_10
be
arrow
but
not
so
with
technology_2
nifi
every
connector_10
arrow
have
a
small
widget
attach
to
it
represent
a
component_11
with
back
pressure
and
can
be
configure
individually
for
example
if
the
logattribute
processor
become
slow
or
freeze
for
some
reason
flowfiles
generate
by
the
generateflowfile
processor
will
be
component_11
in
the
connector_10
after
some
time
back
pressure
will
pause
the
generateflowfile
processor
until
the
component_11
go
below
the
configure
threshold
if
you
be
not
yet
impress
how
about
different
component_11
requirement_4
pattern_5
lifo
and
others
you
can
apply
to
component_12
in
connector_10
connector_data_1
provenance
be
a
big
brother
component_8
which
component_13
almost
everything
in
your
dataflows
a
they
component_9
connector_data_1
this
be
very
handy
a
you
have
component_10
history
of
how
your
dataflow
perform
include
connector_3
content
of
the
flowfiles
but
that
come
with
a
requirement_2
you
should
have
enough
disk
space
to
keep
the
require
backlog
of
provenance
connector_data_1
even
with
these
awesome
feature
and
great
architecture
i
be
not
very
comfortable
with
the
technology_2
nifi
component_3

it
be
definitely
quality_attribute_4
but
not
sexy
streamsets
connector_data_1
collector
then
i
try
streamsets
processor
in
streamsets
exchange
component_10
that
mean
that
everything
you
ingest
into
streamsets
be
convert
automatically
into
the
technology_21
component_10
orient
technology_23
and
all
processor
can
handle
it
a
a
connector_12
of
component_10
there
be
no
component_12
in
between
processor
at
least
they
be
not
represent
visually
we
saw
it
in
technology_2
nifi
one
nice
thing
about
streamsets
be
that
it
can
component_9
binary
connector_data_1
some
component_1
such
a
technology_13
component_14
can
connector_13
connector_data_6
from
the
technology_13
topic
and
pass
them
to
other
processor
or
external
component_15
without
requirement_5
the
connector_data_7
of
the
binary
connector_data_8
into
the
component_10
technology_23
this
allow
u
to
connector_14
the
quality_attribute_5
connector_data_1
to
some
other
destination
with
minimum
overhead
the
more
powerful
option
be
the
whole
connector_data_1
technology_23
support
by
several
origin
include
technology_24
directory
technology_25
and
more
with
the
whole
technology_23
the
be
not
requirement_5
but
metadata
and
a
reference
to
the
content
be
connector_15
along
the
pipeline
processor
can
optionally
act
on
the
content
–
script
evaluator
and
custom
processor
can
connector_16
an
input
connector_12
to
the
content
but
in
the
default
requirement_6
once
the
whole
component_10
arrive
at
the
destination
the
connector_data_1
be
connector_12
directly
from
it
component_1
even
though
there
be
some
complaint
about
lack
of
binary
connector_data_1
support
in
streamsets
the
whole
support
have
be
there
since
version




release
in

in
technology_2
nifi
the
same
processor
should
have
different
version
of
itself
to
handle
different
technology_23
one
version
for
csv
one
for
technology_26
and
another
for
avro
for
example
you
might
guess
that
it
be
not
very
component_3
and
developer
friendly
this
be
connector_17
in
technology_2
nifi


where
most
processor
be
use
the
technology_27
technology_23
so
you
should
convert
to
technology_27
early
and
it
will
be
almost
the
same
experience
a
in
streamsets
after
that
to
connector_18
processor
setting
in
technology_2
nifi
you
must
stop
the
processor
while
in
streamsets
you
must
stop
the
whole
dataflow
that
mean
that
you
always
start
your
dataflow
from
the
begin
after
you
make
any
connector_19
in
it
with
streamsets
with
technology_2
nifi
you
have
a
chance
to
stop
a
misbehave
processor
fix
it
and
start
again
hopefully
component_11
flowfiles
will
be
connector_15
to
the
fix
processor
and
you
will
not
miss
the
connector_data_1
but
that
doesn’t
mean
that
streamsets
dataflows
be
hard
to
debug
actually
it
be
easy
you
have
a
nice
look
live
requirement_7
display
a
lot
of
statistic
for
every
processor
while
your
dataflow
be
run
error
be
cleanly
present
a
red
number
on
the
processor
icon
and
you
can
see
individual
error
for
every
faulty
component_10
with
a
mouse
click
you
even
put
component_10
pattern_2
on
the
connector_10
between
processor
to
inspect
component_13
in
question
pattern_2
can
be
apply
while
your
dataflow
be
run
so
i
use
it
a
live
debug
technology_4
streamsets
have

processor
type
origin
they
connector_16
connector_data_1
from
the
external
component_1
you
have
only
one
origin
processor
in
your
dataflow
processor
connector_data_1
transformer
destination
they
connector_3
connector_data_1
to
the
external
component_15
or

executor
they
component_9

generate
by
other
processor
some
of
the
streamsets
processor
generate

include
error
you
should
use
special
processor
connector_20
executor
to
handle
that
for
example
there
be
executor
which
can
connector_9

when
an
error
have
occur
i
be
definitely
more
happy
with
the
clean
technology_2
nifi
architecture
with
processor
and
pattern_4
component_8
but
the
streamsets
design
be
also
fine
and
can
be
quickly
pick
up
ui
technology_2
nifi
there
be
not
much
to
say
about
the
technology_2
nifi
ui
it
feel
spartan
and
it
be
very
easy
to
follow
thanks
to
the
great
architecture
with
minimum
concept
probably
the
only
drawback
i
discover
be
that
technology_2
nifi
will
not
autosize
text
for
your
long
technology_28
query
so
you
will
have
to
manually
resize
popup
text
every
time
you
want
to
edit
it
streamsets
streamsets
have
a
more
attractive
ui
but
it
be
not
perfect
a
well
the
first
thing
i
quickly
connector_16
annoy
with
be
the
absence
of
pattern_4
component_8
especially
for
technology_10
setting
you
need
to
fill
in
all
technology_10
setting
for
every
processor
that
connector_21
connector_data_1
from
the
same
technology_10
component_1
there
be
no
component_3
friendly
way
to
quality_attribute_6
such
connector_data_5
before
you
can
run
your
dataflow
streamsets
will
connector_22
each
processor
inside
your
dataflow
to
make
sure
all
processor
be
correctly
configure
it
sound
a
quality_attribute_1
thing
and
it
help
me
sometimes
but
other
time
–
harm
me
in
technology_2
nifi
you
can
have
disconnect
processor
and
i
usually
leave
them
so
for
debug
purpose
in
streamsets
you
can
not
do
the
same
since
all
the
processor
must
be
connector_11
to
make
dataflow
pass
validation
another
annoyance
be
that
you
can
not
select
several
processor
at
once
at
least
i
be
unable
to
do
so
move
a
dozen
processor
and
reorganize
them
one
by
one
on
the
screen
can
make
you
mad
streamsets
have
syntax
highlight
for
technology_28
which
be
a
nice
feature
but
not
always
useful
our
connector_data_1
engineer
create
heavy
technology_28
connector_23
which
can
easily
be
a
hundred
line
long
the
syntax
highlight
component_9
become
slow
and
that
connector_data_9
in
another
annoyance
if
you
edit
the
last
line
of
the
long
technology_28
query
the
caret
unexpectedly
move
to
the
begin
and
what
you
type
appear
on
the
first
line
conclusion
i
make
a
very
brief
introduction
to
technology_2
nifi
and
streamsets
they
have
plenty
of
useful
feature
not
cover
in
this

even
if
you
do
not
find
the
require
build
in
or
third
party
processor
you
can
always
use
technology_15
technology_29
r
or
even
technology_2
technology_17
to
component_6
your
complex
connector_data_1
transformation
component_16
in
the
technology_2
nifi
or
streamsets
dataflows
both
technology_2
nifi
and
streamsets
be
mature
open_source
technology_1
technology_4
they
have
very
similar
requirement_8
and
the
only
way
to
make
a
concise
choice
be
to
try
both
that’s
what
i
do
even
after

month
of
run
both
technology_7
i
can
not
see
a
clear
winner
for
me
live
pattern_6
be
the
single
feature
in
streamsets
that
outweigh
all
it
small
glitch
technology_2
nifi
pro
clean
and
well
think
out
implementation
of
the
dataflow
programming
concept
can
handle
binary
connector_data_1
connector_data_1
provenance
con
spartan
component_3
no
live
pattern_7
debug
feature
with
per
component_10
statistic
streamsets
pro
live
pattern_7
debug
feature
with
visual
per
component_10
statistic
for
every
processor
sexy
ui
well
suit
for
component_10
base
connector_data_1
and
connector_7
con
you
need
to
stop
the
whole
dataflow
to
edit
a
single
processor
configuration
no
quality_attribute_7
technology_10
configuration
for
processor
connector_2
this
articlestart
use
cube
requirement_9
todayget
start
for
freeget
cube
connector_data_10
to
your
inboxemailsubscribed
subscribeblogdata
engineeringapplication
developmentall
thing
cuberesourcesdocumentationtutorials
&
examplescommunityeventsawesome
toolscubeaboutcareerswe
re
hire
cube
cloudfollow
usterms
of
useprivacy
policy©

cube
dev
inc
