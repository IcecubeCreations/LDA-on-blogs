compare
technology_1
technology_2
storm
flink
and
samza
connector_1
component_1
component_2
part

home
what
we
do
our
work
who
we
be
career
news
u
scott
component_3
altogether
smart
career
news
what
we
do
our
work
who
we
be
connector_data_1
engineering
·


·

min
connector_2
compare
technology_1
technology_2
storm
flink
and
samza
connector_1
component_1
component_2
part

andrew
carr
andy
aspell
clark
the
rise
of
connector_1
component_1
component_2
quality_attribute_1
connector_1
component_1
component_2
have
be
on
the
rise
in
the
last
few
year
first
technology_3
become
popular
a
a
pattern_1
component_1
component_4
then
focus
shift
towards
connector_1
component_1
component_4
connector_1
component_1
component_2
can
make
the
of
component_1
connector_data_1
that
come
in
via
a
connector_1
easy
than
ever
before
and
by
use
cluster
can
enable
component_1
connector_data_1
in
large
set
in
a
timely
manner
handle
error
scenario
provide
common
component_1

and
make
connector_data_1
manipulation
easy
a
great
example
be
the
technology_4
syntax
that
be
become
common
to
component_1
connector_3
such
a
ksql
for
technology_5
and
technology_2
technology_4
for
technology_1
technology_2
i’ll
look
at
the
technology_4
manipulation
technology_6
in
another
a
they
be
a
large
use
requirement_1
in
themselves
in
part

we
will
show
example
for
a
quality_attribute_2
wordcount
connector_1
processor
in
four
different
connector_1
component_1
component_5
and
will
demonstrate
why
cod
in
technology_1
technology_2
or
flink
be
so
much
fast
and
easy
than
in
technology_1
technology_7
or
samza
in
part

we
will
look
at
how
these
component_5
handle
checkpointing
issue
and
failure
technology_1
technology_2
be
the
most
popular
component_4
which
support
connector_1
component_1

with
an
increase
of
40%
more
ask
for
technology_1
technology_2
skill
than
the
same
time
last
year
accord
to
it
watch
this
compare
to
only
a
7%
increase
in
look
for
technology_3
skill
in
the
same
period

technically
technology_1
technology_2
previously
only
support
pseudo
connector_1
component_1
which
be
more
accurately
connector_4
micro
pattern_1
but
in
technology_2


have
introduce
continuous
component_1
connector_5
mode
which
have
very
low
quality_attribute_3
a
true
connector_1
component_1
component_4
what
be
they
what
really
be
a
connector_1
component_1
component_4
well
they
be
technology_8
and
run
time
component_4
which
enable
the
developer
to
connector_6
to
do
some
form
of
component_1
on
connector_data_1
which
come
in
a
a
connector_1
without
have
to
worry
about
all
the
lower
level
mechanic
of
the
connector_1
itself
some
of
them
also
have
lot
of
technology_9
algorithm
out
of
the
component_6
to
enable
different
type
of
component_1
such
a
the
mllib
requirement_2
algorithm
in
technology_1
technology_2
component_1
component_2
in
general
typically
consider
the
component_1
pipeline
the
that
the
component_7
go
through
in
term
of
a
direct
acyclic
graph
or
dag
this
be
where
the
component_1
can
go
through
in
a
particular
order
where
the
can
be
chain
together
but
the
component_1
must
never
go
back
to
an
early
point
in
the
graph
a
in
the
diagram
below
type
of
component_1
component_2
there
be
two
type
of
component_1
component_2
declarative
where
you
simply
chain
together
and
the
component_4
work
out
the
correct
dag
and
then
pump
the
connector_data_1
through
compositional
where
the
developer
explicitly
define
the
dag
and
then
pump
the
connector_data_1
through
in
declarative
component_2
such
a
technology_1
technology_2
and
flink
the
cod
will
look
very
functional
a
be
show
in
the
example
below
plus
the
component_8
imply
a
dag
through
their
cod
which
could
be
optimise
by
the
component_4
in
compositional
component_2
such
a
technology_1
storm
samza
technology_10
the
cod
be
at
a
lower
level
a
the
component_8
be
explicitly
define
the
dag
and
could
easily
connector_6
a
piece
of
inefficient

but
the
be
at
complete
control
of
the
developer
to
see
the
two
type
in
action
let’s
consider
a
quality_attribute_2
piece
of
component_1
a
word
count
on
a
connector_1
of
connector_data_1
come
in
the
word
count
be
the
component_1
component_4
equivalent
to
printing
“hello
world”
so
we
be
look
to
connector_1
in
some
fix
sentence
and
then
count
the
word
come
out
to
compare
the
two
approach
let’s
consider
solution
in
technology_11
that
connector_7
each
type
of
component_4
technology_1
technology_7
architecture
and
example
word
count
the
technology_1
technology_7
architecture
be
base
on
the
concept
of
spout
and
bolt
spout
be
component_9
of
connector_data_2
and
connector_8
connector_data_2
to
one
or
more
bolt
which
can
then
be
chain
to
other
bolt
and
the
whole
topology
become
a
dag
the
topology
how
the
spout
and
bolt
be
connector_9
together
be
explicitly
define
by
the
developer
once
the
topology
be
up
it
stay
up
component_1
connector_data_1
connector_8
into
the
requirement_3
via
a
spout
until
the
requirement_3
be
stop
to
do
a
word
count
example
in
technology_1
storm
we
need
to
create
a
quality_attribute_2
spout
which
generate
sentence
to
be
connector_1
to
a
bolt
which
break
up
the
sentence
into
word
and
then
another
bolt
which
count
word
a
they
flow
through
the
output
at
each
stage
be
show
in
the
diagram
below
the
follow
example
be
take
from
the
admi
workshop
technology_1
technology_7
word
count
the
first
piece
of
be
a
random
sentence
spout
to
generate
the
sentence
randomsentencespout
extend
baserichspout
{
spoutoutputcollector
_collector
random
_rand
@override
open
connector_data_3
conf
topologycontext
component_10
spoutoutputcollector
collector
{
_collector
=
collector
_rand
=
random
}
@override
nexttuple
{
utils
sleep


sentence
=

{
the
cow
jump
over
the
moon
an
apple
a
day
keep
the
doctor
away
four
score
and
seven
year
ago
snow
white
and
the
seven
dwarf
i
be
at
two
with
nature
}
sentence
=
sentence
_rand
nextint
sentence
length
_collector
emit

requirement_4
sentence
}
@override
ack
connector_data_4

{
}
@override
fail
connector_data_4

{
}
@override
declareoutputfields
outputfieldsdeclarer
declarer
{
declarer
declare


word
}
}
then
you
need
a
bolt
to
split
the
sentence
into
word
splitsentence
extend
basebasicbolt
{
@override
declareoutputfields
outputfieldsdeclarer
declarer
{
declarer
declare


word
}
@override
connector_data_3

connector_data_4
getcomponentconfiguration
{

}
connector_10
tuple
tuple
basicoutputcollector
basicoutputcollector
{
sentence
=
tuple
getstringbyfield
sentence
word
=
sentence
split
\\s+
for

w
word
{
basicoutputcollector
emit

requirement_4
w
}
}
}
then
you
need
a
bolt
which
count
the
word
wordcount
extend
basebasicbolt
{
connector_data_3


count
=
hashmap


@override
connector_10
tuple
tuple
basicoutputcollector
collector
{
word
=
tuple
getstring

count
=
count
connector_11
word
if
count
==

count
=

count++
count
put
word
count
collector
emit

requirement_4
word
count
}
@override
declareoutputfields
outputfieldsdeclarer
declarer
{
declarer
declare


word
count
}
}
lastly
you
need
to
build
the
topology
which
be
how
the
dag
connector_12
define


args
throw
exception
{
topologybuilder
builder
=
topologybuilder
builder
setspout
spout
randomsentencespout

builder
setbolt
split
splitsentence

shufflegrouping
spout
builder
setbolt
count
wordcount

fieldsgrouping
split

word
config
conf
=
config
conf
setdebug
true
if
args
=
&&
args
length

{
conf
setnumworkers

stormsubmitter
submittopologywithprogressbar
args

conf
builder
createtopology
}
else
{
conf
setmaxtaskparallelism

localcluster
cluster
=
localcluster
cluster
submittopology
word
count
conf
builder
createtopology
component_11
sleep

cluster
shutdown
}
}
this
be
a
compositional
component_4
and
a
can
be
see
from
this
example
there
be
quite
a
lot
of
to
connector_11
the
basic
topology
up
and
run
and
a
word
count
work
this
be
in
clear
contrast
to
technology_1
technology_2
technology_1
technology_2
architecture
and
example
word
count
the
technology_1
technology_2
architecture
be
base
on
the
concept
of
rdds
or
resilient
quality_attribute_1
datasets
or
essentially
quality_attribute_1
immutable
component_12
of
connector_data_1
which
be
split
up
and
connector_13
to
component_13
to
be
connector_10
by
their
executor
it
be
very
similar
to
the
mapreduce
concept
of
have
a
control
component_1
and
delegate
component_1
to
multiple
technology_12
which
each
do
their
own
piece
of
component_1
and
then
combine
the
connector_data_5
to
make
a
complete
final
connector_data_6
for
technology_1
technology_2
the
technology_13
be
immutable
so
no
component_14
technology_12
can
modify
it
only
component_1
it
and
output
some
connector_data_6
lend
itself
well
to
the
functional
and
set
theory
base
programming
component_15
such
a
technology_14
the
technology_1
technology_2
word
count
example
take
from
technology_15
technology_2
technology_1

example
technology_16
can
be
see
a
follow
javardd

textfile
=
sc
textfile
technology_17
javapairrdd


count
=
textfile
flatmap
s

aslist
s
split
\\s+
iterator
maptopair
word
tuple2
word

reducebykey
a
b
a
+
b
count
saveastextfile
technology_17
this
be
essentially
connector_14
from
a

split
the
word
by
a
space
create
a
tuple
which
include
each
word
and
a
number

to
start
with
and
then
bring
them
all
together
and

the
count
up
none
of
the
be
concern
explicitly
with
the
dag
itself
a
technology_2
us
a
declarative
component_4
the
define
the
that
need
to
be
perform
on
the
connector_data_1
the
technology_2
technology_11
imply
the
dag
from
the
connector_data_7
technology_1
flink
architecture
and
example
word
count
technology_1
flink
us
the
concept
of
connector_3
and
transformation
which
make
up
a
flow
of
connector_data_1
through
it
component_16
connector_data_1
enter
the
component_16
via
a
“source”
and
exit
via
a
“sink”
to
create
a
flink
technology_18
be
use
to
create
a
skeleton
project
that
have
all
of
the
connector_15
and
packaging
requirement
setup
ready
for
custom
to
be

mvn
archetype
generate
darchetypegroupid=org
technology_1
flink
darchetypeartifactid=flink
quickstart
technology_19
darchetypeversion=1


technology_18
will
ask
for
a
group
and
artifact

for
our
example
wordcount
we
use
uk
co
scottlogic
a
the
and
wc
flink
a
the

once
technology_18
have
finish
create
the
skeleton
project
we
can
edit
the
streamingjob
technology_19
and
connector_16
the
in
line
with
the
flink
wordcount
example
on
technology_20


args
throw
exception
{
set
up
the
connector_17
connector_5
environment
final
streamexecutionenvironment
env
=
streamexecutionenvironment
getexecutionenvironment
textpath
=
text
name
datastreamsource

text
=
env
readtextfile
textpath
datastream
tuple2


count
=
split
up
the
line
into
pair

tuples
contain
word

text
flatmap

tokenizer
group
by
the
tuple

and
sum
up
tuple

keyby

sum

count
writeastext
output
directory
wcflink
connector_data_6
env
connector_10
connector_1
wordcount
}
we
also

the
tokenizer
from
the
example
final
tokenizer
connector_18
flatmapfunction

tuple2


{
private
final
long
serialversionuid
=
1l
@override
flatmap

requirement_4
collector
tuple2


out
throw
exception
{

connector_data_8
=
requirement_4
tolowercase
split
\\s+
for

connector_data_8
connector_data_8
{
if
connector_data_8
length

{
out
connector_19

tuple2


connector_data_8

}
}
}
}
we
can
now
compile
the
project
and
connector_10
it
mvn
clean
package
flink
root
dir
bin
flink



bin
flink
run
target
wc
flink


snapshot
jar
the
connector_data_5
of
the
wordcount
will
be
connector_20
in
the
wcflink
connector_data_6
in
the
output
directory
specify
flink
also
us
a
declarative
component_4
and
the
dag
be
imply
by
the
order
of
the
transformation
flatmap
keyby
sum
if
the
component_4
detect
that
a
transformation
do
not
quality_attribute_4
on
the
output
from
a
previous
transformation
then
it
can
reorder
the
transformation
technology_1
samza
architecture
and
example
word
count
technology_1
samza
be
base
on
the
concept
of
a
pattern_2
connector_data_9
that
listen
to
a
connector_data_1
connector_1
component_7
connector_data_10
a
they
arrive
and
output
it
connector_data_6
to
another
connector_1
a
connector_1
can
be
break
into
multiple
component_17
and
a
copy
of
the
connector_data_9
will
be
spawn
for
each
component_17
technology_1
samza
rely
on
third
party
component_5
to
handle
the
connector_17
of
connector_data_1
between
connector_data_11
technology_1
technology_5
which
have
a
connector_15
on
technology_1
technology_21
the
distribution
of
connector_data_11
among
technology_12
in
a
cluster
technology_1
technology_3
technology_22
connector_3
of
connector_data_1
in
technology_5
be
make
up
of
multiple
component_17
base
on
a
key
requirement_4
a
samza
connector_data_9
connector_21
a
connector_1
of
connector_data_1
and
multiple
connector_data_11
can
be
connector_10
in
parallel
to
connector_22
all
of
the
component_17
in
a
connector_1
simultaneously
samza
connector_data_11
connector_10
in
technology_22
container
technology_22
will
quality_attribute_1
the
container
over
a
multiple
technology_12
in
a
cluster
and
will
evenly
quality_attribute_1
connector_data_11
over
container
the
follow
diagram
show
how
the
part
of
the
samza
word
count
example
component_16
fit
together
connector_data_1
enter
the
component_16
via
a
technology_5
topic
samza
connector_data_11
be
connector_10
in
technology_22
container
and
listen
for
connector_data_1
from
a
technology_5
topic
when
connector_data_1
arrive
on
the
technology_5
topic
the
samza
connector_data_9
connector_23
and
perform
it
component_1
the
samza
connector_data_9
then
connector_24
it
output
to
another
technology_5
topic
which
will
also
component_18
the
topic
connector_data_10
use
technology_21
at
the
end
of
the
word
count
pipeline
we
use
a
console
to
pattern_3
the
technology_5
topic
that
the
word
count
be
connector_25
it’s
output
to
to
define
a
connector_17
topology
in
samza
you
must
explicitly
define
the
input
and
output
of
the
samza
connector_data_11
before
compilation
once
the
component_19
have
be
compile
the
topology
be
fix
a
the
definition
be
embed
into
the
component_19
package
which
be
quality_attribute_1
to
technology_22
an
update
to
the
topology
would
entail
stop
the
exist
connector_data_11
in
technology_22
recompiling
the
component_19
package
quality_attribute_1
the
component_19
package
to
technology_22
to
create
a
word
count
samza
component_19
we
first
need
to
connector_11
a
fee
of
line
into
the
component_16
we
do
this
by
create
a
reader
that
connector_26
in
a
text
publish
it’s
line
to
a
technology_5
topic
readfile
technology_19
package
uk
co
scottlogic
wordcount
readfile
{
private
final
topic
=
sl
line
private
final
bootstrap_servers
=
localhost

localhost

localhost

private
long
index
=

private
component_20
long

createproducer
{
property
prop
=
property
prop
put
producerconfig
bootstrap_servers_config
bootstrap_servers
prop
put
producerconfig
client_id_config
technology_5
prop
put
producerconfig
key_serializer_class_config
longserializer

getname
prop
put
producerconfig
value_serializer_class_config
stringserializer

getname
kafkaproducer
prop
}
runproducer
final
filename
throw
exception
{
final
component_20
long

component_20
=
createproducer
long
time
=
component_16
currenttimemillis
index
=

try
connector_1

connector_1
=

line
path
connector_11
filename
{
component_21

consumernames
=
line
{
try
{
recordmetadata
metadata
=
component_20
connector_27

producerrecord
topic
index++
line
trim
connector_11
}
catch
exception
e
{
e
printstacktrace
}
}
connector_1
foreach
consumernames
}
finally
{
component_20
close
}
}


args
{
if
args
length
==

{
component_16
out

please
specify
a
filename
}
else
{
try
{
readfile
runproducer
args

}
catch
exception
e
{
e
printstacktrace
}
}
}
}
the
next
step
be
to
define
the
first
samza
connector_data_9
this
samza
connector_data_9
will
split
the
incoming
line
into
word
and
output
the
word
onto
another
technology_5
topic
to
do
this
we
create
a
technology_19
that
connector_18
the

technology_1
samza
connector_data_9
streamtask

splitlinetask
technology_19
package
uk
co
scottlogic
wordcount
splitlinetask
connector_18
streamtask
{
private
final
systemstream
output_stream
=
systemstream
technology_5
sl
word
@override
component_1
incomingmessageenvelope
envelope
messagecollector
collector
taskcoordinator
coordinator
{
connector_data_12
=

envelope
getmessage

word
=
connector_data_12
split
\\s+
split
line
on
one
or
more
whitespace
for

word
word
{
try
{
collector
connector_27

outgoingmessageenvelope
output_stream
word
word
}
catch
exception
e
{
e
printstacktrace
}
}
}
}
the
component_1
will
be
connector_10
every
time
a
connector_data_12
be
quality_attribute_5
on
the
technology_5
connector_1
it
be
listen
to
to
define
the
connector_1
that
this
connector_data_9
listen
to
we
create
a
configuration

this
define
what
the
will
be
connector_4
in
technology_22
where
technology_22
can
find
the
package
that
the
executable
be
include
in
it
also
define
the
technology_5
topic
that
this
connector_data_9
will
listen
to
and
how
the
connector_data_10
on
the
incoming
and
outgoing
topic
be
technology_23
sl
splittask
property
#

factory
class=org
technology_1
samza

technology_22
yarnjobfactory

name=sl
splittask
#
technology_22
technology_22
package
path=file
${basedir}
target
${project
artifactid}
${pom
version}
dist
tar
gz
#
connector_data_9
connector_data_9
class=uk
co
scottlogic
wordcount
splitlinetask
connector_data_9
inputs=kafka
sl
line
#
serializers
serializers
registry

class=org
technology_1
samza
serializers
stringserdefactory
serializers
registry
long
class=org
technology_1
samza
serializers
longserdefactory
#
component_5
component_16
technology_5
samza
factory=org
technology_1
samza
component_16
technology_5
kafkasystemfactory
component_16
technology_5
samza
key
serde=string
component_16
technology_5
samza
msg
serde=string
component_16
technology_5
component_21
technology_21
connect=localhost

component_16
technology_5
component_21
auto
offset
reset=largest
component_16
technology_5
component_20
bootstrap
servers=localhost

component_16
technology_5
connector_1
sl
word
samza
key
serde=string
component_16
technology_5
connector_1
sl
word
samza
msg
serde=string
connector_1
sl
word
samza
key
serde=string
connector_1
sl
word
samza
msg
serde=string
we
now
need
a
connector_data_9
to
count
the
word
for
this
we
create
another
that
connector_18
the

technology_1
samza
connector_data_9
streamtask

wordcounttask
technology_19
package
uk
co
scottlogic
wordcount
wordcounttask
connector_18
streamtask
windowabletask
{
private
final
systemstream
output_stream
=
systemstream
technology_5
sl
wordtotals
private
connector_data_3


wordcountswindowed
=
hashmap


@override
component_1
incomingmessageenvelope
envelope
messagecollector
collector
taskcoordinator
coordinator
{
word
=

envelope
getmessage
count
=
wordcountswindwd
connector_11
word
tolowercase
if
count
==

count
=

count++
wordcountswindowed
put
word
tolowercase
count
}
@override
window

technology_1
samza
connector_data_9
messagecollector
collector

technology_1
samza
connector_data_9
taskcoordinator
coordinator
throw
exception
{
connector_27
wordcounts
to
connector_1
try
{
for

key
wordcountswindowed
keyset
{
collector
connector_27

outgoingmessageenvelope
output_stream
key
key
+
+
wordcountswindowed
connector_11
key
}
}
catch
exception
e
{
e
printstacktrace
}
reset
wordcounts
after
windowing
wordcountswindowed
=
hashmap


}
}
this
connector_data_9
also
connector_18
the

technology_1
samza
connector_data_9
windowabletask
to
allow
it
to
handle
a
continuous
connector_1
of
word
and
output
the
total
number
of
word
that
it
have
component_1
during
a
specify
time
window
this
connector_data_9
also
need
a
configuration

sl
wordcount
property
#

factory
class=org
technology_1
samza

technology_22
yarnjobfactory

name=sl
wordcount
#
technology_22
technology_22
package
path=file
${basedir}
target
${project
artifactid}
${pom
version}
dist
tar
gz
#
connector_data_9
connector_data_9
class=uk
co
scottlogic
wordcount
wordcounttask
connector_data_9
inputs=kafka
sl
word
connector_data_9
window
ms=10000
#
serializers
serializers
registry

class=org
technology_1
samza
serializers
stringserdefactory
serializers
registry

class=org
technology_1
samza
serializers
integerserdefactory
#
technology_5
component_16
component_16
technology_5
samza
factory=org
technology_1
samza
component_16
technology_5
kafkasystemfactory
component_16
technology_5
samza
key
serde=string
component_16
technology_5
samza
msg
serde=string
component_16
technology_5
component_21
technology_21
connect=localhost

component_16
technology_5
component_20
bootstrap
servers=localhost

component_16
technology_5
connector_1
sl
word
samza
key
serde=string
component_16
technology_5
connector_1
sl
word
samza
msg
serde=string
connector_1
sl
word
samza
key
serde=string
connector_1
sl
word
samza
msg
serde=string
component_16
technology_5
connector_1
sl
wordtotals
samza
key
serde=string
component_16
technology_5
connector_1
sl
wordtotals
samza
msg
serde=string
connector_1
sl
wordtotals
samza
key
serde=string
connector_1
sl
wordtotals
samza
msg
serde=string
this
configuration
also
specify
the
name
of
the
connector_data_9
in
technology_22
and
where
technology_22
can
find
the
samza
package
it
also
specify
the
input
and
output
connector_1
technology_23
and
the
input
connector_1
to
listen
to
this
configuration
also
specify
the
time
window
that
the
wordcount
connector_data_9
will
use
connector_data_9
window
m
when
these
be
compile
and
packaged
up
into
a
samza
archive

we
can
connector_10
the
samza
connector_data_9
first
we
need
to
make
sure
that
technology_22
technology_21
and
technology_5
be
run
once
the
component_5
that
samza
us
be
run
we
can
extract
the
samza
package
archive
and
then
connector_10
the
connector_data_11
by
use
a
samza
supply
script
a
below
$prj_root
tmp
bin
run

sh
config
factory=org
technology_1
samza
config
factory
propertiesconfigfactory
config
path=file
$prj_root
tmp
config
sl
splittask
property
in
this
snippet
$prj_root
will
be
the
directory
that
the
samza
package
be
extract
into
the
samza
supply
run

sh
connector_23
the

technology_1
samza

jobrunner
and
pass
it
the
configuration
for
our
line
splitter
splittask
samza
then
start
the
connector_data_9
specify
in
the
configuration
in
a
technology_22
container
we
can
then
connector_10
the
word
counter
connector_data_9
$prj_root
tmp
bin
run

sh
config
factory=org
technology_1
samza
config
factory
propertiesconfigfactory
config
path=file
$prj_root
tmp
config
sl
wordcount
property
to
be
able
to
see
the
word
count
be
produce
we
will
start
a
console
window
and
run
the
technology_5
command
line
topic
component_21
$kafka_dir
bin
technology_5
console
component_21
sh
technology_21
localhost

topic
sl
wordtotals
we
can
now
publish
connector_data_1
into
the
component_16
and
see
the
word
count
be
display
in
the
console
window
$prj_root
tmp
bin
run

sh
uk
co
scottlogic
wordcount
readfile
filename
we
should
now
see
wordcounts
be
emit
from
the
samza
connector_data_9
connector_1
at
interval
of

second
a
specify
in
the
sl
wordtotals
property

a
well
a
the
example
above
the
creation
of
a
samza
package
need
a
technology_18
pom
build
and
an
technology_24
to
define
the
content
of
the
samza
package

these
build
need
to
be
correct
a
they
create
the
samza
package
by
extract
some
such
a
the
run

sh
script
from
the
samza
archive
and
create
the
tar
gz
archive
in
the
correct
technology_23
to
conserve
space
these
essential
have
not
be
show
above
technology_1
samza
us
a
compositional
component_4
with
the
topology
of
the
samza
explicitly
define
in
the
codebase
but
not
in
one
place
it
be
spread
out
over
several
with
input
connector_3
be
specify
in
the
configuration
for
each
connector_data_9
and
output
connector_3
be
specify
in
each
task’s

the
connector_1
name
be
text
and
if
any
of
the
specify
connector_3
do
not
match
output
of
one
connector_data_9
to
the
input
of
the
next
then
the
component_16
will
not
component_1
connector_data_1
to
quality_attribute_6
a
samza
component_16
would
require
extensive
test
to
make
sure
that
the
topology
be
correct
this
make
create
a
samza
component_19
error
prone
and
difficult
to
connector_16
at
a
late
date
what
be
connector_1
component_1
component_2
quality_attribute_7
for
why
use
a
connector_1
component_1
component_4
at
all
when
do
it
beat
connector_28
your
own
to
component_1
a
connector_1
connector_1
component_1
component_2
allow
manipulation
on
a
connector_data_1
set
to
be
break
down
into
small
step
each
step
can
be
run
on
multiple
part
of
the
connector_data_1
in
parallel
which
allow
the
component_1
to
quality_attribute_8
a
more
connector_data_1
enter
the
component_16
more
connector_data_11
can
be
spawn
to
connector_22
it
from
the
above
example
we
can
see
that
the
ease
of
cod
the
wordcount
example
in
technology_1
technology_2
and
flink
be
an
order
of
magnitude
easy
than
cod
a
similar
example
in
technology_1
technology_7
and
samza
so
if
implementation
quality_attribute_9
be
a
priority
then
technology_2
or
flink
would
be
the
obvious
choice
if
you
need
complete
control
over
how
the
dag
be
form
then
technology_7
or
samza
would
be
the
choice
technology_1
technology_2
also
offer
several
technology_8
that
could
make
it
the
choice
of
component_4
if
for
example
you
need
to
connector_29
an
technology_4
component_22
technology_2
technology_14
or
requirement_2
mllib
give
all
this
in
the
vast
majority
of
requirement_1
technology_1
technology_2
be
the
correct
choice
due
to
it
extensive
out
of
the
component_6
feature
and
ease
of
cod
in
financial
component_23
there
be
a
huge
drive
in
move
from
pattern_1
component_1
where
connector_data_1
be
connector_13
between
component_5
by
pattern_1
to
connector_1
component_1
a
typical
use
requirement_1
be
therefore
technology_25
between
component_16
technology_1
technology_2
be
a
quality_attribute_7
example
of
a
connector_17
technology_26
that
be
be
use
in
many
technology_25
situation
but
a
well
a
technology_25
component_1
thing
in
real
or
pseudo
real
time
be
a
common
component_19
another
example
be
component_1
a
live
requirement_5
fee
pattern_4
for
requirement_5
to
hit
a
high
or
a
low
and
then
connector_30
off
some
component_1
be
a
quality_attribute_7
example
risk
calculation
be
another
and
be
typically
move
from
daily
pattern_1
component_1
to
real
time
live
component_1
a
requirement_6
want
to
understand
their
exposure
a
and
when
it
happen
each
of
these
technology_11
have
it’s
own
pro
and
con
but
use
any
of
them
free
developer
from
have
to
connector_7
complex
multiprocessing
and
connector_data_1
synchronisation
architecture
in
this
we
look
at
connector_31
a
quality_attribute_2
wordcount
example
in
the
technology_11
in
part

we
will
look
at
how
these
component_5
handle
checkpointing
issue
and
failure
connector_2
more
technology_1
technology_7
taster
a
quick
look
at
technology_1
technology_7
with
a
short
word
count
walk
through
example
use
a
technology_27
component_22
dave
ogle
·
29th
jan

·

min
connector_2
real
time
connector_data_1
analysis
use
technology_2
requirement_7
be
a
pattern_5
topic
these
day
and
one
aspect
of
that
problem
space
be
component_1
connector_3
of
high
technology_28
connector_data_1
in
near
real
time
here
we
re
go
to
look
at
use
requirement_7
style
technique
in
technology_29
on
a
connector_1
of
connector_data_1
from
a
technology_30
james
phillpotts
·
29th
jul

·

min
connector_2
think
of
join
u
if
you
enjoy
this
and
be
interest
in
work
with
smart
developer
on
challenge
project
connector_32
out
our
current
vacancy
andrew
carr
i
lead
the
connector_data_1
engineering
practice
within
scott
component_3
i
have
a
strong
interest
and
expertise
in
low
quality_attribute_3
front
office
requirement_8
component_16
manage
very
large
requirement_3
and
the
technology_6
involve
in
component_1
large
volume
of
connector_data_1
andy
aspell
clark
i
be
a
senior
developer
at
scott
component_3
i
be
interest
in
all
programming
topic
from
how
a
component_24
go
from
power
on
to
display
window
on
the
screen
or
how
a
cpu
handle
branch
prediction
to
how
to
connector_6
a
requirement_9
ui
use
technology_31
or
technology_32
@aspellclark
category
late

resource
requirement_10
tech
ux
design
delivery
test
connector_data_1
engineering
people
video
open_source
podcast
back
to
all

scott
component_3
u
©
copyright
scott
component_3

privacy
