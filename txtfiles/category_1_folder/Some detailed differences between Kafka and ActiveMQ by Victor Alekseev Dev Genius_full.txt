some
detail
difference
between
technology_1
and
technology_2
|
by
victor
alekseev
|
dev
geniusopen
in
apphomenotificationslistsstorieswritepublished
indev
geniusvictor
alekseevfollowmay

2020·22
min
readsavesome
detail
difference
between
technology_1
and
activemqthe
reason
that
inspire
me
to
connector_1
this
be
trivial
—
curiosity
and
a
desire
to
clarify
the
difference
primarily
for
myself
the
idea
of
move
to
quality_attribute_1
architecture
base
on
pattern_1
connector_2
of
loosely
couple
component_1
have
strongly
capture
the
mass
mq
technology_3
technology_4
for
example
be
well

to
developer
since
ancient
time
a
brief
description
of
the
common
requirement_1
provide
by
technology_1
allow
to
consider
it
a
only
a
modern
advance
version
of
the
traditional
pattern_2
component_2
so
it’s
no
surprise
that
many
team
influence
by
hype
and
really
impressive
example
of
kafka’s
apply
in
well

project
without
much
think
make
the
not
quite
inform
decision
to
use
it
a
a
common
project
infrastructure
after
that
they
face
a
lot
of
difficulty
cause
by
the
difference
in
philosophy
between
the
two
component_2
i
would
to
admit
right
away
that
there
be
very
little
in
this
apart
from
my
own
highly
subjective
accent
and
conclusion
base
on
more
than

year
of
experience
with
technology_5
technology_3
mainly
technology_6
technology_2
and
technology_7
mq
primarily
in
bank
relate
but
not
only
project
for
those
who
want
to
deeply
understand
the
subject
i
insistently
recommend
connector_3
the
next
two
book
neha
narkhede
gwen
shapira
todd
palino
“kafka
the
definitive
guide”
very
fundamental
but
unfortunately
already
slightly
outdated
jakub
korab
“understanding
connector_data_1
brokers”
quality_attribute_2
but
require
to
have
some
previous
experience
if
you
don’t
want
to
waste
your
time
i
can
offer
a
brief
extract
from
these
book
and
my
own
humble
experience
storage
of
messagesperhaps
the
most
fundamental
difference
between
these
two
component_3
be
the
next
technology_1
be
not
a
transport
but
connector_4
storage
of
connector_data_2
with
limit
connector_5
capability
abstract
offset
in
the
connector_6
or
pattern_3
which
can
be
roughly
convert
into
offset
connector_data_2
be
not
only
transfer
but
also
permanently
component_4
for
multiple
attempt
of
consumption
in
requirement_2
of
connector_7
of
a
component_5
error
correction
addition
of
component_6
requirement_1
by
many
component_5
by
define
ksql
it
be
possible
to
use
this
quality_attribute_1
storage
for
the
connector_8
of
query
unlike
classical
technology_8
query
they
be
not
connector_9
each
time
“by
demand”
but
work
a
stateful
pattern_4
that
update
their
internal
state
base
on
the
connector_data_2
connector_10
that
pass
through
them
they
can
be
consider
a
some
analog
of
the
materialize
pattern_5
from
the
classical
relation
component_7
at
the
same
time
mq
be
not
initially
design
a
a
long
term
connector_data_2
storage
it
slogan
be
“as
far
a
connector_11
—
delete”
constantly
crowd
and
of

grow
component_8
be
usually
a
symptom
of
some
problemsdurability
and
persistence
significantly
slow
down
the
component_6
and
usually
have
to
be
sacrifice
when
achieve
high
quality_attribute_3
and
low
latencylogical
structureactivemqjms
component_9
connector_12
a
fundamental
semantic
difference
between
a
component_10
peer
to
peer
and
topic
publish
subscribe
destination
technology_2
provide
optional
quality_attribute_4
and
persistence
for
each
of
these
type
all
connector_data_3
be
come
in
one
flow
and
be
round
robin
quality_attribute_1
between
a
potentially
unlimited
number
of
component_11
each
of
which
can
potentially
connector_13
any
connector_data_1
in
the
end
connector_data_3
be
delete
from
component_12
storage
a
soon
a
they
be
connector_14
to
one
all
pattern_6
it
be
possible
to
configure
the
connector_data_1
“obsolescence”
by
time
in
this
requirement_2
they
can
be
not
connector_14
at
all
component_13
fundamentally
can
not
connector_15
already
acknowledge
connector_data_3
again
the
storage

be
a
more
or
le
automatic
component_6
which
do
not
require
advance
administration
and
monitoringkafkait
support
only
topic
with
mandatory
quality_attribute_4
and
persistence
there
be
no
possibility
to
connector_16
a
classic
component_10
with
pattern_7
semantic
you
can
play
with
the
storage
capacity
and
quality_attribute_5
but
connector_data_2
will
always
be
persist
and
quality_attribute_6
for
some
time
connector_data_1
be
reproducibly
quality_attribute_1
over
a
configure
set
of
component_14
evenly
or
under
the
control
of
the
component_15
each
component_11
within
a
single
group
implementation
of
some
requirement_1
can
connector_15
several
component_14
of
the
topic
at
the
same
time
and
connector_13
connector_data_3
from
each
one
in
the
strict
order
they
be
connector_17
by
component_15
at
any
time
each
component_14
can
only
be
component_6
only
by
one
component_11
of
the
group
so
the
maximum
number
of
active
component_13
in
the
group
be
limit
to
the
count
of
component_14
technology_1
connector_data_3
be
delete
not
when
they
already
have
be
connector_14
connector_13
and
acknowledge
by
a
component_11
but
accord
to
the
storage
requirement_3
maximum
configure
storage
time
volume
and
even
if
they
be
not
connector_14
to
anywhere
at
all
any
component_11
at
any
time
can
connector_15
already
previously
connector_18
connector_data_3
again
from
the
begin
of
the
storage
or
use
some
offset
timeout
it
be
principally
not
possible
to
“delete”
the
connector_data_1
so
that
it
would
be
no
long
quality_attribute_6
to
component_11
a
unique
feature
of
technology_1
be
the
“log
compacted”
topic
which
connector_19
only
the
last
requirement_4
for
each
key
technology_9
optional
property
of
the
connector_data_1
physical
storageactivemqit
be
not
mandatory
to
physically
component_4
connector_data_3
on
disk
at
all
—
we
can
work
completely
only
with
memory
up
to
some
connector_data_2
volume
it
allow
if
memory
be
enough
and
component_13
be
connector_20
connector_data_3
fast
to
reach
a
low
delay
and
high
quality_attribute_3
connector_data_1
of
all
the
topic
be
component_4
in
one
requirement_5
locate
on
one
logical
component_16
to
connector_1
connector_data_2
into
this
requirement_5
the
component_17
be
line
up
there
be
only
one
pattern_8
work
with
the
storage
at
any
give
time
the
pattern_8
optionally
provide
a
quality_attribute_7
connector_data_2
storage
strategy
by
force
all
pattern_9
to
be
flush
for
each
connector_data_1
a
group
of
connector_data_3
in
one
transaction
which
slow
down
significantly
the
bandwidth
of
logical
storage
be
limit
by
the
bandwidth
of
one
disk
you
can
raid
or
a
special
storage
plugin
for
several
disk
but
with
some
limitation
especially
concern
transaction
so
this
option
be
not
well
suitable
for
a
requirement_6
where
all
component_3
be
actually
connector_21
via
a
requirement_7
and
therefore
slow
enough
there
be
various
plug
in
strategy
of
permanent
connector_data_2
storage
per
multiple
disk
with
technology_10
and
so
on
kafkakafka
connector_data_3
be
always
permanently
connector_22
there
be
no
other
option
of

we
can
make
the
maximum
time
size
of
storage
minimal
but
we
can’t
make
it
zero
thus
we
also
have
an
additional
feature
—
a
backup
connector_data_2
storage
for
free
in
the
requirement_2
of
right
architecture
and
configuration
all
other
connector_data_2
storage
potentially
can
be
restore
from
the
initial
connector_10
of
connector_data_1
the
connector_data_3
from
each
topic
be
quality_attribute_1
among
several
component_14
each
of
them
be
cluster
for
connector_15
connector_1
storage
load
balance
purpose
accordingly
only
connector_1
the
late
version
of
technology_1
allow
to
use
for
connector_3
the
near
replica
of
the
component_14
to
one
replica
of
one
component_14
of
one
topic
be
line
up
the
rest
be
connector_9
in
parallel
on
one
different
cluster
nodesinstead
of
forcefully
flush
all
pattern_9
for
each
connector_data_1
the
technology_1
pattern_8
rely
on
flexibly
quality_attribute_8
pattern_10
of
connector_data_3
to
other
technology_11
in
the
cluster
this
significantly
increase
bandwidth
sometimes
by
a
thousand
time
because
the
connector_data_3
be
connector_23
with
the
memory
update
rate
and
the
pattern_8
connector_24
more
component_2
pattern_9
rather
than
component_5
memory
heap
be
use
minimally
and
pattern_8
do
not
suffer
from
a
typical
technology_12
component_5
problem
a
an
stw
pause
during
garbage
collection
also
this
architecture
match
up
well
with
requirement_6
deployment
the
traffic
of
connector_data_3
be
more
or
le
evenly
quality_attribute_1
across
the
entire
cluster
by
split
each
topic
into
multiple
section
quality_attribute_1
across
the
clusterthere
be
the
only
strategy
to
component_4
connector_data_2
—
in
a
segment
requirement_5
connector_data_2
be
always
connector_1
in
large
bulk
to
the
end
of
the
contain
an
active
segment
and
connector_25
by
component_18
through
the
component_2
pattern_9
the
absence
of
random
connector_26
to
fragment
provide
significant
requirement_8
gain
in
a
i
o
it
require
thorough
configuration
and
continuous
pattern_11
to
maintain
an
equal
distribution
of
connector_data_2
and
loading
across
cluster
technology_11
general
architecturethe
fundamental
difference
be
“smart
pattern_8
vs
smart
client”the
mq
component_19
be
focus
on
a
highly
functional
brokerit
be
responsible
for
the
centralized
component_20
of
connector_20
and
quality_attribute_1
connector_data_1
but
component_21
should
only
worry
about
connector_27
connector_13
and
acknowledge
individual
connector_data_1
accordingly
it
slow
down
significantly
a
the
number
of
connector_21
component_21
growsit
usually
support
many
technology_13
and
more
complex
connector_28
component_5
requirement_9
template
for
example
temporary
component_10
pattern_12
connector_data_4
connector_29
automate
re
component_6
individual
break
connector_data_3
by
another
component_11
+
dead
letter
component_10
delay
connector_data_1
and
so
on
apart
from
the
connector_data_5
connector_data_3
can
contain
arbitrary

some
of
them
can
carry
the
technology_9
for
technology_5
or
for
a
particular
implementation
component_2
metadata
tell
the
pattern_8
how
to
component_6
the
connector_data_1
for
example
the
obsolescence
time
where
to
connector_27
the
connector_29
be
it
necessary
to
component_4
it
quality_attribute_7
and
so
on
it
be
possible
to
subscribe
with
a
condition
on
the
content
of

due
to
this
and
the
hierarchical
name
component_2
of
topic
component_10
it
be
possible
to
connector_16
many
requirement_10
requirement_9
pattern_13
use
for
complex
connector_data_2
connector_30
between
component_5
component_1
in
pattern_14
architecturekafkait
be
orient
on
“smart”
component_21
who
be
sometimes
together
in
charge
of
many
of
a
traditional
pattern_8
definition
of
a
component_14
for
the
connector_31
connector_data_1
distribution
and
re
distribution
of
component_14
between
component_11
component_6
of
exception
trace
of
the
already
connector_11
connector_data_1
pattern_4
of
already
connector_data_3
by
condition
and
so
on
instead
of
“smart”
one
there
be
a
quality_attribute_9
fast
and
quality_attribute_10
pattern_8
that
slow
down
only
slightly
a
the
number
of
connector_21
component_21
and
connector_data_2
component_6
volume
grow
technology_1
can
be
more
often
find
in
micro
component_1
architecture
and
a
a
component_22
for
quality_attribute_1
component_6
connector_data_2
flow
there
be
an
extremely
limit
set
of
technology_9
metadata
describe
connector_data_1
not
any
technology_9

be
control
the
pattern_8
reader
behavior
at
all
for
example
it
be
not
able
to
connector_11
only
connector_data_3
with
some
specific
requirement_4
of

it
be
possible
to
component_6
or
throw
away
connector_data_3
only
after
connector_32
them
technology_13
of
interactionactivemqbesides
requirement_11
openwire
it
also
support
technology_14
stomp
technology_15
technology_16
some
technology_13
can
be
both
technology_17
and
technology_18
base
it
allow
connector_33
directly
from
for
example
the
technology_19
script
base
ui
the
connector_data_1
size
be
potentially
unlimited
or
split
gather
be
perform
automatically
pattern_8
connector_12
a
“push
“
approach
—
connector_34
out
connector_data_3
from
storage
to
component_23
also
connector_35
approach
can
be
configure
too
kafkaas
a
matter
of
principle
it
do
not
support
anything
but
own
a
binary
technology_13
for
the
sake
of
keep
full
control
over
implementation
a
it
be
an
“always
on”
component_2
it
be
critical
to
maintain
technology_13
quality_attribute_11
a
the
quality_attribute_12
and
the
cluster
technology_11
be
gradually
update
it
also
allow
connector_36
connector_18
connector_data_3
on
disk
“as
is”
without
transformation
and
pass
them
by
connector_data_4
of
the
component_11
directly
from
the
disk
pattern_9
component_18
to
requirement_7
connector_data_6
zero
copy
with
connector_37
a
lot
of
resource
so
due
to
the
long
term
storage
the

version
of
the
can
connector_13
a
connector_data_1
in
the
old
technology_20
via
replica
the
default
connector_data_1
size
be
1mb
and
it
be
highly
discourage
to
increase
it
the
“claim
check”
requirement_9
template
can
help
pattern_8
connector_12
a
“pull”
approach
—
component_21
be
connector_35
connector_data_3
from
the
pattern_8
backpressure
be
not
need
at
all
connector_27
messagesactivemqit
be
usually
there
be
option
but
connector_16
only
a
configuration
and
not
present
in
the
technology_21
perform
by
pattern_12
pattern_15
connector_data_7
from
the
supplier
till
connector_37
the
connector_data_2
into
the
memory
or
the
broker’s
disk
storage
when
physically
component_4
connector_38
be
immediately
quality_attribute_6
to
customerskafkaapart
from
the
previous
requirement_2
it
be
a
more
complex
and
quality_attribute_13
component_6
successfully
connector_39
a
connector_data_1
to
a
pattern_8
doesn’t
mean
that
it
already
be
physically
component_4
somewhere
the
supplier
can
be
release
immediately
after
the
requirement_7
connector_data_7
normally
pattern_8
connector_40
connector_18
connector_data_1
only
in
the
component_2
pattern_9
without
flush
it
not
only
the
lead
pattern_8
of
the
component_14
be
involve
in
the
connector_37

but
also
the
pattern_8
of
it
follow
replica
component_11
will
not
see
the
connector_38
until
they
be
quality_attribute_1
among
the
cluster’s
technology_11
it
take
some
time
which
quality_attribute_14
on
the
cluster
node’s
loading
the
connector_39
component_6
be
by
default
pattern_1
which
allow
automatically
fix
retryable
problem
—
the
inaccessibility
of
the
pattern_8
temporary
absence
of
a
leader
for
the
component_14
etc
but
at
the
same
time
we
sacrifice
a
fix
order
of
connector_39
connector_data_1
it
be
also
possible
to
make
this
component_6
pattern_12
apply
the
configuration
but
it
connector_data_8
in
low
quality_attribute_3
so
a
we
can
see
there
be
a
wide
range
of
requirement_12
off
“throughput”
vs
“reliability
of
connector_data_2
storage”
—
from
“sent
and
forget”
to
wait
until
all
pattern_8
of
the
cluster
confirm
connector_20
the
connector_data_1
connector_data_1
consumptionactivemqthe
centralized
distribution
of
connector_data_3
to
the
connector_41
component_13
be
carry
out
by
the
pattern_8
that
it
be
heavily
loadedeach
connector_data_1
in
a
normal
situation
can
only
be
connector_18
by
the
component_11
once
after
successful
acknowledgment
of
a
specify
connector_data_1
by
the
component_11
it
be
fundamentally
impossible
to
connector_13
the
connector_data_1
a
second
time
but
if
for
example
acknowledgment
be
lose
the
connector_data_1
will
be
connector_14
again
so
real
semantic
be
“as
a
minimum
once”
component_11
can
connector_41
disconnect
to
a
pattern_8
at
any
time
without
any
pause
in
the
connector_data_1
component_6
in
this
requirement_2
we
be
deal
with
a
very
elastic
set
of
component_13
which
be
quality_attribute_2
for
round
robin
load
balance
the
pattern_8
control
the
flow
of
connector_data_3
and
theoretically
can
overload
the
consumerusing
technology_5
selector
component_13
can
pattern_4
the
connector_20
connector_data_3
on
the
broker’s
side
some
useful
use
requirement_2
of
this
quite
important
in
the
mq
architecture
feature
be
the
next
the
connector_8
of
pattern_12
connector_data_4
connector_29
between
two
component_24
through
a
pair
of
component_10
ahead
fetch
from
a
component_10
of
age
connector_data_3
for
which
qos
be
already
close
to
critical
—
some
kind
of
double
end
component_10
which
allow
you
to
redirect
connector_data_3
from
overload
or
dead
pattern_16
to
healthy
or
start
one
to
fix
the
traffic
jam
the
big
disadvantage
of
this
situation
be
that
if
several
component_25
be
connector_21
to
the
same
component_10
which
be
normal
for
example
for
bus
pattern_13
each
of
them
will
connector_13
and
perhaps
at
worst
even
deserialize
all
connector_data_1
correspondingly
we
have
an
excess
of
requirement_7
bandwidth
and
cpu
kafkaconsumers
reach
agreement
among
themselves
who
will
connector_13
connector_data_3
from
which
component_14
also
they
acknowledge
connector_data_3
by
periodically
component_26
their
current
flow
position
and
connector_42
this
connector_data_9
with
“colleagues”
to
be
able
to
apply
it
in
the
requirement_2
of
repartitioning
acknowledge
connector_data_9
can
be
connector_22
in
technology_1
in
technology_22
for
old
component_23
in
a
special
component_1
topic
for
component_23
or
their
storage
for
example
together
with
the
connector_data_10
of
connector_data_1
component_6
in
the
component_7
which
provide
some
kind
of
transactionality
they
can
connector_13
the
last
connector_data_1
a
well
a
connector_data_3
already
connector_15
again
by
pass
a
previously
connector_22
offset
or
pattern_3
component_11
do
not
acknowledge
individual
connector_18
connector_data_3
a
in
the
requirement_2
of
technology_5
but
connector_22
their
offset
of
the
last
connector_18
connector_data_1
thus
the
fix
of
connector_data_1
x
connector_1
offset
x
in
technology_1
or
another
storage
automatically
mean
that
the
connector_data_3
x

x

and
so
on
be
also
safely
connector_13
if
a
package
of
connector_18
connector_data_3
be
pass
to
the
worker’s
pool
for
acceleration
purpose
it
can
be
a
problem
connector_7
disconnection
of
component_13
initiate
the
of
repartitioning
—
redistribution
of
all
component_14
belong
to
all
of
the
affect
topic
between
all
their
involve
component_11
which
connector_data_8
in
a
pause
in
the
consumption
of
connector_data_2
and
cause
duplication
of
connector_data_1
thus
their
number
should
be
a
quality_attribute_15
a
possible
the
intensity
of
connector_data_3
connector_20
be
fully
control
by
the
component_11
moreover
he
must
do
it
fast
enough
not
to
violate
the
configure
timeout
after
which
his
component_14
will
be
pass
to
someone
else
component_11
connector_13
all
connector_data_3
from
assign
component_14
there
be
no
possibility
to
pattern_4
them
on
the
broker’s
side
it
make
for
example
the
already
mention
“synchronous
connector_data_4
response”
pattern_13
hard
to
connector_16
distribution
of
messageskafkamandatory
distribution
of
connector_data_3
to
potential
component_13
be
perform
by
the
component_15
at
the
moment
of
connector_39
the
connector_data_1
in
the
component_14
of
the
topic
within
each
component_14
connector_data_3
be
component_4
in
an
orderly
manner
and
come
to
the
component_11
in
the
same
order
this
greatly
simplify
the
implementation
of
many
requirement_13
but
be
well
suit
for
a
quality_attribute_15
number
of
component_11
a
connector_43
their
number
cause
a
pause
in
the
redistribution
of
component_14
between
component_11
during
which
no
one
connector_44
anything
the
relative
order
of
connector_data_3
in
different
component_14
of
the
same
different
topic
be
not
define
even
if
they
be
consecutively
connector_17
to
the
pattern_8
also
the
fix
number
of
component_14
and
consequently
component_11
and
the
complexity
of
their
extension
limit
the
degree
of
parallelism
in
connector_data_1
component_6
so
for
example
technology_1
be
not
very
quality_attribute_2
for
elastic
load
balance
between
pattern_16
activemqit
quality_attribute_1
connector_data_3
a
a
round
robin
between
all
register
component_13
at
the
moment
of
consumption
partitioning
be
a
quite
rarely
use
feature
if
necessary
it
can
be
connector_16
by

special
metadata
to
connector_data_3
but
it
be
not
a
part
of
the
technology_5
technology_9
some
frequent
functional
requirementstransaction
supportkafkaas
a
matter
of
principle
it
do
not
participate
in
connector_45
transaction
with
other
systemssince
version


it
be
possible
to
transactionally
connector_27
several
connector_data_3
in
several
topic
with
simultaneous
acknowledgment
of
already
connector_18
connector_data_1
a
a
matter
of
principle
there
be
no
automatic
rollback
of
one
connector_data_1
if
there
be
a
problem
with
it
component_6
quality_attribute_14
on
when
we
acknowledge
the
connector_18
connector_data_3
“before”
or
“after”
component_6
the
crash
of
the
component_11
lead
to
the
loss
or
repeat
connector_20
of
a
whole
set
of
connector_data_1
there’s
no
build
in
“dead
letter
queue”
analogactivemqit
allow
not
only
to
organize
a
transaction
between
several
pattern_8
connector_data_11
but
also
comprehensively
participate
a
a
resource
in
local
xa
transaction
an
exception
when
component_6
one
connector_data_1
automatically
connector_data_8
in
only
it
repeat
receivingyou
can
limit
the
number
of
such
attempt
and
organize
a
“dead
letter
queue”
for
undelivered
messagesmessage
orderingactivemqthe
orderliness
of
connector_14
connector_data_3
be
not
guarantee
due
to
the
quality_attribute_1
nature
of
the
pattern_12
component_6
which
deal
with
a
parallel
set
of
component_27
pass
connector_data_3
to
component_11
this
mean
that
the
relative
order
of
component_6
each
several
neighbor
connector_data_3
be
not
define
but
this
uncertainty
window
itself
be
rather
small
but
in
most
requirement_2
the
strict
orderliness
of
connector_data_3
be
not
important
at
all
some
e
g
a
connector_6
of
component_2
metric
or
requirement_5
be
not
sensitive
to
the
order
in
which
individual
connector_data_3
be
connector_18
because
each
be
time
stamp
and
the
component_6
of
each
one
be
independent
if
component_28
component_6
be
base
on
the
state
component_29
and
the
connector_8
of
each
can
be
inited
only
by
the
connector_17
by
the
previous
state
then
there
be
only
one
connector_data_1
connector_5
to
the
component_28
simultaneously
in
the
flow
the
relative
order
of
the
connector_data_3
connector_5
to
different
component_30
be
not
so
important
in
this
requirement_2
in
some
requirement_2
the
time
“distance”
between
two
connector_data_3
that
should
not
be
interchange
be
so
relatively
long
that
the
probability
of
collision
be
negligible
from
my
experience
with
component_3
that
generate
hundred
of
million
of
connector_data_3
a
day
i
can
say
that
the
order
of
the
connector_data_3
be
sensitive
almost
only
when
we
transfer
a
large
portion
of
connector_data_2
divide
into
separate
piece
but
it
be
not
a
quality_attribute_2
practice
and
the
eip
pattern_13
“claim
check”
be
more
preferable
kafkait
guarantee
that
all
connector_data_3
within
the
same
component_14
will
be
connector_14
in
the
order
they
be

the
requirement_14
for
this
be
the
ability
to
connector_11
connector_data_3
from
a
single
component_14
only
by
a
single
component_27
if
we
to
quality_attribute_16
up
connector_data_2
component_6
pass
connector_18
connector_data_3
for
handle
to
the
component_27
pool
the
connector_8
sequence
be
lose
again
we
also
have
problem
with
quality_attribute_17
confirmation
of
connector_18
connector_data_1
no
guarantee
be
give
in
respect
of
connector_data_3
from
different
component_14
due
to
the
quality_attribute_1
nature
of
storage
the
connector_data_2
quality_attribute_3
vary
quite
a
lot
between
different
component_14
therefore
if
the
requirement_13
key
for
partitioning
be
not
selected
or
incorrectly
selected
the
actual
order
in
which
the
connector_data_3
be
component_6
by
all
component_13
differ
very
much
from
the
order
in
which
they
be
initially
generate
it
mean
that
individual
connector_data_2
component_13
within
the
same
component_5
can
live
in
very
different
logical
time
and
this
difference
between
time
quality_attribute_14
on
the
unbalance
distribution
of
connector_data_2
across
the
cluster
which
usually
increase
a
it
be
operate
and
require
manual
intervention
for
elimination
thus
a
thoughtless
desire
to
reduce
the
weak
randomness
of
close
connector_data_3
when
use
technology_2
can
connector_data_10
in
a
significantly
disorder
receipt
of
whole
connector_data_2
fragment
exactly
once
deliverykafkaimplementation
of
exactly
once
delivery
be
only
possible
base
on
the
technology_1
connector_10
technology_23
it
be
not
a
technology_9
pattern_17
feature
of
the
component_9
connector_data_7
but
the
integral
effect
of
the
architecture
and
the
collaborative
component_6
by
component_22
and
individual
connector_data_12
thus
the
component_5
appear
to
be
quite
closely
connector_46
to
the
architecture
and
requirement_1
of
the
technology_23
strictly
speak
we
have
not
an
exactly
once
delivery
of
the
connector_data_1
but
the
ability
to
perform
the
“read
component_6
write”
only
once
for
each
connector_data_1
provide
that
the
connector_data_10
of
component_6
be
also
component_4
through
technology_1
connector_47
transaction
be
not
support
this
be
the
component_31
of
another
problem
quality_attribute_7
storage
of
the
component_6
state
be
perform
by
replicate
all
it
connector_38
to
the
dedicate
topic
if
the
storage
be
large
it
recovery
in
requirement_2
of
a
pattern_16
fall
can
take
quite
a
long
time
more
advance
but
complex
for
implementation
manual
approach
be
bloom
pattern_4
+
local
storage
or
rockdb
a
a
combination
both
+
partitioning
to
reduce
the
size
of
local
storage
+
periodical
backup
of
storage

in
the
requirement_2
of
the
restart
we
can
use
the
destination
topic
a
a
component_31
of
connector_data_9
about
already
component_6
connector_data_3
and
update
restore
from
backup
outdated
storage
before
continue
the
work
activemqthere
be
not
ready
to
use
component_9
or
pattern_13
of
exactly
once
connector_data_3
delivery
it
can
be
easily
connector_16
on
the
base
of
a
unique
connector_data_1
key
generate
by
the
supplier
and
one
of
the
follow

connector_16
a
“process”
step
a
an
idempotent

for
example
by
connector_37
the
key
together
with
the
connector_data_10
of
connector_data_1
component_6
and
at
the
same
time
connector_48
for
connector_data_2
uniqueness
easy
but
not
always
possible
it
also
require
connector_37
the
connector_data_10
of
each
connector_data_1
individually
which
put
more
load
on
the
storage
connector_22
the
key
with
ttl
in
a
separate
fast
usually
technology_24
technology_25
for
example
storage
and
connector_48
before
component_6
each
connector_data_1
the
drawback
be
obvious
transaction
be
not
quality_attribute_6
additional
remote
connector_data_7
continuous
clean
up
ttl
base
key
require
additional
resource
cluster
storage
be
one
more
element
of
infrastructure
for
installation
administration
pattern_18
and
so
on
the
third
possible
opportunity
be
to
move
the
component_10
implementation
inside
the
component_7
which
will
be
further
discuss
partitioningas
be
already
mention
technology_2
do
not
support
partitioning
but
for
technology_1
it
be
the
only
and
default
modus
operandi
for
some
requirement_13
scenario
especially
relate
to
the
component_6
of
infinite
connector_data_2
sequence
it
be
an
actually
necessary
feature
in
the
requirement_2
of
technology_2
it
can
be
achieve
by
generation
component_14
key
in
the
quality_attribute_9
way
it
be
“hash
requirement_13
key
mod
count
of
component_11
”
on
the
provider’s
side
and
two
common
approach
by
technology_5
selector
component_13
subscribe
for
connector_20
the
only
connector_data_1
from
one’s
component_14
it
require
each
component_11
to
have
a
unique
index
by
technology_5
jmxgroupid
pattern_8
organize
some
analog
of
technology_26
stiky
component_32
with
each
component_11
base
on
the
requirement_4
of
this

which
be
initialize
with
component_14
key
or
requirement_13
keyat
the
same
time
many
scenario
do
not
require
full
partitioning
at
all
due
to
the
independence
of
individual
connector_data_3
among
themselves
it
be
enough
that
at
every
moment
any
requirement_13
component_28
be
update
in
an
exclusive
mode
it
can
be
connector_16
base
on
the
“select
for
update”
query
which
be
apply
to
the
component_28
before
component_6
it
be
a
quite
quality_attribute_9
direct
and
quality_attribute_18
approach
but
it
also
have
some
disadvantage
you
have
to
take
into
account
the
risk
of
deadlock
for
example
for
transfer
connector_data_2
between
bank
account
it
be
always
necessary
to
acquire
lock
of
them
in
the
same
order
for
example
alphabetical
by
hold
the
lockout
it
be
extremely
undesirable
to
make
remote
connector_data_7
a
this
increase
the
average
transaction
time
which
should
normally
be
minimal
this
situation
can
be
improve
by
introduce
intermediate
state
for
requirement_13
component_30
ready_for_update
=
locked_for_update
but
this
significantly
complicate
the
implementation
of
the
solution
for
example
it
require
the
introduction
of
timeouts
and
a
rescue
manager
that
pick
up
the
component_30
that
be
long
in
an
intermediate
state
due
to
the
fall
of
the
pattern_16
thus
a
fairly
quality_attribute_9
at
first
glance
the
connector_data_12
of
an
exclusive
update
of
requirement_13
component_30
can
easily
become
much
more
complicate
a
the
requirement_13
component_20
develop
consistently
consistent
backup
of
the
systemkafkathe
connector_data_12
of
the
consistent
backup
of
a
quality_attribute_1
component_2
with
multiple
connector_data_2
storage
be
complex
in
itself
in
the
requirement_2
of
technology_1
the
situation
become
even
more
complicate
a
we
connector_49
additional
quality_attribute_1
storage
where
individual
pattern_8
cannot
be
back
up
consistently
luckily
the
technology_1
cluster
itself
be
quite
quality_attribute_15
and
be
itself
a
backup
it
be
a
normal
practice
also
to
organize
a
special
disaster
recovery
cluster
and
replicate
all
connector_38
to
it
with
mirror
maker
switch
to
a
backup
cluster
be
not
so
quality_attribute_9
nowadays
it
be
fundamentally
impossible
to
switch
to
a
backup
cluster
without
lose
connector_data_2
and
or
duplicate

a
well
a
artifact
of
connector_data_2
inconsistency
a
quality_attribute_2
alternative
be
“stretch
cluster”
when
one
cluster
be
instal
simultaneously
in
several
connector_data_2
center
and
switch
component_5
do
not
suffer
from
inconsistency
but
this
choice
require
a
quality_attribute_2
pattern_19
between
two
region
unfortunately
this
do
not
solve
the
problem
of
the
component_2
be
able
to
roll
back
to
a
previous
state
once
connector_17
into
technology_1
connector_data_2
remain
forever
cannot
be
update
or
delete
activemqin
the
requirement_2
of
technology_4
the
situation
be
more
quality_attribute_9
—
the
broker’s
storage
can
be
consistently
component_4
a
backup
without
connector_data_2
loss
so
the
only
we
have
to
do
be
to
backup
it
after
other
storage
and
connector_16
one
time
handle
“orphan”
connector_data_1
which
be
bear
by
connector_data_2
not
exist
yet
in
the
other
connector_data_2
storage
yes
it
be
a
complex
requirement_13
connector_data_12
which
can’t
be
perform
automatically
but
it
be
more
quality_attribute_9
possible
alternativethe
only
quality_attribute_9
general
way
to
be
able
to
perform
a
consistent
backup
of
all
component_5
connector_data_2
be
have
only
one
storage
base
on
the
classic
relational
component_7
in
this
requirement_2
the
component_10
can
be
connector_16
base
on
the
dedicate
component_33
with
connector_data_3
and
connector_20
connector_16
a
“select
for
update
skip
locked”
query
it
be
quite
quality_attribute_9
quality_attribute_13
and
quality_attribute_7
enough
for
quality_attribute_3
of
about

per
second
provide
that
the
duration
of
each
do
not
exceed

millisecond
long
transaction
kill
component_7
requirement_8
if
the
amount
of
connector_data_3
in
the
component_10
at
the
same
time
be
not
large
the
component_7
component_12
actually
keep
them
all
in
memory
work
almost
a
real
pattern_8
it
be
therefore
recommend
that
the
minimum
amount
of
connector_data_2
require
be
connector_50
through
the
component_10
the
major
advantage
apart
from
subj
we
can
connector_16
very
quality_attribute_13
connector_data_1
prioritization
requirement_3
base
not
only
on
pattern_3
but
also
on
some
requirement_13
relate
propertiesdata
component_6
be
multithreaded
on
the
one
hand
and
strictly
pattern_17
on
the
other
handautomatic
retry
in
requirement_2
of
any
exceptionwe
also
connector_49
the
strong
persistency
guaranty
for
connector_data_12
delivery
and
at
least
once
delivery
semanticthe
disadvantage
of
this
approach
be
obvious
every
connector_data_1
must
be
component_6
very
fast
it
mean
the
inability
to
make
remote
connector_data_11
inside
a
transaction
or
complex
component_6
component_20
with
intermediate
state
and
lock’s
timeoutsconsumers
who
constantly
re
connector_data_4
connector_data_2
even
if
the
connector_data_1
component_10
be
empty
it’s
not
very
quality_attribute_10
in
term
of
quality_attribute_3
and
the
number
of
participant
the
situation
can
be
improve
with
the
tune
and
partitioning
of
the
component_33
by
vendor
specific
option
for
example
it
be
possible
to
split
the
single
component_33
into
several
one
locate
into
different
physical
component_34
and
logically
join
by
something
“dblink”
for
technology_27
or
“foreign
connector_data_2
wrapper”
for
postgresqlthere
be
a
ready
implementation
of
this
approach
—
“db
queue”
which
be
the
kernel
of
the
payment
component_2
from
yandex
quality_attribute_19
and
fault
tolerancekafkait
be
originally
design
for
a
cluster
with
redundant
connector_data_2
pattern_10
and
distribution
of
user’s
connector_data_13
among
technology_11
this
allow
almost
unlimited
increasing
capacity
and
quality_attribute_3
the
degree
of
parallelism
of
connector_26
to
the
topic’s
connector_data_2
be
limit
by
the
number
of
component_14
maximum
one
reader
of
the
group
per
component_14
and
should
preferably
be
determine
in
advance
during
design
increasing
the
number
of
component_14
be
complex
cause
a
pause
in
the
connector_3
of
the
whole
topic
and
affect
only
connector_data_2
connector_data_2
can
only
be
fairly
redistribute
among
the
expand
count
of
component_14
by
create
the
topic
and
copy
all
the
exist
connector_data_2
into
it
quality_attribute_19
and
fault
tolerance
can
be
reach
horizontally
base
on
ordinary
hardware
which
be
rebuild
remove

to
the
cluster
a
need
without
stop
always
on
component_2
problem
of
one
topic
be
more
or
le
isolate
and
lead
to
problem
only
of
some
pattern_8
due
to
this
the
component_2
be
more
quality_attribute_20
but
the
difference
between
the
presence
and
absence
of
problem
for
the
technology_1
cluster
be
more
blur
if
individual
pattern_8
of
the
cluster
fail
periodically
the
unbalance
distribution
of
load
on
the
cluster
increase
and
can
only
be
fix
by
a
manual

activemqit
allow
only
master
slave
configuration
base
on
one
connector_45
storage
thus
quality_attribute_19
be
limit
and
almost
only
vertical
a
the
traffic
grow
it
need
to
quality_attribute_21
separate
component_34
with
separate
connector_19
and
manually
quality_attribute_1
application’s
destination
between
them
the
degree
of
parallelism
be
limit
by
the
broker’s
resource
theoretically
it
support
an
infinite
number
of
reader
per
each
component_10
topical
without
the
need
to
redistribute
connector_data_2
in
order
to
increase
the
degree
of
parallelism
quality_attribute_22
“on
the
fly”
be
difficult
and
fundamentally
limit
it
be
require
a
pause
while
quality_attribute_23
increasing
capacity
can
be
achieve
only
vertically
and
require
more
and
more
powerful
hardware
problem
of
one
component_10
topic
be
usually
cause
by
a
lack
of
disk
space
and
immediately
become
problem
of
the
pattern_8
a
a
whole
it
make
the
infrastructure
more
fragile
but
quality_attribute_9
and
encourage
administrator
to
be
more
attentive
operate
costskafkalow
level
component_9
be
unique
and
complex
enough
the
behavior
of
the
component_11
component_15
top
pattern_8
can
be
configure
by
about

configuration
parameter
for
each
type
many
of
these
parameter
fundamentally
affect
it
behavior
not
only
nonfunctional
characteristic
such
a
quality_attribute_3
quality_attribute_24
balance
for
example
the
implementation
of
quality_attribute_7
solution
require
a
quality_attribute_2
understand
of
component_2
architecture
therefore
it
be
almost
necessary
to
base
on
connector_10
connector
ksql
technology_23
which
increase
the
already
not
low
cost
of
entry
apply
technology_1
require
much
more
experience
from
the
developer
and
architect
it
be
difficult
enough
to
install
and
administrate
for
production
usage
usually
an
advance
devops
culture
be
require
for
successful
quality_attribute_23
lack
of
ready
for
use
out
of
the
component_35
requirement_15
and
pattern_11
technology_28
which
be
necessary
to
operate
the
component_2
and
meet
qos
requirement
the
critical
obstacle
for
the
usual
requirement_10
component_25
—
inability
to
participate
in
quality_attribute_1
transaction
it
require
developer
to
move
to
base
architecture
and
to
have
painful
discussion
with
requirement_13
analyst
activemqit
be
quality_attribute_9
enough
to
install
it
can
be
connector_9
even
in
the
embed
mode
which
be
quite
quality_attribute_2
for
component_36
test
most
of
the
time
it
do
not
require
any
complex
requirement_15
and
continuous
pattern_18
technology_29
component_9
be
classic
compact
quality_attribute_9
enough
well

and
quality_attribute_25
there
be
no
problem
with
quality_attribute_1
transaction
and
therefore
requirement_10
developer
can
safely
follow
a
comfortable
for
them
acid
paradigm
summarygenerally
speak
technology_1
can
be
coarsely
describe
a
a
“nosql
style”
solution
in
the
ancient
world
of
mq
it
be
a
modern
wonderful
lightweight
quite
quality_attribute_13
technology_28
but
it
also
provide
a
great
opportunity
to
connector_49
a
reach
experience
by
shoot
yourself
in
the
leg
however
with
the
right
architecture
and
deep
understand
of
how
it
work
it
be
possible
to
achieve
the
impressive
in
term
of
qos
connector_data_8
with
the
optimal
loading
of
your
resource
unavoidable
pay
for
it
be
the
complexity
of
your
component_5
but
connector
and
technology_1
connector_10
part
of
the
technology_23
can
simplify
significantly
the
implementation
of

due
to
the
next
pattern_20
of
abstraction
which
can
make
the
component_2
behavior
even
more
mysterious
than
before
kafkait
be
quite
quality_attribute_2
when
we
need
to
organize
quality_attribute_7
storage
of
very
large
amount
of
connector_data_2
with
very
high
10–100
time
difference
quality_attribute_3
and
relatively
quality_attribute_9
delivery
component_20
to
a
fairly
constant
number
of
requirement_16
the
requirement_14
of
entry
development
and
quality_attribute_23
of
the
ready
component_2
be
relatively
high
well
suit
for
high
volume
component_1
project
especially
in
the
internet
and
iot
domain
with
requirement_6
base
deploymentactivemqit
be
quite
quality_attribute_2
a
a
smart
transport
for
implementation
of
complex
connector_data_1
connector_30
flowsit
allow
quality_attribute_26
balance
connector_data_13
among
a
dynamic
number
of
concurrent
consumersthe
potential
quality_attribute_3
be
not
so
high
and
degrade
with
the
increasing
amount
of
component_4
connector_data_2
and
the
number
of
connector_21
component_23
well
can
be
quality_attribute_27
with
traditional
requirement_10
landscape
especially
in
financial
and
advance
document
workflow
domain
with
on
premise
deploymentin
general
if
you’re
not
a
or
developer
and
you
don’t
critically
need
for
requirement_17
storage
endless
connector_data_2
flow
component_6
or
such
advance
feature
real
partitioning
strict
connector_data_1
order
or
exactly
once
delivery
then
technology_1
be
likely
to
be
too
much
for
you
thanks
toалексей
власов

1more
from
dev
geniusfollowcoding

news
ux
ui
and
much
more
relate
to
developmentread
more
from
dev
geniusrecommended
from
mediumsammy
jo
wymerinrewrite
tech
by
diconiumgetting
friendly
with
the
terminal
a
super
friendly
beginner’s
guidehasintha
indrajeesaml
bearer
grant
type
with
technology_30
identity
component_12


0jonathan
airdupstate
behind
the
scenesthe
pragmatic
programmersinthe
pragmatic
programmersusing
multiple
fixturesshr
tip
and
trick
from
a
beginner
carlos
h
hello
to
the
worldjama
softwarefour
fundamental
of
requirement
managementdavid
h
deansintechnology
|

|
telecomlow

technology_28
accelerate
digital
innovationabouthelptermsprivacyget
the
appget
startedvictor
alekseev61
followerssoftware
architect
and
lead
developer
with

year
experience
in
technology_19
ee
technology_31
and
technology_27
more
at
technology_26
www
linkedin
technology_32
in
victoralekseev
followmore
from
mediumkyle
carterindev
geniusdiving
into
technology_1
partitioning
by
build
a
custom
component_14
assignorsumit
kumarintowards
devconsume
avro
connector_data_3
from
technology_1
without
schemaall
about
codethe

major
component_37
in
technology_6
kafkajeyakeerthananexplore
prototype
design
patternhelpstatuswritersblogcareersprivacytermsaboutknowable
