technology_1
–
brave
geek
skip
to
content
brave
geek
introspection
of
a
engineer
home
about
me
archive
real
kinetic
technology_2
tag
technology_1

on

2016december

2016benchmarking
connector_data_1
component_1
quality_attribute_1
about
a
year
and
a
half
ago
i
publish
dissect
connector_data_1
component_1
which
break
down
a
few
different
pattern_1
component_2
and
do
some
requirement_1
benchmarking
it
be
a
naive
attempt
and
have
a
lot
of
problem
but
it
be
also
my
first
time
do
any
kind
of
component_3
benchmarking
it
turn
out
benchmarking
component_2
correctly
be
actually
pretty
difficult
and
many
folk
connector_1
it
wrong
i
don’t
claim
to
have
connector_1
it
right
but
over
the
past
year
and
a
half
i’ve

a
lot
try
to
build
some
quality_attribute_2
technology_3
and
improve
my
methodology
technology_3
and
methodology
the
dissect
connector_data_1
component_4
benchmark
use
a
technology_4
i
connector_2
which
publish
a
specify
number
of
connector_data_2
effectively
a
fast
a
possible
connector_3
them
and
component_5
the
end
to
end
quality_attribute_1
there
be
several
problem
with
this
first
load
generation
and
consumption
run
on
the
same
component_6
second
the
component_3
under
test
run
on
the
same
component_6
a
the
benchmark
client—both
of
these
confound
measurement
third
run
“pedal
to
the
metal”
and
look
at
the
connector_data_3
quality_attribute_1
isn’t
a
very
useful
benchmark
because
it’s
not
representative
of
a
production
environment
a
gil
tene

to
say
this
be
drive
your
car
a
fast
a
possible
crash
it
into
a
pole
and
look
at
the
shape
of
the
bumper
afterwards—it’s
always
go
to
look
bad
lastly
the
benchmark
component_5
average
quality_attribute_1
which
for
all
intent
and
purpose
be
a
useless
metric
to
look
at
i
connector_2
flotilla
to
automate
“scaled
up”
benchmarking—running
the
pattern_2
and
benchmark
component_7
on
separate
quality_attribute_3
vms
flotilla
also
attempt
to
capture
a
quality_attribute_2
pattern_3
of
quality_attribute_1
by
look
at
the
quality_attribute_1
distribution
though
it
only
go
up
to
the
99th
percentile
which
can
sweep
a
lot
of
really
bad
thing
under
the
rug
a
we’ll
see
late
however
it
still
run
test
at
full
throttle
which
isn’t
great
bench
be
an
attempt
to
connector_1
back
to
basic
it’s
a
quality_attribute_4
generic
benchmarking
technology_5
for
measure
quality_attribute_1
it
provide
a
straightforward
requester
which
can
be
connector_4
for
various
component_2
under
test
bench
work
by
attempt
to
issue
a
fix
rate
of
connector_data_4
per
second
and
measure
the
quality_attribute_1
of
each
connector_data_5
issue
synchronously
quality_attribute_1
be
capture
use
hdr
histogram
which
observe
the
complete
quality_attribute_1
distribution
and
allow
u
to
look
for
example
at
“six
nines”
quality_attribute_1
introduce
a
connector_data_5
schedule
allow
u
to
measure
quality_attribute_1
for
different
configuration
of
connector_data_5
rate
and
connector_data_1
size
but
in
a
“closed
loop”
test
it
create
another
problem
connector_5
coordinate
omission
the
problem
with
a
lot
of
benchmark
be
that
they
end
up
measure
component_8
time
rather
than
response_time
but
the
latter
be
likely
what
you
care
about
because
it’s
what
your
component_9
experience
the
best
way
to
describe
component_8
time
vs
response_time
be
to
think
of
a
cash
register
the
cashier
might
be
able
to
ring
up
a
requirement_2
in
under

second
99%
of
the
time
but
1%
of
the
time
it
take
three
minute
the
time
it
take
to
ring
up
a
requirement_2
be
the
component_8
time
while
the
response_time
consist
of
the
component_8
time
plus
the
time
the
requirement_2
wait
in
line
thus
the
response_time
be
dependent
upon
the
variation
in
both
component_8
time
and
the
rate
of
arrival
when
we
measure
quality_attribute_1
we
really
want
to
measure
response_time
now
let’s
think
about
how
most
quality_attribute_1
benchmark
work
they
usually
do
this
note
pattern_4
before
connector_data_5
t0
make
pattern_5
connector_data_5
note
pattern_4
after
connector_data_5
t1
component_5
quality_attribute_1
t1
–
t0
repeat
a
need
for
connector_data_5
schedule
what’s
the
problem
with
this
nothing
a
long
a
our
connector_data_4
fit
within
the
specify
connector_data_5
schedule
for
example
if
we’re
issue

connector_data_4
per
second
and
each
connector_data_5
take

m
to
complete
we’re
quality_attribute_2
however
if
one
connector_data_5
take

m
to
complete
that
mean
we
issue
only
one
connector_data_5
during
those

m
when
accord
to
our
schedule
we
should
have
issue

connector_data_4
in
that
window
nine
other
connector_data_4
should
have
be
issue
but
the
benchmark
effectively
coordinate
with
the
component_3
under
test
by
back
off
in
reality
those
nine
connector_data_4
wait
in
line—one
for

m
one
for

m
one
for

m
etc
most
benchmark
don’t
capture
this
time
spend
wait
in
line
yet
it
can
have
a
dramatic
effect
on
the
connector_data_3
the
graph
below
show
the
same
benchmark
with
coordinate
omission
both
uncorrected
red
and
correct
blue
hdr
histogram
attempt
to
correct
coordinate
omission
by
fill
in
additional
sample
when
a
connector_data_5
fall
outside
of
it
expect
interval
we
can
also
deal
with
coordinate
omission
by
simply
avoid
it
altogether—always
issue
connector_data_4
accord
to
the
schedule
connector_data_1
component_1
benchmark
i
benchmarked
several
pattern_1
component_2
use
bench—rabbitmq



technology_6




and




technology_7



pub
sub
and
nats



in
this
component_10
a
“request”
consist
of
publish
a
connector_data_1
to
the
component_11
and
wait
for
a
connector_6
i
e
a
quality_attribute_5
we
attempt
to
issue
connector_data_4
at
a
fix
rate
and
correct
for
coordinate
omission
then
plot
the
complete
quality_attribute_1
distribution
all
the
way
up
to
the

9999th
percentile
we
repeat
this
for
several
configuration
of
connector_data_5
rate
and
connector_data_5
size
it’s
also
important
to
note
that
each
connector_data_1
go
to
and
come
back
from
the
component_11
be
of
the
specify
size
i
e
the
“response”
be
the
same
size
a
the
“request
”
the
configuration
use
be
connector_data_6
below
each
configuration
be
run
for
a
sustain

second
256b
connector_data_4
at


connector_data_5
sec

kb
s
1kb
connector_data_4
at


connector_data_5
sec

connector_data_7
s
5kb
connector_data_4
at


connector_data_5
sec

connector_data_7
s
1kb
connector_data_4
at


connector_data_5
sec


connector_data_7
s
1mb
connector_data_4
at

connector_data_5
sec

connector_data_7
s
these
connector_data_1
size
be
mostly
arbitrary
and
there
might
be
a
quality_attribute_2
way
to
go
about
this
though
i
think
it’s
worth
point
out
that
the
ethernet
mtu
be

byte
so
accounting
for

the
maximum
amount
of
connector_data_8
you’ll
connector_1
in
a
single
technology_8
packet
will
likely
be
between

and

byte
the
component_3
under
test
and
benchmarking
component_12
be
on
two
different
m4
xlarge
technology_9
instance


ghz
intel
xeon
haswell
16gb
ram
with
enhance
requirement_3
enable
technology_7
and
nats
technology_7
pattern_6
and
nats
have
similar
requirement_1
characteristic
both
offer
very
lightweight
non
pattern_7
pattern_1
with
no
persistence
option
discount
redis’
rdb
and
aof
persistence
which
don’t
apply
to
pub
sub
and
both
support
some
level
of
topic
pattern_8
match
i’m
hesitant
to
connector_data_9
either
a
“message
queue”
in
the
traditional
sense
so
i
usually
refer
to
them
a
connector_data_1
pattern_2
or
bus
because
of
their
ephemeral
nature
both
be
a
nice
choice
for
low
quality_attribute_1
lossy
connector_data_1
technology_7
tail
quality_attribute_1
peak
around


m
nats
requirement_1
look
comparable
to
technology_7
quality_attribute_1
peak
around


m
the
resemblance
become
more
apparent
when
we
overlay
the
two
distribution
for
the
1kb
and
5kb
run
nats
tend
to
be
about


to


m
fast
the
1kb


connector_data_5
sec
run
us

concurrent
connector_7
with
concurrent
load
tail
quality_attribute_1
jump
up
peak
around

and

m
at
the

9999th
percentile
in
nats
and
technology_7
respectively
large
connector_data_2
1mb
don’t
hold
up
nearly
a
well
exhibit
large
tail
quality_attribute_1
start
around
the
95th
and
97th
percentile
in
nats
and
technology_7
respectively
1mb
be
the
default
maximum
connector_data_1
size
in
nats
the
quality_attribute_1
peak
around

m
again
keep
in
mind
these
be
pattern_5
quality_attribute_5
quality_attribute_1
apcera’s
ivan
kozlovic
point
out
that
the
version
of
the
nats
component_12
i
be
use
didn’t
include
a
recent
requirement_1
optimization
before
the
technology_10
requirement_4
scan
over
each
byte
in
the
connector_data_10
but
the

version
skip
to
the
end
the
previous
benchmark
be
update
to
use
the

version
the
optimization
do
have
a
noticeable
effect
illustrate
below
there
be
about
a
30%
improvement
with
the
5kb
quality_attribute_1
the
difference
be
even
more
pronounce
in
the
1mb
requirement_5
which
have
roughly
a
90%
improvement
up
to
the
90th
percentile
the
linear
quality_attribute_6
in
the
graph
below
hide
this
fact
but
at
the
90th
percentile
for
example
the
pre
optimization
quality_attribute_1
be

m
and
the
optimize
quality_attribute_1
be


m
clearly
the
large
tail
be
mostly
unaffected
however
in
general
this
show
that
nats
and
technology_7
be
quality_attribute_2
suit
to
small
connector_data_2
well
below
1mb
in
which
quality_attribute_1
tend
to
be
sub
millisecond
up
to
four
nine
technology_1
and
technology_6
technology_1
be
a
popular
technology_11
implementation
unlike
nats
it’s
a
more
traditional
connector_data_1
component_1
in
the
sense
that
it
support
bind
component_4
and
pattern_7
delivery
semantics
consequently
technology_1
be
a
more
“heavyweight”
pattern_9
solution
and
tend
to
pay
an
additional
premium
with
quality_attribute_1
in
this
benchmark
non
quality_attribute_7
component_4
be
use
a
a
connector_data_3
we
should
see
reduce
quality_attribute_1
since
we
aren’t
go
to
disk
quality_attribute_1
tend
to
be
sub
millisecond
up
to
the

7th
percentile
but
we
can
see
that
it
doesn’t
hold
up
to
nats
beyond
that
point
for
the
1kb
and
5kb
connector_data_10
technology_6
on
the
other
hand
require
disk
persistence
but
this
doesn’t
have
a
dramatic
effect
on
quality_attribute_1
until
we
look
at
the
94th
percentile
and
beyond
when
compare
to
technology_1
connector_8
should
be
to
component_13
pattern_10
with
flush
to
disk
happen
asynchronously
the
graph
below
be
for




once
again
the
1kb


connector_data_5
sec
run
be
quality_attribute_3
across

concurrent
connector_7
with
technology_1
we
see
the
dramatic
increase
in
tail
quality_attribute_1
a
we
do
with
technology_7
and
nats
the
technology_1
quality_attribute_1
in
the
concurrent
requirement_5
stay
in
line
with
the
previous
quality_attribute_1
up
to
about
the
99th
percentile
interestingly
technology_6
doesn’t
appear
to
be
significantly
affect
the
quality_attribute_1
of


connector_data_5
sec
at
1kb
per
connector_data_5
be
not
terribly
different
than
the
quality_attribute_1
of


connector_data_5
sec
at
1kb
per
connector_data_5
both
peak
around

m
what’s
particularly
interest
be
the
behavior
of
1mb
connector_data_2
vs
the
rest
with
technology_1
there’s
almost
a
14x
difference
in
max
quality_attribute_1
between
the
5kb
and
1mb
run
with
1mb
be
the
fast
with
technology_6




the
difference
be
over
126x
in
the
same
direction
we
can
plot
the
1mb
quality_attribute_1
for
technology_1
and
technology_6
since
it’s
difficult
to
discern
them
with
a
linear
quality_attribute_6
i
try
to
understand
what
be
cause
this
behavior
i’ve
yet
to
find
a
reasonable
explanation
for
technology_1
intuition
tell
me
it’s
a
connector_data_3
of
buffering—either
at
the
o
level
or
elsewhere—and
the
large
connector_data_2
cause
more
frequent
flush
remember
that
these
benchmark
be
with
transient
publish
there
should
be
no
disk
connector_9
occur
though
my
knowledge
of
rabbit’s
internals
be
admittedly
limit
the
fact
that
this
behavior
occur
in
technology_1
and
not
technology_7
or
nats
seem
odd
nagle’s
algorithm
be
disable
in
all
of
the
benchmark
tcp_nodelay
after
inspect
packet
with
wireshark
it
doesn’t
appear
to
be
a
problem
with
delay
acks
to
show
how
stagger
the
difference
be
we
can
plot
technology_6




and
technology_1
1mb
quality_attribute_1
alongside
technology_7
and
nats
5kb
quality_attribute_1
they
be
all
within
the
same
ballpark
whatever
the
requirement_5
be
both
technology_1
and
technology_6
appear
to
handle
large
connector_data_2
extremely
well
in
contrast
to
technology_7
and
nats
this
lead
me
to
believe
you’ll
see
quality_attribute_2
overall
quality_attribute_8
in
term
of
raw
connector_data_8
with
technology_1
and
technology_6
but
more
quality_attribute_9
tight
tail
quality_attribute_1
with
technology_7
and
nats
where
slas
be
important
it’s
hard
to
beat
nats
of

it’s
unfair
to
compare
technology_6
with
something
nats
or
technology_7
or
even
technology_1
since
they
be
very
different
and
sometimes
complementary
but
it’s
also
worth
point
out
that
the
former
be
much
more
operationally
complex
however
benchmarking
technology_6




blue
and
red
show
an
astound
difference
in
tail
quality_attribute_1
compare
to




orange
and
green
technology_6

9’s
requirement_1
be
much
more
in
line
with
rabbitmq’s
at
high
percentile
a
see
below
likewise
it’s
a
much
close
comparison
to
nats
when
look
at
the
1kb
and
5kb
run
a
with


technology_6


do
an
impressive
deal
with
1mb
connector_data_2
in
comparison
to
nats
especially
when
look
at
the
92nd
percentile
and
beyond
it’s
hard
to
decipher
in
the
graph
below
but
technology_6

9’s
99th

9th
and

99th
percentile
quality_attribute_1
be




and


m
respectively
my
initial
think
be
that
the
difference
between
technology_6


and


be
attribute
to
a
connector_10
in
fsync
behavior
to
quote
the
technology_6
documentation
technology_6
always
immediately
connector_8
all
connector_data_8
to
the
filesystem
and
support
the
ability
to
configure
the
flush
requirement_6
that
control
when
connector_data_8
be
force
out
of
the
o
pattern_10
and
onto
disk
use
the
and
flush
this
flush
requirement_6
can
be
control
to
force
connector_data_8
to
disk
after
a
period
of
time
or
after
a
certain
number
of
connector_data_2
have
be
connector_2
however
there
don’t
appear
to
be
any
connector_11
in
the
default
flush
configuration
between


and


the
default
configuration
disable
component_14
fsync
entirely
instead
rely
on
the
os’s
background
flush
jay
kreps
indicate
it’s
a
connector_data_3
of
several
“high
percentile
quality_attribute_1
issues”
that
be
fix
in


after
scan
the


release
note
i
be
unable
to
determine
specifically
what
those
fix
might
be
either
way
the
difference
be
certainly
not
something
to
scoff
at
conclusion
a
always
interpret
these
benchmark
connector_data_11
with
a
critical
eye
and
perform
your
own
test
if
you’re
evaluate
these
component_3
this
be
more
an
exercise
in
benchmark
methodology
and
technology_3
than
an
actual
component_3
analysis
and
a
always
there’s
still
a
lot
of
room
for
improvement
if
anything
i
think
these
connector_data_11
show
how
much
we
can
miss
by
not
look
beyond
the
99th
percentile
in
almost
all
requirement_5
everything
look
pretty
quality_attribute_2
up
to
that
point
but
after
that
thing
can
connector_1
really
bad
this
be
important
to
be
conscious
of
when
discuss
slas
i
think
the
key
takeaway
be
to
consider
your
expect
load
in
production
benchmark
configuration
around
that
determine
your
allowable
component_8
level
and
iterate
or
provision
more
resource
until
you’re
within
those
limit
the
other
important
takeaway
with
respect
to
benchmarking
be
to
look
at
the
complete
quality_attribute_1
distribution
otherwise
you’re
not
connector_12
a
clear
picture
of
how
your
component_3
actually
behave
follow
@tyler_treat

on

2014october

2020dissecting
connector_data_1
component_4
disclaimer



–
the
benchmark
and
requirement_1
analysis
present
in
this
should
not
be
rely
on
this
be
connector_2
roughly
six
year
ago
and
at
the
time
be
the
connector_data_3
of
my
exploration
of
various
pattern_1
component_3
the
benchmark
be
not
connector_4
in
a
meaningful
way
which
i
discuss
in
a
follow
up

this
will
remain
for
posterity
and

purpose
but
i
do
not
claim
that
this
connector_data_12
be
quality_attribute_10
or
useful
continue
my
series
on
connector_data_1
component_1
i
spend
this
weekend
dissect
various
technology_5
for
perform
quality_attribute_3
connector_data_1
in
this
analysis
i
look
at
a
few
different
aspect
include
component_15
characteristic
ease
of
deployment
and
quality_attribute_11
and
requirement_1
quality
the
connector_data_1
component_4
have
be
categorize
into
two
group
brokerless
and
pattern_2
brokerless
connector_data_1
component_4
be
pattern_11
such
that
there
be
no
middleman
involve
in
the
transmission
of
connector_data_1
while
pattern_2
component_4
have
some
sort
of
component_11
in
between

the
component_2
i’ll
be
analyze
be
brokerless
nanomsg
technology_12
pattern_2
technology_13
nats
technology_6
kestrel
nsq
technology_1
technology_7
technology_14
nats
to
start
let’s
look
at
the
requirement_1
metric
since
this
be
arguably
what
people
care
the
most
about
i’ve
measure
two
key
metric
quality_attribute_8
and
quality_attribute_1
all
test
be
run
on
a
macbook
pro


ghz
i7
16gb
ram
these
test
be
evaluate
a
pattern_12
topology
with
a
single
component_16
and
single
component_17
this
provide
a
quality_attribute_2
baseline
it
would
be
interest
to
benchmark
a
quality_attribute_6
up
topology
but
require
more
instrumentation
the
use
for
benchmarking
connector_2
in
go
be
quality_attribute_12
on
technology_15
the
connector_data_11
below
shouldn’t
be
take
a
gospel
a
there
be
likely
optimization
that
can
be
make
to
squeeze
out
requirement_1
gain
connector_13
connector_data_4
be
welcome
quality_attribute_8
benchmark
quality_attribute_8
be
the
number
of
connector_data_2
per
second
the
component_3
be
able
to
component_18
but
what’s
important
to
note
here
be
that
there
be
no
single
“throughput”
that
a
component_1
might
have
we’re
connector_14
connector_data_2
between
two
different

so
what
we
observe
be
a
“sender”
quality_attribute_8
and
a
“receiver”
throughput—that
be
the
number
of
connector_data_2
that
can
be
connector_15
per
second
and
the
number
of
connector_data_2
that
can
be
connector_3
per
second
this
test
be
perform
by
connector_14



1kb
connector_data_2
and
measure
the
time
to
connector_16
and
connector_17
on
each
side
many
requirement_1
test
tend
to
use
small
connector_data_2
in
the
range
of

to

byte
i
choose
1kb
because
it’s
more
representative
of
what
you
might
see
in
a
production
environment
although
this
vary
requirement_5
by
requirement_5
for
connector_data_1
orient
technology_16
component_3
only
one
pattern_2
be
use
in
most
requirement_5
a
cluster
environment
would
yield
much
quality_attribute_2
connector_data_3
unsurprisingly
there’s
high
quality_attribute_8
on
the
connector_14
side
what’s
interest
however
be
the
disparity
in
the
sender
to
receiver
ratio
technology_12
be
capable
of
connector_14
over



connector_data_2
per
second
but
be
only
able
to
connector_17
about


second
in
contrast
nanomsg
connector_18
shy
of



second
but
can
connector_17
almost



now
let’s
take
a
look
at
the
pattern_2
connector_data_1
component_1
intuitively
we
observe
that
pattern_2
connector_data_1
component_4
have
dramatically
le
quality_attribute_8
than
their
brokerless
counterpart
by
a
couple
order
of
magnitude
for
the
most
part
half
the
pattern_2
component_4
have
a
quality_attribute_8
below


connector_data_1
second
the
number
for
technology_7
might
be
a
bit
mislead
though
despite
provide
pattern_6
requirement_7
it’s
not
really
design
to
operate
a
a
quality_attribute_13
pattern_1
component_1
in
a
similar
fashion
to
technology_17
technology_7
disconnect
slow
component_12
and
it’s
important
to
point
out
that
it
be
not
able
to
quality_attribute_14
handle
this
volume
of
connector_data_1
a
such
we
consider
it
an
outlier
technology_6
and
technology_14
nats
have
similar
requirement_1
characteristic
to
technology_7
but
be
able
to
quality_attribute_14
handle
the
connector_data_1
volume
without
intermittent
failure
the
go
implementation
of
nats
gnatsd
have
exceptional
quality_attribute_8
for
a
pattern_2
connector_data_1
component_1
outlier
aside
we
see
that
the
pattern_2
component_4
have
fairly
uniform
quality_attribute_8
unlike
the
brokerless
technology_5
there
be
little
to
no
disparity
in
the
sender
to
receiver
ratio
which
themselves
be
all
very
close
to
one
quality_attribute_1
benchmark
the
second
key
requirement_1
metric
be
connector_data_1
quality_attribute_1
this
measure
how
long
it
take
for
a
connector_data_1
to
be
connector_19
between

intuition
might
tell
u
that
this
be
simply
the
inverse
of
quality_attribute_8
i
e
if
quality_attribute_8
be
connector_data_1
second
quality_attribute_1
be
second
connector_data_1
however
by
look
closely
at
this
image
borrow
from
a
technology_12
white
paper
we
can
see
that
this
isn’t
quite
the
requirement_5
the
reality
be
that
the
quality_attribute_1
per
connector_data_1
connector_15
over
the
wire
be
not
uniform
it
can
vary
wildly
for
each
one
in
truth
the
relationship
between
quality_attribute_1
and
quality_attribute_8
be
a
bit
more
involve
unlike
quality_attribute_8
however
quality_attribute_1
be
not
measure
at
the
sender
or
the
receiver
but
rather
a
a
whole
but
since
each
connector_data_1
have
it
own
quality_attribute_1
we
will
look
at
the
average
of
all
of
them
go
further
we
will
see
how
the
average
connector_data_1
quality_attribute_1
fluctuate
in
relation
to
the
number
of
connector_data_2
connector_16
again
intuition
tell
u
that
more
connector_data_2
mean
more
component_1
which
mean
high
quality_attribute_1
a
we
do
before
we’ll
start
by
look
at
the
brokerless
component_3
in
general
our
hypothesis
prove
correct
in
that
a
more
connector_data_2
be
connector_15
through
the
component_3
the
quality_attribute_1
of
each
connector_data_1
increase
what’s
interest
be
the
taper
at
the


point
in
which
quality_attribute_1
appear
to
increase
at
a
slow
rate
a
we
approach



connector_data_1
another
interest
observation
be
the
initial
spike
in
quality_attribute_1
between


and


connector_data_1
which
be
more
pronounce
with
nanomsg
it’s
difficult
to
pinpoint
causation
but
these
connector_11
might
be
indicative
of
how
connector_data_1
pattern_13
and
other
requirement_3
technology_18
traversal
optimization
be
connector_4
in
each
technology_5
more
connector_data_8
point
provide
quality_attribute_2
visibility
we
see
some
similar
pattern_8
with
pattern_2
component_4
and
also
some
interest
one
technology_7
behave
in
a
similar
manner
a
before
with
an
initial
quality_attribute_1
spike
and
then
a
quick
taper
off
it
differ
in
that
the
taper
become
essentially
constant
right
after


connector_data_1
nsq
doesn’t
exhibit
the
same
spike
in
quality_attribute_1
and
behave
more
or
le
linearly
kestrel
fit
our
hypothesis
notice
that
technology_14
nats
and
nats
hardly
even
register
on
the
requirement_8
they
exhibit
surprisingly
low
quality_attribute_1
and
unexpected
relationship
with
the
number
of
connector_data_1
interestingly
the
connector_data_1
quality_attribute_1
for
technology_14
nats
and
nats
appear
to
be
constant
this
be
counterintuitive
to
our
hypothesis
you
have
notice
that
technology_6
technology_19
and
technology_1
be
absent
from
the
above
requirement_8
this
be
because
their
quality_attribute_1
tend
to
be
order
of
magnitude
high
than
the
other
pattern_2
connector_data_1
component_1
so
technology_13
and
technology_1
be
group
into
their
own
technology_11
category
i’ve
also
include
technology_6
since
it’s
in
the
same
ballpark
here
we
see
that
rabbitmq’s
quality_attribute_1
be
constant
while
technology_13
and
technology_6
be
linear
what’s
unclear
be
the
apparent
disconnect
between
their
quality_attribute_8
and
mean
quality_attribute_1
qualitative
analysis
now
that
we’ve
see
some
empirical
connector_data_8
on
how
these
different
technology_5
perform
i’ll
take
a
look
at
how
they
work
from
a
pragmatic
point
of
pattern_3
connector_data_1
quality_attribute_8
and
quality_attribute_15
be
important
but
it
isn’t
very
practical
if
the
technology_5
be
difficult
to
use
quality_attribute_16
or
maintain
technology_12
and
nanomsg
technically
speak
nanomsg
isn’t
a
connector_data_1
component_1
but
rather
a
connector_data_13
style
technology_5
for
perform
quality_attribute_3
pattern_1
through
a
variety
of
convenient
pattern_8
a
a
connector_data_3
there’s
nothing
to
quality_attribute_16
aside
from
embed
the
technology_5
itself
within
your
component_14
this
make
deployment
a
non
issue
nanomsg
be
connector_2
by
one
of
the
technology_12
author
and
a
i
discuss
before
work
in
a
very
similar
way
to
that
technology_5
from
a
development
standpoint
nanomsg
provide
an
overall
clean
technology_20
unlike
technology_17
there
be
no
notion
of
a
component_10
in
which
technology_21
be
bind
to
furthermore
nanomsg
provide
pluggable
transport
and
pattern_1
technology_10
which
make
it
more
open
to
extension
it
additional
build
in
quality_attribute_17
technology_10
also
make
it
quite
appeal
technology_17
it
guarantee
that
connector_data_2
will
be
connector_20
atomically
intact
and
order
but
do
not
guarantee
the
delivery
of
them
partial
connector_data_2
will
not
be
connector_20
and
it’s
possible
that
some
connector_data_2
won’t
be
connector_20
at
all
the
library’s
author
martin
sustrik
make
this
abundantly
clear
guarantee
delivery
be
a
myth
nothing
be
100%
guarantee
that’s
the
nature
of
the
world
we
live
in
what
we
should
do
instead
be
to
build
an
internet

component_3
that
be
resilient
in
face
of
failure
and
connector_21
around
damage
the
philosophy
be
to
use
a
combination
of
topology
to
build
resilient
component_2
that
in
these
guarantee
in
a
best
effort
sort
of
way
on
the
other
hand
nanomsg
be
still
in
beta
and
not
be
consider
production
ready
consequently
there
aren’t
a
lot
of
resource
quality_attribute_12
and
not
much
of
a
development
around
it
technology_12
be
a
battle
test
pattern_1
technology_5
that’s
be
around
since

some
perceive
it
a
a
predecessor
to
nanomsg
but
what
nano
lack
be
where
technology_12
thrives—a
flourish
developer
and
a
deluge
of
resource
and
support
material
for
many
it’s
the
de
facto
technology_3
for
build
fast
pattern_14
quality_attribute_3
pattern_1
component_2
that
quality_attribute_6
nanomsg
technology_12
be
not
a
connector_data_1
orient
technology_16
and
simply
operate
a
a
connector_data_13
abstraction
in
term
of
quality_attribute_18
it’s
very
much
the
same
a
nanomsg
although
it
component_15
be
marginally
more
involve
technology_13
and
technology_1
technology_13
and
technology_1
be
implementation
of
technology_22
they
act
a
pattern_2
which
ensure
connector_data_2
be
connector_20
technology_13
and
technology_1
support
both
persistent
and
non
persistent
delivery
by
default
connector_data_2
be
connector_2
to
disk
such
that
they
survive
a
pattern_2
restart
they
also
support
pattern_5
and
pattern_14
connector_14
of
connector_data_2
with
the
former
have
substantial
impact
on
quality_attribute_1
to
guarantee
delivery
these
pattern_2
use
connector_data_1
acknowledgement
which
also
incur
a
massive
quality_attribute_1
penalty
a
far
a
quality_attribute_19
and
fault
tolerance
go
these
pattern_2
support
cluster
through
connector_22
storage
or
connector_22
nothing
component_4
can
be
replicate
across
cluster
technology_23
so
there
be
no
single
point
of
failure
or
connector_data_1
loss
technology_11
be
a
non
trivial
technology_10
which
it
creator
claim
to
be
over
engineer
these
additional
guarantee
be
make
at
the
expense
of
major
complexity
and
requirement_1
requirement_9
off
fundamentally
component_7
be
more
difficult
to
connector_4
and
use
since
they’re
connector_data_1
pattern_2
technology_13
and
technology_1
be
additional
move
part
that
need
to
be
manage
in
your
quality_attribute_3
component_3
which
bring
deployment
and
quality_attribute_11
cost
the
same
be
true
for
the
remain
connector_data_1
component_4
be
discuss
nats
and
technology_14
nats
nats
gnatsd
be
a
pure
go
implementation
of
the
technology_14
nats
pattern_1
component_3
nats
be
quality_attribute_3
pattern_1
rethink
to
be
le
enterprisey
and
more
lightweight
this
be
in
direct
contrast
to
component_2
technology_19
technology_1
and
others
apcera’s
derek
collison
the
library’s
author
and
former
technology_24
architect
describe
nats
a
“more
a
nervous
system”
than
an
requirement_10
connector_data_1
component_1
it
doesn’t
do
persistence
or
connector_data_1
transaction
but
it’s
fast
and
easy
to
use
cluster
be
support
so
it
can
be
build
on
top
of
with
high
quality_attribute_19
and
failover
in
mind
and
component_7
can
be
sharded
unfortunately
tl
and
technology_25
be
not
yet
support
in
nats
they
be
in
the
technology_14
nats
but
on
the
roadmap
a
we
observe
early
nats
perform
far
quality_attribute_2
than
the
original
technology_26
implementation
component_7
can
be
use
interchangeably
with
nats
and
technology_14
nats
technology_6
originally
develop
by
linkedin
technology_6
connector_23
pattern_12
pattern_1
through
a
quality_attribute_3
connector_24
requirement_11
it’s
design
to
operate
a
a
cluster
that
can
be
connector_25
by
large
amount
of
component_12
horizontal
quality_attribute_20
be
do
effortlessly
use
technology_27
so
that
additional
component_19
and
pattern_2
can
be
introduce
seamlessly
it
also
transparently
take
care
of
cluster
rebalancing
technology_6
us
a
persistent
connector_24
requirement_11
to
component_20
connector_data_2
on
the
pattern_2
unlike
other
quality_attribute_7
component_4
which
usually
remove
persist
connector_data_2
on
consumption
technology_6
retain
them
for
a
configure
period
of
time
this
mean
that
connector_data_2
can
be
“replayed”
in
the
that
a
component_17
fail
technology_27
make
manage
technology_6
cluster
relatively
easy
but
it
do
introduce
yet
another
element
that
need
to
be
maintain
that
say
technology_6
connector_26
a
great
component_15
and
technology_28
have
an
excellent
go
component_12
connector_5
sarama
that
make
interfacing
with
technology_6
very
quality_attribute_21
kestrel
kestrel
be
a
quality_attribute_3
connector_data_1
component_1
open
component_21
by
twitter
it’s
intend
to
be
fast
and
lightweight
because
of
this
it
have
no
concept
of
cluster
or
failover
while
technology_6
be
build
from
the
grind
up
to
be
cluster
through
technology_27
the
onus
of
connector_data_1
partitioning
be
put
upon
the
component_7
of
kestrel
there
be
no
cross
connector_27
between
technology_23
it
make
this
requirement_9
off
in
the
name
of
quality_attribute_22
it
feature
quality_attribute_7
component_1
item
expiration
pattern_7
connector_28
and
fanout
component_4
while
operate
over
technology_29
or
memcache
technology_10
kestrel
be
design
to
be
small
but
this
mean
that
more
work
must
be
do
by
the
developer
to
build
out
a
quality_attribute_13
pattern_1
component_3
on
top
of
it
technology_6
seem
to
be
a
more
“all
in
one”
solution
nsq
nsq
be
a
pattern_1
component_22
build
by
bitly
i
use
the
word
component_22
because
there’s
a
lot
of
technology_3
build
around
nsq
to
make
it
useful
for
real
time
quality_attribute_3
connector_data_1
the
daemon
that
connector_17
component_1
and
connector_29
connector_data_2
to
component_7
be
connector_5
nsqd
the
daemon
can
run
standalone
but
nsq
be
design
to
run
in
a
a
quality_attribute_3
decentralized
topology
to
achieve
this
it
leverage
another
daemon
connector_5
nsqlookupd
nsqlookupd
act
a
a
component_8
discovery
mechanism
for
nsqd
instance
nsq
also
provide
nsqadmin
which
be
a
web
ui
that
display
real
time
cluster
statistic
and
act
a
a
way
to
perform
various
administrative
connector_data_14
clear
component_4
and
manage
topic
by
default
connector_data_2
in
nsq
be
not
quality_attribute_7
it’s
primarily
design
to
be
an
in
memory
connector_data_1
component_1
but
component_1
size
can
be
configure
such
that
after
a
certain
point
connector_data_2
will
be
connector_2
to
disk
despite
this
there
be
no
build
in
pattern_15
nsq
us
acknowledgement
to
guarantee
connector_data_1
delivery
but
the
order
of
delivery
be
not
guarantee
connector_data_2
can
also
be
connector_20
more
than
once
so
it’s
the
developer’s
responsibility
to
introduce
idempotence
similar
to
technology_6
additional
technology_23
can
be

to
an
nsq
cluster
seamlessly
it
also
connector_26
both
an
technology_30
and
technology_8
technology_20
which
mean
you
don’t
actually
need
a
component_12
technology_5
to
connector_30
connector_data_2
into
the
component_3
despite
all
the
move
part
it’s
actually
quite
easy
to
quality_attribute_16
it
component_15
be
also
easy
to
use
and
there
be
a
number
of
component_12
technology_5
quality_attribute_12
technology_7
last
up
be
technology_7
while
technology_7
be
great
for
lightweight
pattern_1
and
transient
storage
i
can’t
advocate
it
use
a
the
technology_31
of
a
quality_attribute_3
pattern_1
component_3
it
pattern_6
be
fast
but
it
capability
be
limit
it
would
require
a
lot
of
work
to
build
a
quality_attribute_13
component_3
there
be
solution
quality_attribute_2
suit
to
the
problem
such
a
those
describe
above
and
there
be
also
some
quality_attribute_20
concern
with
it
these
matter
aside
technology_7
be
easy
to
use
it’s
easy
to
quality_attribute_16
and
manage
and
it
have
a
relatively
small
footprint
quality_attribute_23
on
the
use
requirement_5
it
can
be
a
great
choice
for
real
time
pattern_1
a
i’ve
explore
before
conclusion
the
purpose
of
this
analysis
be
not
to
present
some
sort
of
“winner”
but
instead
showcase
a
few
different
option
for
quality_attribute_3
connector_data_1
there
be
no
“one
size
fit
all”
option
because
it
quality_attribute_23
entirely
on
your
need
some
use
requirement_5
require
fast
fire
and
forget
connector_data_1
others
require
delivery
guarantee
in
fact
many
component_2
will
connector_data_9
for
a
combination
of
these
my
hope
be
that
this
dissection
will
offer
some
insight
into
which
solution
work
best
for
a
give
problem
so
that
you
can
make
an
intelligent
decision
follow
@tyler_treat
popularyou
cannot
have
exactly
once
deliveryeverything
you
about
quality_attribute_1
be
wrongbenchmarking
connector_data_1
component_1
latencystructuring
a
requirement_12
infrastructure
organizationa
look
at
nanomsg
and
quality_attribute_17
technology_10
why
technology_12
shouldn
t
be
your
first
choice
recent
sre
doesn’t
quality_attribute_6
connector_data_15
a
requirement_12
infrastructure
organization
we
suck
at
meet
connector_12
big
win
with
small
team
on
tight
deadline
continuous
deployment
for
technology_32
glue
category
algorithm
requirement_13
technology_33
technology_32
bash
benchmarking
requirement_14
requirement_12
component_23
science
pattern_16
consult
culture
connector_data_8
connector_data_16
component_24
design
pattern_8
devops
quality_attribute_3
component_2
economics
gcp
go
infinitum
technology_34
technology_35
technology_36
liftbridge
requirement_15
mathematics
pattern_1
postmortem
technology_37
real
kinetic
quality_attribute_24
architecture
engineering
technology_38
component_2
theory
unix
archive


























































tag
agile
algorithm
technology_33
component_25
component_26
architecture
benchmarking
requirement_14
cap
theorem
requirement_12
requirement_12
requirement_16
consensus
consistency
consult
culture
component_24
design
pattern_8
devops
quality_attribute_3
requirement_11
quality_attribute_3
component_2
engineering
culture
engineering
empathy
fault
tolerance
gcp
go
infinitum
technology_34
technology_6
connector_data_1
orient
technology_16
connector_data_1
component_4
pattern_1
pattern_17
nats
nats
connector_31
ops
requirement_1
component_18
technology_39
development
productivity
raft
quality_attribute_17
serverless
technology_40
engineering
connector_32
component_18
component_2
proudly
powered
by
technology_41
