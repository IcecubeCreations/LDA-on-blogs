build
and
quality_attribute_1
quality_attribute_2
requirement_1
in
production
with
kafkaregister
for
demo
|
rbac
at
quality_attribute_3
technology_1
cdc
component_1
connector
and
more
within
our
q2
launch
for
confluent
cloudcontact
usproductschoose
your
deploymentconfluent
requirement_2
requirement_3
login

confluent
component_2
subscription
connector
technology_2
connector_1
governance
confluent
vs
technology_3
why
you
need
confluent
solutionsby
requirement_4
by
use
requirement_5
by
architecture
by
requirement_6
all
solution
hybrid
and
multicloud
modernization
pattern_1
pattern_2
connector_2
technology_4
use
requirement_5
showcase
connector_2
use
requirement_5
to
transform
your
requirement_7
learnblog
resource
train
professional
component_3
career
meetups
technology_3
summit
webinars
connector_2
technology_4
requirement_2
demo
master
technology_3
connector_3
and
technology_2
pattern_2
with
confluent
developersconfluent
developer
doc
technology_5
technology_3
quick
start
connector_2
audio
podcast
ask
the
connector_4
start
freeus
englishget
start
freeproductschoose
your
deploymentconfluent
requirement_2
requirement_3
login

confluent
component_2
subscription
connector
technology_2
connector_1
governance
confluent
vs
technology_3
why
you
need
confluent
solutionsby
requirement_4
by
use
requirement_5
by
architecture
by
requirement_6
all
solution
hybrid
and
multicloud
modernization
pattern_1
pattern_2
connector_2
technology_4
use
requirement_5
showcase
connector_2
use
requirement_5
to
transform
your
requirement_7
learnblog
resource
train
professional
component_3
career
meetups
technology_3
summit
webinars
connector_2
technology_4
requirement_2
demo
master
technology_3
connector_3
and
technology_2
pattern_2
with
confluent
developersconfluent
developer
doc
technology_5
technology_3
quick
start
connector_2
audio
podcast
ask
the
connector_4
start
freeanalyticshow
to
build
and
quality_attribute_1
quality_attribute_2
requirement_1
in
production
with
technology_5
kafkakai
waehnersep

2017scalable
requirement_1
in
production
with
technology_5
kafka®
intelligent
real
time
component_4
be
a
game
changer
in
any
requirement_4
requirement_1
and
it
sub
topic
deep

be
gain
momentum
because
requirement_1
allow
component_5
to
find
hide
insight
without
be
explicitly
component_6
where
to
look
this
capability
be
need
for
analyze
pattern_3
connector_data_1
image
recognition
speech
recognition
and
intelligent
decision
make
it
be
an
important
difference
from
traditional
programming
with
technology_6
net
or
technology_7
while
the
concept
behind
requirement_1
be
not

the
quality_attribute_4
of
requirement_8
set
and
component_7
power
allow
every
requirement_9
to
build
powerful
analytic
component_8
plenty
of
use
requirement_5
exist
in
any
requirement_4
to
increase
revenue
reduce
cost
or
improve
requirement_10
by
apply
analytic
component_9
in
requirement_9
component_4
and
pattern_2
this
discus
potential
use
requirement_5
for
requirement_1
in
mission
critical
real
time
component_4
leverage
technology_5
kafka®
a
central
quality_attribute_2
mission
critical
nervous
component_10
plus
technology_5
kafka’s
connector_3
component_11
to
build
intelligent
connector_2
component_12
quality_attribute_2
mission
critical
real
time
component_4
the
emergence
of
the
internet
smartphones
and
always
on
think
have
connector_5
how
people
behave
today
this
include
people’s
expectation
about
how
component_13
technology_8
and
component_3
connector_6
with
them
people
expect
connector_data_2
in
real
time
now
the
challenge
for
requirement_9
be
to
act
on
critical
requirement_7
moment
before
it
be
too
late
pattern_4
component_7
be
not
sufficient
anymore
you
need
to
act
immediately
or
even
quality_attribute_5
proactively
traditional
requirement_9
can
connector_7
very
powerful
real
time
component_7
for
their
daily
requirement_7
often
domain
knowledge
be
need
to
understand
the
scenario
and
build
connector_2
requirement_11
to
requirement_7
requirement_12
connector_1
component_7
use
requirement_5
exist
in
every
requirement_4
for
example
fraud
detection
correlate
payment
connector_data_2
with
other
historical
connector_data_1
or

pattern_5
to
detect
fraud
before
it
happen
this
typically
need
very
fast
component_7
a
you
must
decline
a
transaction
before
settle
the
requirement_13
movement
connector_8
the
connector_data_2
or
ship
the
item
cross
sell
correlate
requirement_6
requirement_8
to
make
component_14
specific
personal
customize
offer
or
discount
before
the
requirement_6
leave
the
component_15
you
leverage
real
time
connector_data_2

location
base
connector_data_1
payment
connector_data_1
but
also
historical
connector_data_1

connector_data_2
from
your
crm
or
loyalty
component_2
to
make
the
best
offer
to
every
single
requirement_6
predictive
quality_attribute_6
correlate
component_16
requirement_8
to
predict
failure
before
it
happen
this
allow
replace
part
before
they
break
quality_attribute_7
on
the
requirement_4
and
use
requirement_5
this
can
connector_9
a
lot
of
money
e
g
manufacture
increase
revenue
e
g
vending
component_16
or
increase
requirement_10
e
g
telco
requirement_14
failure
prediction
the
key
in
all
these
use
requirement_5
be
that
you
component_7
requirement_8
while
it
be
in
motion
you
need
to
handle
the
before
it
be
too
late
to
act
be
proactive
not
reactive
your
component_10
should
make
decision
before
a
fraudulent
transaction
happen
before
the
requirement_6
leave
the
component_15
before
a
component_16
break
this
do
not
always
mean
that
you
need
millisecond
response_time
though
even
pattern_4
component_7
of
be
fine
in
several
use
requirement_5
for
example
in
most
manufacture
or
internet
of
thing
iot
use
requirement_5
for
predictive
quality_attribute_6
you
pattern_6
time
window
of
several
hour
or
even
day
to
detect
issue
in
infrastructure
or
component_13
replacement
of
defective
part
be
sufficient
within
a
day
or
week
this
be
a
huge
requirement_7
requirement_5
and
connector_10
a
lot
of
money
because
you
can
detect
issue
and
fix
them
before
they
happen
or
even
also
destroy
other
part
in
the
environment
intelligent
real
time
component_4
leverage
requirement_1
mission
critical
real
time
component_4
the
above
have
be
build
for
years—without
requirement_1
why
be
requirement_1
the
game
changer
if
you
connector_11
about
requirement_1
and
it
sub
topic
deep

you
often
see
example
these
image
recognition
connector_12
a
picture
to
your
timeline
and
connector_data_3
your
friend
the
background
or
the
beer
in
your
hand
be
analyze
speech
translation
this
enable
chat
requirement_15
that
connector_13
with
human
via
generate
text
or
speech
human

behavior
watson
have
beat
the
best
jeopardy
player
google’s
alphago
have
beat
professional
go
player
these
example
become
more
and
more
relevant
for
requirement_9
look
to
build
innovative
component_4
and
differentiate
from
competitor
in
the
same
way
you
can
apply
requirement_1
to
more
“traditional
scenarios”
fraud
detection
cross
sell
or
predictive
quality_attribute_6
to
enhance
your
exist
requirement_7
component_17
and
make
quality_attribute_5
connector_data_1
drive
decision
the
exist
requirement_7
component_7
can
stay
a
it
be
you
merely
replace
the
quality_attribute_8
custom
cod
requirement_7
component_18
and
rule
by
analytic
component_9
to
improve
the
automate
decision
the
follow
section
show
how
to
build
operate
and
pattern_6
analytic
component_9
in
a
quality_attribute_2
mission
critical
way
by
leverage
technology_5
kafka®
a
a
connector_2
component_2
requirement_1
–
the
development
lifecycle
to
quality_attribute_1
analytic
component_9
let’s
first
think
about
the
development
lifecycle
of
analytic
component_8
build
use
requirement_1
algorithm
glm
naive
bay
random
forest
gradient
technology_9
neural
requirement_14
or
others
to
analyze
historical
connector_data_1
to
find
insight
this
step
include
connector_data_4
collection
preparation
or
transformation
of
connector_data_1
validate
use
technique
such
a
cross
validation
to
double
connector_14
that
the
build
analytic
component_8
work
on
input
connector_data_1
operate
quality_attribute_1
the
build
analytic
component_8
to
a
production
environment
to
apply
it
on
incoming
in
real
time
pattern_6
watch
the
outcome
of
the
apply
component_8
this
contain
two
part
connector_15
alert
if
a
threshold
be
reach
requirement_7
pattern_6
assure
that
the
quality_attribute_9
and
other
metric
be
quality_attribute_5
enough
analytic
component_8
pattern_6
continuous
loop
improve
the
analytic
component_8
by
go
through
all
above
step
continuously
this
can
be
do
in
manual
pattern_4
mode
say
once
a
week
or
online
where
the
component_8
be
update
for
every
incoming

the
whole
project
team
must
work
together
from
the
begin
to
discus
question

how
do
it
need
to
perform
in
production
what
technology_10
do
the
production
component_10
use
or
support
how
will
we
pattern_6
the
component_8
inference
and
requirement_16
do
we
build
a
complete
requirement_1
infrastructure
cover
the
whole
lifecycle
or
use
exist
technology_11
to
separate
component_8
train
from
component_8
inference
for
example
a
connector_data_1
scientist
can
build
a
technology_7
component_6
which
create
a
component_8
that
score
very
well
with
high
quality_attribute_9
but
this
do
not
help
a
you
cannot
quality_attribute_1
it
to
production
because
it
do
not
quality_attribute_3
or
perform
a
need
i
suspect
you
can
already
imagine
why
technology_5
kafka®
be
a
perfect
fit
for
productionizing
analytic
component_8
the
follow
section
will
explain
the
usage
of
technology_5
kafka®
a
a
connector_2
component_2
in
conjunction
with
requirement_1
deep

technology_11
think
technology_5
technology_12
to
build
operate
and
pattern_6
analytic
component_8
reference
architecture
for
requirement_1
with
technology_5
kafka®
after
you
understand
the
requirement_1
development
lifecycle
let’s
look
at
a
reference
architecture
for
build
operate
and
pattern_7
analytic
component_9
with
technology_3
the
essence
of
this
architecture
be
that
it
us
technology_3
a
an
pattern_8
between
the
various
connector_data_1
component_19
from
which
feature
connector_data_1
be
connector_16
the
component_8
build
environment
where
the
component_8
be
fit
and
the
production
component_12
that
serve
prediction
feature
connector_data_1
be
connector_17
into
technology_3
from
the
various
component_20
and
component_21
that
component_22
it
this
connector_data_1
be
use
to
build
component_8
the
environment
for
this
will
vary
base
on
the
skill
and
prefer
toolset
of
the
team
the
component_8
build
could
be
a
connector_data_1
requirement_17
a
requirement_8
environment
technology_5
technology_12
or
technology_13
or
a
quality_attribute_8
component_23
run
technology_7
script
the
component_8
can
be
publish
where
the
production
component_24
that
connector_18
the
same
component_8
parameter
can
apply
it
to
incoming
example
perhaps
use
technology_3
connector_3
to
help
index
the
feature
connector_data_1
for
easy
usage
on
demand
the
production
component_24
can
either
connector_19
connector_data_1
from
technology_3
a
a
pipeline
or
even
be
a
technology_3
connector_3
component_12
itself
technology_3
become
the
central
nervous
component_10
in
the
ml
architecture
to
fee
build
apply
and
pattern_6
analytic
component_8
this
establish
huge
benefit
connector_data_1
pipeline
be
simplify
build
analytic
be
decouple
from
component_25
them
usage
of
real
time
or
pattern_4
a
need
analytic
component_9
can
be
quality_attribute_1
in
a
performant
quality_attribute_2
and
mission
critical
environment
in
addition
to
leverage
technology_3
a
a
quality_attribute_2
quality_attribute_10
pattern_9
pattern_10
you
can
also
optional
component_26
of
the
technology_3
ecosystem
technology_3
connector_20
technology_3
connector_1
confluent
pattern_11
pattern_12
confluent
schema
registry
or
ksql
instead
of
rely
on
the
technology_3
component_27
and
component_28
apis
the
next
two
section
explain
how
to
leverage
kafka’s
connector_3
component_11
to
easily
quality_attribute_1
analytic
component_9
to
production
example
for
requirement_1
development
lifecycle
let’s
now
dive
into
a
more
specific
example
of
an
ml
architecture
design
around
technology_3
in
green
you
see
the
component_26
to
build
and
validate
an
analytic
component_8
in
orange
you
see
the
connector_2
component_2
where
the
analytic
component_8
be
quality_attribute_1
infer
to

and
pattern_6
connector_data_1
component_29
connector_15
connector_data_5
continuously
the
requirement_11
component_2
connector_21
this
connector_data_1
either
in
pattern_4
or
real
time
it
us
requirement_1
algorithm
to
build
analytic
component_8
the
analytic
component_9
be
quality_attribute_1
to
the
connector_2
component_2
the
connector_2
component_2
apply
the
analytic
component_9
to
to
infer
a
connector_data_6
i
e
do
a
prediction
the
outcome
be
connector_22
to
a
connector_data_1
component_28
in
this
example
we
separate
component_8
train
from
component_8
inference
which
be
the
typical
setup
i
have
see
in
most
of
today’s
requirement_1
project
component_8
train
requirement_8
be
ingest
into
a
technology_13
cluster
via
technology_3
h2o
requirement_18
be
use
to
analyze
the
historical
connector_data_1
in
technology_13
to
build
a
neural
requirement_14
the
connector_data_1
scientist
can
use
it
prefer
interface—r
technology_7
technology_14
web
ui
notebook
etc
—for
this
the
component_8
build
and
validation
run
on
the
technology_13
cluster
component_7
the
connector_data_1
at
rest
the
connector_data_6
be
a
train
analytic
component_8
generate
a
technology_6
by
h2o
requirement_18
this
be
ready
for
production
deployment
component_8
inference
the
neural
requirement_14
be
then
quality_attribute_1
to
a
technology_3
connector_3
component_12
the
technology_3
connector_3
component_12
can
run
anywhere
whether
it’s
a
standalone
technology_6
component_7
a
technology_15
container
or
a
technology_16
cluster
here
it
be
apply
to
every
in
real
time
to
do
a
prediction
technology_3
connector_3
leverage
the
technology_3
cluster
to
provide
quality_attribute_2
mission
critical
of
analytic
component_9
and
performant
component_8
inference
online
component_8
train
instead
of
separate
component_8
train
and
component_8
inference
we
can
also
build
a
complete
infrastructure
for
online
component_8
train
many
tech
giant
linkedin
do
this
in
the
past
leverage
technology_5
kafka®
for
component_8
input
train
inference
and
output
this
alternative
have
several
requirement_19
off
most
traditional
requirement_20
use
the
first
approach
which
be
appropriate
for
most
use
requirement_5
today
component_8
pattern_7
and
alerting
deployment
of
an
analytic
component_8
to
production
be
the
first
step
pattern_7
the
component_8
for
quality_attribute_9
score
slas
and
other
metric
and
provide
automate
alerting
in
real
time
be
a
important
the
metric
be
feed
back
to
the
requirement_1
technology_17
through
technology_3
to
improve
or
replace
the
component_8
development
of
an
analytic
component_8
with
h2o
requirement_18
the
follow
show
an
example
of
build
an
analytic
component_8
with
h2o
an
open_source
requirement_1
technology_11
which
leverage
other
technology_11
technology_5
technology_12
or
technology_18
under
the
hood
the
connector_data_1
scientist
can
use
his
or
her
favorite
programming
technology_19
r
technology_7
or
technology_14
the
great
benefit
be
the
output
of
the
h2o
component_30
technology_6

the
generate
typically
perform
very
well
and
can
be
quality_attribute_3
easily
use
technology_3
connector_1
here
be
some
screenshots
of
h2o
requirement_18
flow
web
ui
notebook
and
alternative
r
to
build
an
analytic
component_8
build
an
analytic
component_8
with
h2o
flow
web
ui
build
an
analytic
component_8
with
h2o’s
r
technology_20
the
output
be
an
analytic
component_8
generate
a
technology_6

this
can
be
use
without
re
development
in
mission
critical
production
environment
therefore
you
do
not
have
to
think
about
how
to
“migrate”
a
technology_7
or
r
component_8
to
a
production
component_10
base
on
the
technology_6
component_2
while
this
example
us
h2o’s
capability
to
generate
technology_6

you
can
do
similar
thing
with
other
technology_11
technology_18
technology_5
mxnet
or
technology_21
deployment
of
an
analytic
component_8
with
technology_5
kafka’s
connector_3
component_11
deployment
of
the
analytic
component_8
be
easy
with
technology_3
connector_1
simply
the
component_8
to
your
connector_1
component_7
application—which
recall
be
a
technology_6
application—to
apply
it
on
incoming

technology_5
kafka’s
connector_3
component_11
to
embed
h2o
requirement_18
component_8
into
technology_3
connector_3
since
the
technology_3
connector_3
component_12
leverage
all
technology_3
feature
under
the
hood
this
component_12
be
ready
for
quality_attribute_3
and
mission
critical
usage
there
be
no
additional
need
to
tweak
the
component_8
because
of
production
consideration
you
can
find
the
run
example
on
technology_22
technology_23
technology_22
technology_24
kaiwaehner
technology_3
connector_1
component_16

example
simply
clone
the
project
run
the
technology_25
build
and
see
how
the
h2o
component_8
be
use
in
the
technology_3
connector_3
component_12
this
example
will
continue
to
quality_attribute_11
with
more
sophisticate
example
and
use
requirement_5
leverage
h2o
and
other
requirement_1
technology_11
technology_18
or
technology_21
in
the
roadmap
for
late
such
an
implementation
of
apply
requirement_1
to
connector_1
component_7
can
easily
be
quality_attribute_12
into
any
automate
continuous
requirement_21
workflow
use
your
favorite
technology_17
technology_26
for
ci
cd
environment
such
a
technology_25
gradle
technology_27
puppet
or
technology_28
connector_8
of
an
analytic
component_8
between
train
and
inference
use
open
technology_29
a
discuss
early
already
you
need
to
use
an
appropriate
technology_10
for
build
your
analytic
component_8
otherwise
you
will
not
be
able
to
quality_attribute_1
it
into
production
in
a
mission
critical
performant
and
quality_attribute_2
way
some
alternative
to
connector_23
and
update
component_9
between
connector_data_1
scientist
to
develop
and
improve
the
component_8
and
devops
team
to
embed
and
productize
the
component_8
requirement_22
component_8
directly
quality_attribute_1
a
component_8
to
the
connector_1
component_7
component_30
quality_attribute_1
a
technology_7
component_8
via
technology_30
in
a
technology_6
component_12
generate

independent
of
the
technology_19
use
to
build
the
component_8
a
generate
binary
or
component_1
can
be
quality_attribute_1
to
the
connector_1
component_7
component_30
which
be
optimize
for
requirement_16
for
example
the
component_8
be
generate
technology_6
bytecode
even
though
the
connector_data_1
scientist
use
r
or
technology_7
to
train
it
external
component_23
connector_data_7
to
an
external
requirement_11
component_23
via
connector_data_8
connector_24
use
requirement_11
technology_17
sa
matlab
knime
or
h2o
this
be
typically
do
via
a
pattern_11

pmml
predictive
component_8
markup
technology_19
an
old
connector_data_9
technology_29
with
several
limitation
and
drawback
but
support
in
some
requirement_11
technology_17
pfa
quality_attribute_13
technology_31
for
requirement_11
a
modern
technology_29
include
preprocessing
in
addition
to
the
component_8
pfa
leverage
technology_32
and
technology_5
avro
and
support
hadrian
it
be
not
yet
support
out
of
the
component_31
in
most
requirement_11
technology_17
there
be
various
requirement_19
off
between
these
alternative
for
instance
use
a
technology_29
pfa
create
additional
effort
and
restriction
but

independence
and
quality_attribute_14
from
technology_3
perspective
where
you
typically
have
mission
critical
deployment
with
high
volume
the
prefer
option
today
be
often
generate
technology_6

which
be
performant
quality_attribute_3
well
and
can
easily
embed
into
a
technology_3
connector_3
component_12
it
also
avoid
connector_25
with
an
external
pattern_11
component_23
for
component_8
inference
conclusion
use
a
connector_2
component_2
to
quality_attribute_1
analytic
component_9
into
mission
critical
deployment
requirement_1
can
create
requirement_12
in
any
requirement_4
also
technology_5
kafka®
be
rapidly
become
the
central
nervous
component_10
in
many
requirement_9
requirement_1
be
a
fantastic
use
requirement_5
for
it
you
can
leverage
technology_3
for
inference
of
the
analytic
component_8
in
real
time
pattern_7
and
alerting
online
train
of
component_9
ingestion
into
the
pattern_4
pattern_13
requirement_11
cluster
to
train
analytic
component_9
there
you
have
see
some
example
in
this
how
to
leverage
technology_5
kafka®
and
it
connector_3
component_11
to
build
a
quality_attribute_2
performant
mission
critical
infrastructure
for
apply
and
pattern_7
analytic
component_8
a
live
demo
of
this
example
can
be
find
on
confluent’s
youtube
pattern_14
a
a
follow
up
to
this

we
will
present
a
demonstration
of
how
to
realize
“online
component_8
training”
with
technology_5
technology_3
kafka’s
connector_3
technology_33
and
technology_5
technology_34
to
build
and
quality_attribute_1
an
online
logistic
regression
algorithm
here
we
will
both
train
and
apply
our
requirement_1
component_9
in
real
time
stay
tune
about
technology_5
kafka’s
connector_3
component_11
if
you
have
enjoy
this

continue
with
the
follow
resource
to
more
about
technology_5
kafka’s
connector_3
technology_33
connector_4
start
with
the
technology_3
connector_3
component_11
to
build
your
own
real
time
component_4
and
pattern_2
walk
through
our
confluent
for
the
technology_3
connector_3
component_11
with
technology_15
and
play
with
our
confluent
demo
component_12
do
you
this

connector_23
it
nowsubscribe
to
the
confluent
blogsubscribemore

thiswhat’s
in
technology_5
technology_3


0i’m
proud
to
announce
the
release
of
technology_5
technology_3



on
behalf
of
the
technology_5
kafka®

the



release
contain
many
feature
and
improvement
this
will
highlightreadwhat’s
in
technology_5
technology_3


0on
behalf
of
the
technology_5
kafka®

it
be
my
pleasure
to
announce
the
release
of
technology_5
technology_3



the



release
contain
many
improvement
and
feature
we’ll
highlightread4
key
design
principle
and
guarantee
of
connector_2
databasesclassic
relational
component_21
requirement_23
component_32
technology_35
quality_attribute_10
and
organize
connector_data_1
in
a
relatively
storage
pattern_13
when
connector_26
be
connector_data_8
they
compute
on
the
component_15
connector_data_1
and
then
resultsreadproductconfluent
platformconnectorsksqldbstream
governanceconfluent
hubsubscriptionprofessional
servicestrainingcustomerscloudconfluent
cloudsupportsign
uplog
incloud
faqsolutionsfinancial
servicesinsuranceretail
and
ecommerceautomotivegovernmentgamingcommunication
component_25
providerstechnologymanufacturingfraud
detectioncustomer
360messaging
modernizationstreaming
etlevent
drive
microservicesmainframe
offloadsiem
optimizationhybrid
and
multicloudinternet
of
thingsdata
warehousedevelopersconfluent
developerwhat
be
technology_3
resourceseventsonline
talksmeetupskafka
summittutorialsdocsblogaboutinvestor
relationscompanycareerspartnersnewscontacttrust
and
securityterms
&
condition
|
privacy
requirement_24
|
do
not
sell
my
connector_data_2
|
modern
slavery
requirement_24
|
settingscopyright
©
confluent
inc


technology_5
technology_5
technology_3
technology_3
and
associate
open_source
project
name
be
trademark
of
the
technology_5
foundation
