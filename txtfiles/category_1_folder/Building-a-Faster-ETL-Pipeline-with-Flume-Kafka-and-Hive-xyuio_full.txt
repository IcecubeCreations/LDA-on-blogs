build
a
fast
technology_1
pipeline
with
technology_2
technology_3
and
technology_4
–
xyu
io
skip
to
content
xyu
io
and
widget
twitter
technology_5
linkedin
xiao
recent
visualize
elasticsearch
score
balance
technology_3
on
jbod
requirement_1
analysis
with
technology_4
build
a
fast
technology_1
pipeline
with
technology_2
technology_3
and
technology_4
technology_6
requirement_2
with
hhvm
build
a
fast
technology_1
pipeline
with
technology_2
technology_3
and
technology_4
at
technology_6
technology_7
we
component_1
a
lot
of
include
some
some
that
be
pattern_1
and
connector_1
asynchronously
sometimes
day
late
but
when
query
this
connector_data_1
we
be
likely
to
care
more
about
when
the
occur
rather
then
when
it
be
connector_1
to
our
component_2
this
we
component_3
our
connector_data_1
in
technology_4
component_4
by
when
the
occur
rather
then
when
they
be
ingest
ingestion
take
one
the
initial
design
of
our
technology_1
pipeline
look
something
this
raw
requirement_1
be
aggregate
and
component_1
by
a
custom
requirement_3
which
a
both
an
aggregator
for
high
level
stats
a
well
a
emitter
of
raw
requirement_1
into
the
various
technology_3
topic
a
technology_8
agent
then
u
the
technology_3
component_5
to
connector_2
from
the
appropriate
topic
the
technology_8
morphline
pattern_2
be
then
use
to
do
a
series
of
transformation
include
annotate
what
type
of
the
requirement_1
line
represent
be
buffer
via
a
memory
pattern_3
and
connector_1
to
the
kite
dataset
connector_3
kite
then
handle
connector_4
with
technology_4
and
persist
the
in
technology_9
with
this
technology_1
connector_data_1
be
quality_attribute_1
for
query
almost
immediately
and
be
component_3
in
close
to
their
final
state
within
technology_4
component_6
a
an
bonus
because
we
be
use
kite
datasets
and
the
accompany
technology_8
connector_3
technology_4
component_4
be
handle
for
u
automatically
however
due
to
the
pattern_4
nature
of
our
collection
we
end
up
have
to
connector_5
to
multiple
component_4
at
the
same
time
which
connector_data_2
in
the
formation
of
many
small
to
connector_6
around
this
we
simply
run
a
compaction
with
technology_10
after
some
date
cutoff
for
this
component_1
work
very
well
until
a
couple
day
ago
when
some
requirement_4
issue
and
some
bug
cause
oom
error
which
connector_data_3
in
our
technology_8
agent
sporadically
loose
all
buffer
in
memory
fortunately
we
persist
our
technology_3
topic
of
important
requirement_1
for
about
a
month
so
we
can
replay
it
then
merge
and
dedupe
the
connector_data_1
in
technology_4
not
only
that
but
we
have
a
lot
of
component_7
in
technology_11
manager
so
we
can
our
backfilling
technology_8
agent
role
to
a
bunch
of
them
and
we’ll
be
do
in
no
time
#winning
ala
the
dream
be
not
to
be
because
we
must
connector_5
to
many
component_4
at
once
connector_7
to
the
kite
dataset
connector_3
with
so
many
technology_8
agent
cause
our
technology_4
metastore
to
become
unstable
not
only
limit
the
rate
of
our
ingestion
but
also
cause
numerous
query
failure
for
our
growth
explorer
sure
we
could
have
slow
ingestion
down
but
be
snake
people
that
won’t
do
after
all
we
have
the
technology_12
we
can
make
our
etl…
better…
stronger…
fast
with
what
we
have
about
our
first
technology_1
pipeline
we’ve
decide
to
rewrite
it
but
this
time
with
an
eye
on
quality_attribute_2
and
ingestion
requirement_2
a
well
we
keep
our
custom
requirement_1
requirement_3
aggregator
technology_3
emitter
a
it’s
do
an
admirable
in
addition
we
decide
to
stick
with
technology_8
and
it
technology_3
component_5
for
our
technology_1
component_1
because
of
it
ease
of
use
and
quality_attribute_3
we
do
not
the
fact
that
when
a
technology_8
agent
crash
it
would
drop
in
the
memory
pattern_3
on
the
floor
so
to
make
our
component_1
more
quality_attribute_4
we
opt
to
use
flume’s
technology_3
pattern_3
instead
this
allow
u
to
multiplex
and
publish
component_8
after
they
have
be
transform
by
the
morphline
pattern_2
to
technology_3
so
that
they
be
persist
when
the
technology_8
agent
die
do
this
come
at
the
expense
of
possible
duplicate
be
emit
when
anomaly
happen
however
we
figure
it’s
far
easy
to
dedupe
with
technology_4
connector_8
then
it
be
to
recreate
miss
connector_data_1
to
make
connector_9
more
performant
we
reconfigured
morphline
to
convert
our
to
technology_13
component_8
with
our
predefined
schema
and
then
serialize
and
compress
those
component_8
make
them
ready
for
connector_5
once
the
raw
technology_13
byte
have
be
generate
we
multiplex
it
into
the
proper
technology_3
pattern_3
finally
we
use
the
technology_9
connector_3
to
connector_2
these
from
technology_3
in
pattern_1
and
connector_5
the
raw
technology_13
byte
to
technology_9
component_4
by
when
the
component_9
be
connector_5
we
do
this
directly
without
touch
technology_4
or
it
metastore
by
partitioning
on
when
the
component_9
be
connector_5
we
can
ensure
we
only
connector_5
to
a
single
component_4
at
a
time
this
connector_data_2
in
few
and
large
which
be
not
only
more
performant
but
it
also
give
u
the
ability
to
when
component_4
can
be
consider
complete
with
this
knowledge
we
can
now
have
technology_10
that
merge
dedupe
and
compact
from
the
intermediate
component_6
component_4
by
the
time
an
be
component_9
into
another
component_6
component_4
by
when
occur
for
optimal
query
requirement_2
show
me
the
so
what
do
this
look
here’s
a
simplify
technology_2
conf
example
#
#
setup
some
name
#
agent
component_5
=
sr
technology_3
agent
pattern_3
=
ch
technology_3
type1
ch
technology_3
type2
agent
connector_3
=
sk
technology_9
type1
sk
technology_9
type2
#
#
configure
same
technology_3
component_5
for
all
pattern_3
#
agent
component_5
sr
technology_3
pattern_3
=
ch
technology_3
type1
ch
technology_3
type2
agent
component_5
sr
technology_3
type
=
technology_14
technology_2
component_5
technology_3
kafkasource
agent
component_5
sr
technology_3
zookeeperconnect
=
host1
port
host2
port
host3
port
path
agent
component_5
sr
technology_3
=
flume_source_20150712
agent
component_5
sr
technology_3
topic
=
technology_3
topic
#
connector_10
in
pattern_1
of
or
every
second
agent
component_5
sr
technology_3
batchsize
=
agent
component_5
sr
technology_3
batchdurationmillis
=
#
connector_11
from
start
of
topic
agent
component_5
sr
technology_3
technology_3
auto
offset
reset
=
small
#
#
configure
pattern_2
#
agent
component_5
sr
technology_3
pattern_2
=
in
morphline
technology_1
in
component_10
set
agent
component_5
sr
technology_3
pattern_2
in
morphline
technology_1
type
=
technology_14
technology_2
connector_3
technology_15
morphline
morphlineinterceptor$builder
agent
component_5
sr
technology_3
pattern_2
in
morphline
technology_1
morphlinefile
=
path
to
morphline
conf
agent
component_5
sr
technology_3
pattern_2
in
morphline
technology_1
morphlineid
=
morphline_id
agent
component_5
sr
technology_3
pattern_2
in
component_10
set
type
=
component_10
agent
component_5
sr
technology_3
pattern_2
in
component_10
set
useip
=
false
agent
component_5
sr
technology_3
pattern_2
in
component_10
set
hostheader
=
flume_host
#
#
multiplex
our
component_8
into
pattern_3
base
on
the
requirement_5
of
`eventmarker`
which
come
from
morphline
#
agent
component_5
sr
technology_3
selector
type
=
multiplexing
agent
component_5
sr
technology_3
selector
=
eventmarker
agent
component_5
sr
technology_3
selector
default
=
ch
technology_3
type1
agent
component_5
sr
technology_3
selector
connector_data_4
type1
=
ch
technology_3
type1
agent
component_5
sr
technology_3
selector
connector_data_4
type2
=
ch
technology_3
type2
agent
component_5
sr
technology_3
selector
connector_data_4
type3
=
ch
technology_3
type1
ch
technology_3
type2
#
#
configure
the
pattern_3
we
multiplexed
into
#
agent
pattern_3
ch
technology_3
type1
type
=
technology_14
technology_2
pattern_3
technology_3
kafkachannel
agent
pattern_3
ch
technology_3
type1
brokerlist
=
host1
port
host2
port
host3
port
agent
pattern_3
ch
technology_3
type1
zookeeperconnect
=
host1
port
host2
port
host3
port
path
agent
pattern_3
ch
technology_3
type1
=
flume_channel_20150712
agent
pattern_3
ch
technology_3
type1
topic
=
technology_3
topic
technology_2
type1
agent
pattern_3
ch
technology_3
type2
type
=
technology_14
technology_2
pattern_3
technology_3
kafkachannel
agent
pattern_3
ch
technology_3
type2
brokerlist
=
host1
port
host2
port
host3
port
agent
pattern_3
ch
technology_3
type2
zookeeperconnect
=
host1
port
host2
port
host3
port
path
agent
pattern_3
ch
technology_3
type2
=
flume_channel_20150712
agent
pattern_3
ch
technology_3
type2
topic
=
technology_3
topic
technology_2
type2
#
#
configure
connector_3
we
connector_2
from
technology_3
in
pattern_1
and
connector_5
large
into
technology_9
#
agent
connector_3
sk
technology_9
type1
pattern_3
=
ch
technology_3
type1
agent
connector_3
sk
technology_9
type1
type
=
technology_9
agent
connector_3
sk
technology_9
type1
technology_9
path
=
technology_9
path
to
component_11
etl_type1
record_ymdh=%y%m%d%h
#
prefix
with
the
technology_8
agent&#039
s
hostname
so
we
can
run
multiple
agent
without
collision
agent
connector_3
sk
technology_9
type1
technology_9
fileprefix
=
%{flume_host}
#
technology_4
need
to
end
in
avro
agent
connector_3
sk
technology_9
type1
technology_9
filesuffix
=
avro
#
roll
in
technology_9
every
min
or
at
255mb
don&#039
t
roll
base
on
number
of
component_8
#
we
roll
at
255mb
because
our
block
size
be
128mb
we
want
full
block
without
go
over
agent
connector_3
sk
technology_9
type1
technology_9
rollinterval
=
agent
connector_3
sk
technology_9
type1
technology_9
rollsize
=
agent
connector_3
sk
technology_9
type1
technology_9
rollcount
=
#
connector_5
to
technology_9
in
pattern_1
of
component_8
agent
connector_3
sk
technology_9
type1
technology_9
batchsize
=
#
we
already
serialize
and
encode
the
component_9
into
technology_13
in
morphline
so
connector_5
the
byte
agent
connector_3
sk
technology_9
type1
technology_9
filetype
=
datastream
#
give
u
a
high
timeout
because
we
be
connector_12
in
pattern_1
agent
connector_3
sk
technology_9
type1
technology_9
calltimeout
=
#
use
current
time
in
utc
for
the
requirement_5
of
`record_ymdh=%y%m%d%h`
above
agent
connector_3
sk
technology_9
type1
technology_9
timezone
=
utc
agent
connector_3
sk
technology_9
type1
technology_9
uselocaltimestamp
=
true
#
our
component_9
be
serialize
via
technology_13
agent
connector_3
sk
technology_9
type1
serializer
=
technology_14
technology_2
connector_3
technology_9
avroeventserializer$builder
agent
connector_3
sk
technology_9
type2
pattern_3
=
ch
technology_3
type2
agent
connector_3
sk
technology_9
type2
type
=
technology_9
agent
connector_3
sk
technology_9
type2
technology_9
path
=
technology_9
path
to
component_11
etl_type2
record_ymdh=%y%m%d%h
agent
connector_3
sk
technology_9
type2
technology_9
fileprefix
=
%{flume_host}
agent
connector_3
sk
technology_9
type2
technology_9
filesuffix
=
avro
agent
connector_3
sk
technology_9
type2
technology_9
rollinterval
=
agent
connector_3
sk
technology_9
type2
technology_9
rollsize
=
agent
connector_3
sk
technology_9
type2
technology_9
rollcount
=
agent
connector_3
sk
technology_9
type2
technology_9
batchsize
=
agent
connector_3
sk
technology_9
type2
technology_9
filetype
=
datastream
agent
connector_3
sk
technology_9
type2
technology_9
calltimeout
=
agent
connector_3
sk
technology_9
type2
technology_9
timezone
=
utc
agent
connector_3
sk
technology_9
type2
technology_9
uselocaltimestamp
=
true
agent
connector_3
sk
technology_9
type2
serializer
=
technology_14
technology_2
connector_3
technology_9
avroeventserializer$builder
the
most
important
part
of
the
above
be
that
we
set
the
technology_9
connector_3
use
the
technology_13
serializer
and
instruct
that
it
should
simply
connector_5
the
raw
byte
a
we
ve
already
serialize
the
technology_13
component_9
and
compress
it
with
morphline
speak
of
which
here
s
our
example
morphline
conf
morphlines
{
morphline_id
#
the
kite
technology_16
and
any
custom
libs
you
have
and
need
importcommands
kitesdk
**
technology_7
a8c
**
command
#
each
command
connector_13
the
output
component_9
of
the
previous
command
#
and
pip
another
component_9
downstream
{
#
requirement_6
input
attachment
and
emit
a
component_9
for
each
input
line
readline
{
charset
utf
}
}
{
#
more
command
for
your
technology_1
component_1
}
{
#
say
we
set
a
name
`eventmarker`
somewhere
above
to
indicate
the
#
type
of
component_9
this
be
and
we
have
a
different
schema
if
{
condition
{
equal
{
eventmarker
type1
}
}
then
{
#
set
the
schema
for
the
technology_8
technology_9
connector_3
setvalues
{
technology_2
avro
schema
url
path
to
schema
type1
avsc
}
}
{
#
convert
this
to
an
technology_13
component_9
accord
to
schema
toavro
{
schemafile
path
to
schema
type1
avsc
}
}
else
{
setvalues
{
technology_2
avro
schema
url
path
to
schema
type2
avsc
}
}
{
toavro
{
schemafile
path
to
schema
type2
avsc
}
}
}
}
{
#
serialize
the
technology_13
component_9
into
a
byte
compress
with
snappy
writeavrotobytearray
{
technology_17
containerlessbinary
codec
snappy
}
}
}
with
these
configs
technology_8
will
connector_5
compress
technology_13
directly
to
technology_9
but
we
will
need
to
technology_4
about
where
to
look
so
we
need
to
create
the
component_6
in
technology_4
component_6
name
create
component_6
if
not
exist
etl_type1
we
need
to
specify
how
we
be
partitioning
this
component_6
with
the
technology_8
technology_9
connector_3
component_4
by
record_ymdh
be
connector_5
in
avro
row
technology_17
serde
&#039
technology_14
technology_18
technology_4
serde2
avro
avroserde&#039
component_3
a
inputformat
&#039
technology_14
technology_18
technology_4
ql
io
avro
avrocontainerinputformat&#039
outputformat
&#039
technology_14
technology_18
technology_4
ql
io
avro
avrocontaineroutputformat&#039
we
be
connector_12
to
this
dir
in
technology_9
from
technology_8
location
&#039
technology_9
path
to
component_11
etl_type1&#039
we
also
component_3
the
technology_13
schema
in
a
hide
dir
on
technology_9
for
convenience
tblproperties
&#039
avro
schema
url&#039
=&#039
technology_9
path
to
component_11
etl_type1
schema
type1
avsc&#039
for
convenience
i
also
component_3
the
technology_13
schema
for
the
component_6
in
the
schema
directory
on
technology_9
but
that
schema
can
really
be
anywhere
readable
by
technology_4
of
a
we
ingest
connector_data_1
the
technology_8
technology_9
connector_3
with
start
create
directory
a
k
a
component_4
in
technology_9
but
technology_4
will
nothing
about
them
so
you
will
need
to
technology_4
repair
it
metastore
by
scan
technology_9
before
you
can
query
for
connector_data_1
msck
repair
component_6
etl_type1
select
*
from
etl_type1
where
what’s
next
we
have
not
really
connector_14
this
pipeline
to
see
where
the
limit
be
however
a
i
connector_5
this
we
be
on
track
to
ingest
a
month
of
connector_data_1
in
le
then
hour
in
addition
quality_attribute_5
this
pipeline
by
simply
spin
up
more
technology_8
agent
have
thus
far
be
linear
the
one
down
side
be
that
for
the
most
up
to
date
connector_data_5
we
will
now
need
to
look
at
separate
technology_4
component_12
with
different
component_4
strategy
make
connector_8
a
bit
more
complicate
we
have
in
effect
make
what
i
to
connector_data_6
the
“iota
architecture”
—
a
component_13
that’s
of
the
way
to
a
true
lambda
architecture
we
currently
have
a
component_13
that
emit
a
connector_15
of
that
can
be
connector_11
in
pattern_1
or
by
a
connector_15
processor
but
we
only
have
a
pattern_1
component_1
in
place
to
allow
for
performant
connector_8
on
“archival”
connector_data_1
perhaps
someday
we’ll
connector_6
the
other
in
place
for
our
growth
explorer
to
easily
connector_6
a
unify
pattern_5
connector_16
click
to
connector_16
on
twitter
open
in
window
click
to
connector_16
on
linkedin
open
in
window
click
to
connector_16
on
open
in
window
relate
on
27author
xyucategories
big
datatags
technology_11
technology_18
technology_3
leave
a
connector_data_7
cancel
connector_data_7
navigation
previous
previous
technology_6
requirement_2
with
hhvmnext
next
requirement_1
analysis
with
technology_4
proudly
powered
by
technology_6
