real
time
connector_1
component_1
in
requirement_1
component_2
the
digital
transformation
people
advisory
consultancy
talent
late

executive
briefing
book
summary
lead
digital
transformation
podcast
series
news
in
digital
archive
the
requirement_2
for
digital
transformation
strategy
&
innovation
people
&
connector_2
delivery
requirement_3
engagement
enabling
technology_1
connector_data_1
&
requirement_4
cyber
quality_attribute_1
join
login
search
search
knowledge
base
search
component_3
directory
go
advisory
consultancy
talent
late

executive
briefing
book
summary
lead
digital
transformation
podcast
series
news
in
digital
archive
the
requirement_2
for
digital
transformation
strategy
&
innovation
people
&
connector_2
delivery
requirement_3
engagement
enabling
technology_1
connector_data_1
&
requirement_4
cyber
quality_attribute_1
join
login
search
search
knowledge
base
search
component_3
directory
real
time
connector_1
component_1
in
requirement_1
component_2


by
birendra
kumar
sahu
firsthive
cto
&
vice
president
firsthive
more
about
birendra
kumar
sahu
join
u
for
requirement_5
&
quality
resource
to
help
you
and
your
team
succeed
in
digital
transformation
join
u
a
we
the
technology_2
pattern_1
component_1
component_4
have
quality_attribute_2
and
mature
over
the
past
few
year
for
excellent
offline
connector_data_1
component_1
component_2
for
requirement_1
technology_2
be
a
high
quality_attribute_3
component_4
which
can
crunch
a
huge
volume
of
connector_data_1
use
a
quality_attribute_4
parallel
component_1
paradigm
connector_3
mapreduce
but
there
be
many
use
requirement_2
across
various
domain
which
require
real
time
near
real
time
connector_4
on
requirement_1
for
fast
decision
make
technology_2
be
not
suitable
for
those
use
requirement_2
credit
card
fraud
requirement_4
requirement_5
fault
prediction
from
sensor
connector_data_1
quality_attribute_1
threat
prediction
and
so
forth
need
to
component_1
real
time
connector_data_1
connector_1
on
the
fly
to
predict
if
a
give
transaction
be
a
fraud
if
the
component_4
be
develop
a
fault
or
if
there
be
a
quality_attribute_1
threat
in
the
requirement_5
if
decision
such
a
these
be
not
take
in
real
time
the
opportunity
to
mitigate
the
damage
be
lose
real
time
component_5
perform
requirement_4
on
short
time
window
i
e
correlate
and
predict
connector_5
generate
for
the
last
few
minute
now
for
quality_attribute_5
prediction
capability
realtime
component_5
often
leverage
pattern_1
component_1
component_5
such
a
technology_2
the
heart
of
any
prediction
component_4
be
the
component_6
there
be
various
requirement_6
algorithm
quality_attribute_6
for
different
type
of
prediction
component_4
any
prediction
component_4
will
have
high
probability
of
quality_attribute_7
if
the
component_6
be
build
use
quality_attribute_5
train
sample
this
component_6
build
phase
can
be
do
offline
for
instance
a
credit
card
fraud
prediction
component_4
could
leverage
a
component_6
build
use
previous
credit
card
transaction
connector_data_1
over
a
period
of
time
imagine
a
credit
card
component_4
for
a
give
credit
card
technology_3
serve
hundred
of
thousand
of
component_7
have
million
of
transaction
connector_data_1
over
give
period
of
time
we
need
a
technology_2

component_4
to
component_1
them
once
build
this
component_6
can
be
feed
to
a
real
time
component_4
to
find
if
there
be
any
deviation
in
the
real
time
connector_1
connector_data_1
in
this
we
will
showcase
how
real
time
requirement_4
use
requirement_2
can
be
solve
use
popular
open_source
technology_1
to
component_1
real
time
connector_data_1
in
a
fault
tolerant
and
quality_attribute_4
manner
the
major
challenge
here
be
it
need
to
be
always
quality_attribute_6
to
component_1
real
time
feed
it
need
to
be
quality_attribute_8
enough
to
component_1
hundred
of
thousand
of
connector_data_2
per
second
and
it
need
to
support
a
quality_attribute_9
out
quality_attribute_4
architecture
to
component_1
the
connector_1
in
parallel
real
time
connector_data_1
component_1
challenge
real
time
connector_data_1
component_1
challenge
be
very
complex
a
we
all

requirement_1
be
commonly
categorize
into
volume
technology_4
and
variety
of
the
connector_data_1
and
technology_2
component_4
handle
the
volume
and
varity
part
of
it
along
with
the
volume
and
variety
the
real
time
component_4
need
to
handle
the
technology_4
of
the
connector_data_1
a
well
and
handle
the
technology_4
of
requirement_1
be
not
an
easy
connector_data_3
first
the
component_4
should
be
able
to
connector_6
the
connector_data_1
generate
by
real
time
connector_5
come
in
at
a
rate
of
million
of
per
second
second
it
need
to
handle
the
parallel
component_1
of
this
connector_data_1
a
and
when
it
be
be
connector_6
third
it
should
perform
correlation
use
a
complex
component_1
component_8
to
extract
the
meaningful
connector_data_4
from
this
move
connector_1
these
three
step
should
happen
in
a
fault
tolerant
and
quality_attribute_4
way
the
real
time
component_4
should
be
a
low
quality_attribute_10
component_4
so
that
the
computation
can
happen
very
fast
with
near
real
time
connector_4
capability
to
solve
this
complex
real
time
component_1
challenge
we
have
evaluate
two
popular
open_source
technology_1
technology_5
technology_6
technology_7
technology_6
technology_5

design
technology_8
which
be
the
quality_attribute_4
pattern_2
component_4
and
technology_9
technology_7
storm
project
net
which
be
a
quality_attribute_4
connector_1
component_1
component_8
technology_9
and
technology_6
be
the
future
of
connector_1
component_1
and
they
be
already
in
use
at
a
number
of
high
profile
requirement_7
include
groupon
alibaba
the
weather
pattern_3
and
many
more
an
idea
bear
inside
of
twitter
technology_9
be
a
“distributed
real
time
computation
system”
meanwhile
technology_6
be
a
pattern_2
component_4
develop
at
linkedin
to
serve
a
the
foundation
for
their
activity
connector_1
and
the
connector_data_1
component_1
pipeline
behind
it
with
technology_9
and
technology_6
you
can
conduct
connector_1
component_1
at
linear
quality_attribute_9
assure
that
every
connector_data_2
be
component_1
in
a
real
time
quality_attribute_11
manner
technology_9
and
technology_6
can
handle
connector_data_1
technology_4
of
ten
of
thousand
of
connector_data_5
every
second
connector_1
component_1
solution
such
a
technology_9
and
technology_6
have
catch
the
attention
of
many
requirement_8
due
to
their
superior
approach
to
technology_10
extract
transform
and
load
and
connector_data_1
requirement_9
let’s
take
a
close
look
at
technology_6
and
technology_9
to
how
they
have
achieve
the
parallelism
and
quality_attribute_12
component_1
of
connector_1
connector_data_1
quality_attribute_4
pattern_2
architecture
technology_5
technology_6
technology_5
technology_6
be
the
pattern_2
component_4
originally
develop
at
linkedin
for
component_1
linkedin’s
activity
connector_1
coding_keyword_1
look
at
the
issue
with
the
traditional
pattern_2
component_4
which
cause
people
to
seek
an
alternative
any
pattern_2
component_4
have
three
major
component_9
the
connector_data_2
component_10
the
connector_data_2
component_11
and
the
connector_data_2
pattern_4
connector_data_2
component_12
and
connector_data_2
component_13
use
connector_data_2
component_14
for
pattern_5
inter
component_1
connector_7
any
pattern_2
support
both
point
to
point
a
well
a
pattern_6
connector_7
in
point
to
point
paradigm
the
component_10
of
the
connector_data_5
connector_8
the
connector_data_5
to
a
component_14
there
can
be
multiple
component_13
associate
with
the
component_14
but
only
one
component_11
can
connector_9
a
give
connector_data_2
from
a
component_14
in
pattern_6
paradigm
there
can
be
multiple
component_12
produce
connector_data_5
for
a
give
component_15
connector_3
topic
and
there
can
be
multiple
component_13
subscribe
for
that
topic
each
subscription
connector_10
the
copy
of
each
connector_data_2
connector_11
for
that
topic
this
differ
from
point
topoint
connector_7
where
one
connector_data_2
be
connector_9
by
only
one
component_11
the
connector_data_2
pattern_4
be
the
heart
of
the
whole
pattern_2
component_4
which
bridge
the
gap
between
the
component_10
and
component_11
a
“durable
message”
be
a
connector_data_2
where
the
pattern_4
will
hold
on
to
a
connector_data_2
if
the
pattern_7
be
temporarily
unavailable
so
the
quality_attribute_13
be
define
by
the
relationship
between
a
“topic
subscriber”
and
the
“broker”
quality_attribute_13
be
applicable
only
to
the
pattern_6
paradigm
a
“persistent
message”
be
a
connector_data_2
that
define
the
relationship
between
a
“message
producer”
and
the
“broker”
this
can
be
establish
for
both
point
to
point
and
publish
subscribe
this
have
to
do
with
the
guarantee
delivery
of
the
connector_data_2
by
persist
the
connector_data_2
after
it
have
be
connector_12
from
the
connector_data_2
component_10
both
connector_data_2
quality_attribute_13
and
connector_data_2
persistency
come
with
a
cost
keep
the
state
of
the
connector_data_2
whether
connector_data_2
be
connector_9
or
not
be
a
tricky
problem
the
traditional
pattern_2
pattern_4
keep
track
of
the
component_11
state
it
us
metadata
about
the
connector_data_5
and
connector_13
this
metadata
in
pattern_4
connector_14
metadata
about
billion
of
connector_data_5
create
large
overhead
for
pattern_4
on
top
of
that
the
relational
component_16
for
connector_14
the
connector_data_2
metadata
do
not
quality_attribute_9
very
well
however
the
pattern_4
try
to
keep
the
metadata
size
small
by
delete
connector_data_5
which
be
already
connector_9
the
challenge
problem
arise
about
how
pattern_4
and
component_11
conclude
that
a
give
connector_data_2
be
connector_9
can
a
pattern_4
mark
a
connector_data_2
connector_9
a
and
when
it
put
the
connector_data_2
in
the
requirement_5
for
delivery
what
will
happen
if
the
component_11
be
down
by
the
time
the
connector_data_2
reach
the
component_11
to
solve
this
most
pattern_2
component_5
keep
an
acknowledgement
component_4
when
a
connector_data_2
be
connector_15
by
a
pattern_4
it
be
mark
a
“sent”
and
when
the
component_11
connector_16
the
connector_data_2
and
connector_8
an
acknowledgement
the
pattern_4
mark
it
a
“consumed”
but
what
will
happen
if
the
component_11
actually
connector_9
the
connector_data_2
but
a
requirement_5
failure
occur
before
the
acknowledgement
reach
the
pattern_4
the
pattern_4
will
still
keep
the
state
a
“sent”
not
“consumed”
if
the
pattern_4
resends
the
connector_data_2
the
connector_data_2
will
be
connector_9
twice
the
major
problem
arise
around
the
requirement_10
of
the
pattern_4
now
the
pattern_4
must
keep
track
of
each
and
every
connector_data_2
imagine
the
overhead
of
the
pattern_4
in
requirement_2
when
thousand
of
connector_data_5
be
be
produce
every
second
this
be
a
major
reason
why
the
traditional
pattern_2
component_4
be
not
able
to
quality_attribute_9
beyond
a
certain
limit
the
follow
be
how
technology_6
solve
these
problem
pattern_2
component_4
the
technology_6
way
below
figure
show
how
different
type
of
component_12
can
connector_17
to
different
type
of
component_13
through
the
technology_6
pattern_4
this
happen
fairly
naturally
for
pattern_4
and
component_10
but
component_13
require
particular
support
each
component_11
component_1
belong
to
a
component_11
group
and
each
connector_data_2
be
connector_15
to
exactly
one
component_1
within
every
component_11
group
hence
a
component_11
group
allow
many
component_17
or
component_18
to
logically
act
a
a
single
component_11
the
concept
of
component_11
group
be
very
powerful
and
can
be
use
to
support
the
semantics
of
either
a
component_14
or
topic
a
find
in
technology_11
to
support
component_14
semantics
we
can
put
all
component_13
in
a
single
component_11
group
in
which
requirement_2
each
connector_data_2
will
go
to
a
single
component_11
to
support
topic
semantics
each
component_11
be
put
in
it
own
component_11
group
and
then
all
component_13
will
connector_18
each
connector_data_2
technology_6
have
the

benefit
in
the
requirement_2
of
large
connector_data_1
that
no
matter
how
many
component_13
a
topic
have
a
connector_data_2
be
component_19
only
a
single
time
the
overall
architecture
of
technology_6
be
show
in
below
technology_6
be
quality_attribute_4
in
nature
a
technology_6
cluster
typically
consist
of
multiple
pattern_4
to
balance
load
a
topic
be
divide
into
multiple
component_20
and
each
pattern_4
connector_13
one
or
more
of
those
component_20
multiple
component_12
and
component_13
can
publish
and
connector_19
connector_data_5
at
the
same
time
technology_6
rely
heavily
on
the
component_4
for
connector_14
and
pattern_8
connector_data_2
there
be
a
general
perception
that
“disks
be
slow”
which
make
people
skeptical
that
a
persistent
connector_data_6
can
offer
competitive
requirement_10
in
fact
disk
be
both
much
slow
and
much
fast
than
people
expect
quality_attribute_14
on
how
they
be
use
a
properly
design
disk
connector_data_6
can
often
be
a
fast
a
the
requirement_5
this
suggest
a
design
which
be
very
quality_attribute_15
rather
than
maintain
a
much
a
possible
in
memory
and
flush
to
the
component_4
only
when
necessary
technology_6
invert
that
all
connector_data_1
be
immediately
connector_20
to
a
persistent
requirement_11
on
the
component_4
without
any
connector_data_7
to
flush
the
connector_data_1
in
effect
this
mean
that
it
be
transfer
into
the
kernel’s
component_21
pattern_8
where
the
o
can
flush
it
late
technology_6
have
a
very
quality_attribute_15
storage
layout
each
component_20
of
a
topic
correspond
to
a
logical
requirement_11
physically
a
requirement_11
be
connector_21
a
a
set
of
segment
of
approximately
the
same
size
e
g
1gb
every
time
a
component_10
publish
a
connector_data_2
to
a
component_20
the
pattern_4
simply
append
the
connector_data_2
to
the
last
segment

for
quality_attribute_5
requirement_10
technology_6
flush
the
segment
to
disk
only
after
a
quality_attribute_16
number
of
connector_data_5
have
be
publish
or
a
certain
amount
of
time
have
elapsed
a
connector_data_2
be
only
connector_22
to
the
component_13
after
it
be
flush
unlike
typical
pattern_2
component_4
a
connector_data_2
component_19
in
technology_6
doesn’t
have
an
explicit
connector_data_2
coding_keyword_2
instead
each
connector_data_2
be
connector_23
by
it
logical
offset
in
the
requirement_11
this
avoid
the
overhead
of
maintain
auxiliary
seek
intensive
random
connector_24
index
connector_data_8
that
connector_data_9
the
connector_data_2
coding_keyword_2
to
the
actual
connector_data_2
location
if
the
pattern_2
component_4
be
design
around
this
kind
of
design
of
connector_25
ahead
and
connector_20
behind
how
do
technology_6
support
the
component_11
state
problem
we
define
early
i
e
how
do
technology_6
keep
track
of
which
connector_data_5
be
be
“sent”
or
“consumed”
fact
of
the
matter
be
technology_6
pattern_4
never
keep
track
of
this
in
technology_6
it
be
component_11
which
keep
track
of
the
connector_data_5
it
connector_9
component_11
be
maintain
something
a
watermark
which
tell
which
offset
in
the
requirement_11
segment
be
connector_9
a
component_11
always
connector_16
connector_data_5
from
a
particular
component_20
sequentially
if
the
component_11
acknowledge
a
particular
connector_data_2
offset
it
imply
that
the
component_11
have
connector_12
all
connector_data_5
prior
to
that
offset
in
the
component_20
this
provide
quality_attribute_17
to
the
component_11
to
connector_9
old
connector_data_2
by
lower
the
watermark
typically
component_11
connector_13
the
state
connector_data_4
in
technology_12
which
be
use
for
quality_attribute_4
consensus
component_22
otherwise
component_11
can
maintain
this
watermark
level
in
any
connector_data_1
connector_data_6
it
wish
which
quality_attribute_14
on
the
component_11
for
instance
if
technology_2
be
connector_26
connector_data_5
from
technology_6
it
can
component_19
the
watermark
requirement_12
into
technology_13
along
with
the
architectural
detail
mention
above
technology_6
also
have
many
advance
configuration
such
a
topic
partitioning
automatic
load
balance
and
so
on
more
advance
detail
can
be
find
on
the
technology_6

this
design
of
technology_6
make
it
highly
quality_attribute_8
able
to
component_1
million
of
connector_data_5
per
second
the
component_10
of
real
time
connector_data_1
can
connector_20
connector_data_5
into
technology_6
cluster
and
the
real
time
component_11
can
connector_25
the
connector_data_5
from
technology_6
cluster
for
parallel
component_1
technology_5
technology_9
be
one
such
quality_attribute_4
real
time
parallel
component_1
component_2
develop
by
twitter
with
technology_9
and
technology_6
one
can
conduct
connector_1
component_1
at
linear
quality_attribute_9
assure
that
every
connector_data_2
be
quality_attribute_18
component_1
in
real
time
technology_9
and
technology_6
can
handle
connector_data_1
technology_4
of
ten
of
thousand
of
connector_data_5
every
second
coding_keyword_1
u
now
look
at
the
technology_9
architecture
and
how
it
can
connector_27
to
technology_6
for
real
time
component_1
real
time
connector_data_1
component_1
component_2
technology_9
technology_9
be
a
free
and
open_source
quality_attribute_4
real
time
computation
component_4
technology_9
have
many
use
requirement_2
real
time
requirement_4
online
requirement_6
continuous
computation
quality_attribute_4
pattern_9
technology_10
and
more
technology_9
be
fast
a
benchmark
clock
it
at
over
a
million
tuples
component_1
per
second
per
technology_14
it
be
quality_attribute_8
fault
tolerant
guarantee
your
connector_data_1
will
be
component_1
and
be
easy
to
set
up
and
operate
the
concept
behind
technology_9
be
somewhat
similar
to
technology_2
in
technology_2
cluster
you
run
connector_data_9
reduce

in
technology_9
cluster
there
be
topology
the
core
abstraction
in
technology_9
be
the
“stream”
a
connector_1
be
an
unbounded
sequence
of
tuples
a
show
in
below
technology_9
topology
be
combination
of
spout
and
bolt
spout
be
where
the
connector_data_1
connector_1
be
inject
into
the
topology
bolt
component_1
the
connector_5
that
be
pip
into
it
bolt
can
fee
connector_data_1
from
spout
or
other
bolt
technology_9
take
care
of
parallel
component_1
of
spout
and
bolt
and
move
connector_data_1
around
the
offline
component_1
component_6
build
the
failure
prediction
component_4
be
one
of
the
major
use
requirement_2
in
real
time
requirement_4
a
mention
early
to
detect
failure
in
a
give
connector_1
of
sensor
connector_data_1
we
need
to
first
define
normal
behavior
for
this
we
need
to
build
component_6
around
the
historical
sensor
connector_data_1
to
build
the
component_6
we
can
use
an
offline
pattern_1
component_1
component_4
such
a
technology_2
to
extract
transform
and
load
historical
sensor
component_23
from
requirement_11
for
offline
connector_data_1
component_1
we
use
technology_2
to
connector_28
the
historical
connector_data_1
component_19
in
the
technology_6
cluster
and
then
component_1
the
same
in
technology_2
to
component_20
the
requirement_11
component_23
and
then
try
to
extract
a
correct
sequence
of
for
give
requirement_5
segment
once
this
component_6
be
build
it
be
use
in
the
real
time
component_4
storm
technology_6
for
fault
prediction
a
representative
depiction
of
the
architecture
be
show
in
below
reference
requirement_4
on
big
fast
connector_data_1
use
real
time
connector_1
connector_data_1
component_1
architecture
by
dibyendu
bhattacharya
&
manidipa
mitra
by
birendra
kumar
sahu
firsthive
cto
&
vice
president
firsthive
more
about
birendra
kumar
sahu
browse
by
pattern_3
select
a
pattern_3
requirement_3
engagement
cyber
quality_attribute_1
connector_data_1
&
requirement_4
delivery
enabling
technology_1
lead
publication
people
&
connector_2
strategy
&
innovation
quality_attribute_19
the
requirement_2
for
digital
transformation
think
leader
train

&
workshop
search
everything
you
need
to
about
digital
transformation
subscribe
the
best

news
and
direct
to
your
inbox
connector_25
more

tag
requirement_1
feature
enabling
technology_1
popular
now
the
requirement_2
for
digital
transformation
an
executive
summary
lead
digital
by
george
westerman
didier
bonnet
&
andrew
mcafee
the
requirement_2
for
digital
transformation
the
digital
transformation
pyramid
a
requirement_13
drive
approach
for
corporate
initiative
strategy
&
innovation
target
operate
component_24
&
roadmaps
for
connector_2
delivery
connector_data_1
asset
requirement_14
dam
strategy
&
innovation
the
innovation
requirement_14
theory
evolution
connector_data_9
relate

strategy
&
innovation
growth
innovation
they
come
from
requirement_3
people
&
connector_2
requirement_15
in
the
component_25
of
corporate

a
roundtable
enabling
technology_1
real
time
requirement_1
computing
with
component_26
component_19
and
blink
from
peer
think
leader
and
expert
practitioner
for
the
best
chance
of
success
digital
transformation
be
a
complex
and
difficult
connector_data_3
join
the
for
quality
resource
peer
support
and
expert
help
join
u
requirement_7
about
u
meet
the
team
press
web
privacy
&
requirement_16
term
of
use
connector_27
subscribe
to
news
in
digital
linkedin
twitter
youtube
googleplus
for
supplier
supplier
directory
supplier
directory
feature
&
requirement_17
register
in
supplier
directory
independent
consultant
for
author
meet
the
author
become
an
author
faq
talent
solution
executive
search
interim
consultancy
career
opportunity
candidate
registration
©
copyright
the
digital
transformation
people

