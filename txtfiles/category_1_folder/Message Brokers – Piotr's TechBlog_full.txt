connector_data_1
pattern_1
–
piotr
s
techblog
skip
to
content
home
my
book

&
train
about
me
this
have
be
move
to
the
piotrminkowski
technology_1
piotr
s
techblog
about
technology_2
pattern_2
technology_3
container
and
more
category
connector_data_1
pattern_1
technology_4
pattern_3
with
technology_2
and
kubemq

on


by
piotr
mińkowski
have
you
ever
try
to
run
any
connector_data_1
pattern_1
on
technology_4
kubemq
be
relatively
solution
and
be
not
a
popular
a
competitive
technology_5
technology_6
technology_7
or
technology_8
however
it
have
one
big
advantage
over
them
–
it
be
technology_4
requirement_1
connector_data_1
pattern_1
which
be
quality_attribute_1
there
use
a
single
command
without
prepare
any
additional
template
or
manifest
this
convinced
me
to
take
a
close
look
at
kubemq
continue
connector_1
“kubernetes
pattern_3
with
technology_2
and
kubemq”
→
tag
technology_2
jib
kubemq
technology_4
connector_data_1
pattern_1
connector_data_1
minikube
skaffoldleave
a
technology_7
in
pattern_2
with
micronaut

on

2019august


by
piotr
mińkowski
today
we
be
go
to
build
some
pattern_2
connector_2
with
each
other
asynchronously
through
technology_9
technology_7
topic
we
use
micronaut
technology_10
which
provide
dedicate
technology_11
for
requirement_2
with
technology_7
let’s
take
a
brief
look
at
the
architecture
of
our
sample
component_1
we
have

pattern_2
order
component_2
trip
component_2
driver
component_2
and
technology_12
component_2
the
implementation
of
these
component_3
be
very
quality_attribute_2
all
of
them
have
in
memory
storage
and
connector_3
to
the
same
technology_7
instance
continue
connector_1
“kafka
in
pattern_2
with
micronaut”
→
tag
technology_9
technology_7
quality_attribute_3
trace
connector_data_1
micronaut
pattern_2
zipkinleave
a
technology_13
in
pattern_2
architecture

on

2019may


by
piotr
mińkowski
technology_13
can
be
widely
use
in
pattern_2
architecture
it
be
probably
one
of
the
few
popular
solution
that
be
leverage
by
your
component_4
in
such
many
different
way
quality_attribute_4
on
the
requirement
it
can
act
a
a
primary
component_5
pattern_4
connector_data_1
pattern_1
while
it
be
also
a
key
requirement_3
component_6
we
can
use
it
a
a
configuration
component_7
or
discovery
component_7
in
your
pattern_2
architecture
although
it
be
usually
define
a
an
in
memory
connector_data_2
connector_data_3
we
can
also
run
it
in
persistent
mode
today
i’m
go
to
show
you
some
example
of
use
technology_13
with
pattern_2
build
on
top
of
technology_3
and
technology_3
requirement_4
technology_10
these
component_4
will
connector_2
between
each
other
asynchronously
use
technology_13
pub
sub
use
technology_13
a
a
pattern_4
or
primary
component_5
and
finally
use
technology_13
a
a
configuration
component_7
continue
connector_1
“redis
in
pattern_2
architecture”
→
tag
connector_data_1
pattern_1
pattern_2
technology_13
technology_3
requirement_4
technology_3
requirement_4
config
technology_3
dataleave
a
technology_6
cluster
with
consul
and
vault

on

2018january


by
piotr
mińkowski
almost
two
year
ago
i
connector_4
an
about
technology_6
cluster
technology_6
in
cluster
it
be
one
of
the
first
on
my

and
it’s
really
hard
to
believe
it
have
be
two
year
since
i
start
this

anyway
one
of
the
question
about
the
topic
describe
in
the
mention
inspire
me
to
to
that
subject
one
more
time
that
question
point
to
the
problem
of
an
approach
to
set
up
the
cluster
this
approach
assume
that
we
be
manually
attach
technology_14
to
the
cluster
by
connector_5
the
command
rabbitmqctl
join_cluster
with
cluster
name
a
a
parameter
if
i
remember
correctly
it
be
the
only
one
quality_attribute_5
of
create
cluster
at
that
time
today
we
have
more
choice
what
illustrate
an
evolution
of
technology_6
during
last
two
year
continue
connector_1
“rabbitmq
cluster
with
consul
and
vault”
→
tag
cluster
consul
technology_15
connector_data_1
pattern_1
technology_6
vault2

build
and
test
connector_data_1
drive
pattern_2
use
technology_3
requirement_4
connector_6

on

2018june


by
piotr
mińkowski
technology_3
and
technology_3
requirement_4
give
you
a
great
opportunity
to
build
pattern_2
fast
use
different
style
of
connector_7
you
can
create
pattern_5
pattern_6
pattern_2
base
on
technology_3
requirement_4
netflix
technology_11
a
show
in
one
of
my
previous

quick
guide
to
pattern_2
with
technology_3


eureka
and
technology_3
requirement_4
you
can
create
pattern_7
reactive
pattern_2
quality_attribute_1
on
technology_16
with
technology_3
webflux
project
and
combine
it
succesfully
with
some
technology_3
requirement_4
technology_11
a
show
in
my
reactive
pattern_2
with
technology_3
webflux
and
technology_3
requirement_4
and
finally
you
connector_8
connector_data_1
drive
pattern_2
base
on
pattern_8
component_8
use
technology_3
requirement_4
connector_6
and
connector_data_1
pattern_1
technology_9
technology_7
or
technology_6
the
last
of
connector_data_4
approach
to
build
pattern_2
be
the
subject
of
this

i’m
go
to
show
you
how
to
effectively
build
quality_attribute_6
run
and
test
pattern_3
pattern_2
base
on
technology_6
pattern_1
architecture
for
the
purpose
of
demonstrate
technology_3
requirement_4
connector_6
feature
we
will
design
a
sample
component_1
which
us
pattern_8
component_8
for
inter
component_2
connector_7
we
have
three
pattern_2
order
component_2
technology_17
component_2
and
account
component_2
component_4
order
component_2
connector_9
technology_18
that
be
responsible
for
component_9
order
connector_10
to
our
component_1
all
the
incoming
order
be
component_9
asynchronously
–
order
component_2
prepare
and
connector_11
connector_data_1
to
technology_6
exchange
and
then
respond
to
the
connector_12
component_10
that
the
connector_data_5
have
be
connector_13
for
component_9
component_3
account
component_2
and
technology_17
component_2
be
listen
for
the
order
connector_data_6
incoming
to
the
exchange
pattern_9
account
component_2
be
responsible
for
connector_14
if
there
be
sufficient
fund
on
customer’s
account
for
order
realization
and
then
withdraw
cash
from
this
account
pattern_9
technology_17
component_2
connector_15
if
there
be
sufficient
amount
of
technology_17
in
the
component_6
and
connector_16
the
number
of
quality_attribute_5
technology_17
after
component_9
order
both
account
component_2
and
technology_17
component_2
connector_11
pattern_7
connector_17
through
technology_6
exchange
this
time
it
be
one
to
one
connector_7
use
direct
exchange
with
a
status
of

pattern_9
order
component_2
after
connector_18
connector_17
connector_data_6
set
the
appropriate
status
of
the
order
and
connector_9
it
through
pattern_6
connector_19
order
{id}
to
the
external
component_10
if
you
feel
that
the
description
of
our
sample
component_1
be
a
little
incomprehensible
here’s
the
diagram
with
architecture
for
clarification
enabling
technology_3
requirement_4
connector_6
the
recommend
way
to
include
technology_3
requirement_4
connector_6
in
the
project
be
with
a
connector_20
requirement_5
component_1
technology_3
requirement_4
connector_6
have
an
independent
release
train
requirement_5
in
relation
to
the
whole
technology_3
requirement_4
technology_10
however
if
we
have
declare
technology_3
requirement_4
connector_20
in
the
elmhurst
release
version
inside
the
dependencymanagement
section
we
wouldn’t
have
to
declare
anything
else
in
pom
technology_19
if
you
prefer
to
use
only
the
technology_3
requirement_4
connector_6
project
you
should
define
the
follow
section
dependencymanagement
connector_20
connector_20


springframework
requirement_4


technology_3
requirement_4
connector_6
connector_20

version
elmhurst
release
version
type
pom
type
scope

scope
connector_20
connector_20
dependencymanagement
the
next
step
be
to
technology_3
requirement_4
connector_6
artifact
to
the
project
connector_20
i
also
recommend
you
include
at
least
the
technology_3
requirement_4
sleuth
technology_11
to
provide
connector_21
pattern_3
with
the
same
traceid
a
the
component_11
connector_data_5
incoming
to
order
component_2
connector_20


springframework
requirement_4


technology_3
requirement_4
connector_6

connector_20
connector_20


springframework
requirement_4


technology_3
requirement_4
sleuth

connector_20
technology_3
requirement_4
connector_6
programming
component_8
to
enable
connector_22
to
a
connector_data_1
pattern_1
for
your
component_4
annotate
the
with
@enablebinding
the
@enablebinding
annotation
take
one
or
more
a
parameter
you
choose
between
three
provide
by
technology_3
requirement_4
connector_6
connector_23
this
be
use
for
mark
a
component_2
that
connector_24
connector_data_6
from
the
inbound
pattern_10
component_11
this
be
use
for
connector_21
connector_data_6
to
the
outbound
pattern_10
processor
this
can
be
use
in
requirement_6
you
need
both
an
inbound
pattern_10
and
an
outbound
pattern_10
a
it
extend
the
component_11
and
connector_23

because
order
component_2
connector_25
connector_data_1
a
well
a
connector_24
them
it
have
be
annotate
with
@enablebinding
processor

here’s
the
of
order
component_2
that
enable
technology_3
requirement_4
connector_6
bind
@springbootapplication
@enablebinding
processor

orderapplication
{


args
{
springapplicationbuilder
orderapplication

web
true
run
args
}
}

connector_data_1
pattern_1
in
technology_3
requirement_4
connector_6
nomenclature
the
implementation
responsible
for
requirement_2
with
specific
connector_data_1
pattern_1
be
connector_26
binder
by
default
technology_3
requirement_4
connector_6
provide
binder
implementation
for
technology_7
and
technology_6
it
be
able
to
automatically
detect
and
use
a
binder
find
on
the
classpath
any
technology_20
specific
setting
can
be
override
through
external
configuration
property
in
the
form
support
by
technology_3
such
a
component_4
argument
environment
variable
or
the
component_4
yml

to
include
support
for
technology_6
which
use
it
this
a
a
connector_data_1
pattern_1
you
should
the
follow
connector_20
to
the
project
connector_20


springframework
requirement_4


technology_3
requirement_4
starter
connector_6
rabbit

connector_20
now
our
component_3
need
to
connector_27
with
one
connector_28
instance
of
technology_6
pattern_1
that’s
why
i
run
technology_15
image
with
technology_6
connector_29
outside
on
default

port
it
also
launch
web
requirement_7
quality_attribute_5
under
connector_30
technology_18





$
technology_15
run
technology_21
name
rabbit
p


p


technology_6
requirement_5
we
need
to
override
default
connector_30
of
technology_6
for
every
technology_3
component_4
by
setting
property
technology_3
technology_6
component_12
to
technology_15
component_13
ip




technology_3
technology_6
component_12




port

connector_31
connector_data_1
drive
pattern_2
technology_3
requirement_4
connector_6
be
build
on
top
of
technology_3
requirement_2
project
technology_3
requirement_2
extend
the
technology_3
programming
component_8
to
support
the
well

requirement_8
requirement_2
pattern_11
eip
eip
define
a
number
of
component_14
that
be
typically
use
for
pattern_12
in
quality_attribute_3
component_1
you
have
probably
hear
about
pattern_11
such
a
connector_data_1
pattern_10
pattern_13
aggregator
or

let’s
proceed
to
the
implementation
we
begin
from
order
component_2
that
be
responsible
for
connector_32
order
publish
them
on
connector_28
topic
and
then
connector_33
pattern_7
connector_17
from
downstream
component_2
here’s
the
@service
which
build
connector_data_1
and
publish
it
to
the
remote
topic
use
component_11
component_15
@service
ordersender
{
@autowired
private
component_11
component_11
boolean
connector_11
order
order
{
this
component_11
output
connector_11
messagebuilder
withpayload
order
build
}
}
that
@service
be
connector_26
by
the
pattern_14
which
connector_9
the
technology_18
for
submit
order
and
connector_34
order
with
status
by

@restcontroller
ordercontroller
{
private
final
logger
logger
=
loggerfactory
getlogger
ordercontroller

private
objectmapper
mapper
=
objectmapper
@autowired
orderrepository
pattern_15
@autowired
ordersender
sender
@postmapping
order
component_9
@requestbody
order
order
throw
jsonprocessingexception
{
order
o
=
pattern_15

order
logger
info
order
connector_35
{}
mapper
writevalueasstring
order
boolean
issent
=
sender
connector_11
o
logger
info
order
connector_11
{}
mapper
writevalueasstring
collection
singletonmap
issent
issent
o
}
@getmapping
{id}
order
findbyid
@pathvariable

long

{
pattern_15
findbyid

}
}
now
let’s
take
a
close
look
on
component_16
side
the
connector_data_1
connector_10
by
ordersender
component_15
from
order
component_2
be
connector_36
by
account
component_2
and
technology_17
component_2
to
connector_37
the
connector_data_1
from
topic
exchange
we
have
to
annotate
the
that
take
the
order
connector_data_7
a
a
parameter
with
@streamlistener
we
also
have
to
define
target
pattern_10
for
component_17
–
in
that
requirement_6
it
be
processor
input
@springbootapplication
@enablebinding
processor

orderapplication
{
private
final
logger
logger
=
loggerfactory
getlogger
orderapplication

@autowired
orderservice
component_2


args
{
springapplicationbuilder
orderapplication

web
true
run
args
}
@streamlistener
processor
input
receiveorder
order
order
throw
jsonprocessingexception
{
logger
info
order
connector_37
{}
mapper
writevalueasstring
order
component_2
component_9
order
}
}
connector_36
order
be
then
component_9
by
accountservice
component_15
order
be
connector_13
or
reject
by
account
component_2
dependending
on
sufficient
fund
on
customer’s
account
for
order’s
realization
the
connector_17
with
acceptance
status
be
connector_10
back
to
order
component_2
via
output
pattern_10
invoke
by
the
ordersender
component_15
@service
accountservice
{
private
final
logger
logger
=
loggerfactory
getlogger
accountservice

private
objectmapper
mapper
=
objectmapper
@autowired
accountrepository
accountrepository
@autowired
ordersender
ordersender
component_9
final
order
order
throw
jsonprocessingexception
{
logger
info
order
component_9
{}
mapper
writevalueasstring
order
connector_data_4
account
=
accountrepository
findbycustomer
order
getcustomerid
account
account
=
account
connector_19

logger
info
account
find
{}
mapper
writevalueasstring
account
if
order
getprice
=
account
getbalance
{
order
setstatus
orderstatus
connector_13
account
setbalance
account
getbalance
order
getprice
}
else
{
order
setstatus
orderstatus
reject
}
ordersender
connector_11
order
logger
info
order
connector_17
connector_11
{}
mapper
writevalueasstring
order
}
}
the
last
step
be
configuration
it
be
provide
inside
component_4
yml

we
have
to
properly
define
destination
for
pattern_10
while
order
component_2
be
assign
order
out
destination
to
output
pattern_10
and
order
in
destination
to
input
pattern_10
account
component_2
and
technology_17
component_2
do
the
opposite
it
be
logical
because
connector_data_1
connector_10
by
order
component_2
via
it
output
destination
be
connector_36
by
connector_38
component_18
via
their
input
destination
but
it
be
still
the
same
destination
on
connector_28
broker’s
exchange
here
be
configuration
setting
of
order
component_2
technology_3
requirement_4
connector_6
bind
output
destination
order
out
input
destination
order
in
rabbit
bind
input
component_16
exchangetype
direct
here’s
configuration
provide
for
account
component_2
and
technology_17
component_2
technology_3
requirement_4
connector_6
bind
output
destination
order
in
input
destination
order
out
rabbit
bind
output
component_19
exchangetype
direct
routingkeyexpression
#
finally
you
can
run
our
sample
pattern_9
for
now
we
need
to
run
a
single
instance
of
each
pattern_9
you
can
easily
generate
some
test
connector_data_8
by
run
junit
test
ordercontrollertest
provide
in
my
component_11
pattern_15
inside
order
component_2
this
requirement_6
be
quality_attribute_2
in
the
next
we
will
study
more
advance
sample
with
multiple
run
instance
of
connector_38
component_2
quality_attribute_7
up
to
quality_attribute_6
up
our
technology_3
requirement_4
connector_6
component_3
we
need
to
launch
additional
instance
of
each
pattern_9
they
will
still
listen
for
the
incoming
connector_data_6
on
the
same
topic
exchange
a
the
currently
run
instance
after

one
instance
of
account
component_2
and
technology_17
component_2
we
connector_11
a
test
order
the
connector_data_9
of
that
test
won’t
be
satisfactory
for
us…
why
a
single
order
be
connector_36
by
all
the
run
instance
of
every
pattern_9
this
be
exactly
how
topic
exchange
work
–
the
connector_data_1
connector_10
to
topic
be
connector_36
by
all
component_16
which
be
listen
on
that
topic
fortunately
technology_3
requirement_4
connector_6
be
able
to
solve
that
problem
by
provide
solution
connector_26
component_16
group
it
be
responsible
for
guarantee
that
only
one
of
the
instance
be
expect
to
handle
a
give
connector_data_1
if
they
be
place
in
a
compete
component_16
relationship
the
transformation
to
component_16
group
mechanism
when
run
multiple
instance
of
the
component_2
have
be
visualize
on
the
follow
figure
configuration
of
a
component_16
group
mechanism
be
not
very
difficult
we
have
to
set
group
parameter
with
name
of
the
group
for
give
destination
here’s
the
current
bind
configuration
for
account
component_2
the
order
in
destination
be
a
component_20
create
for
direct
connector_7
with
order
component_2
so
only
order
out
be
group
use
technology_3
requirement_4
connector_6
bind
group
property
technology_3
requirement_4
connector_6
bind
output
destination
order
in
input
destination
order
out
group
account
component_16
group
mechanism
be
a
concept
take
from
technology_9
technology_7
and
connector_8
in
technology_3
requirement_4
connector_6
also
for
technology_6
pattern_1
which
do
not
natively
support
it
so
i
think
it
be
pretty
interest
how
it
be
configure
on
technology_6
if
you
run
two
instance
of
the
component_2
without
set
group
name
on
destination
there
be
two
bind
create
for
a
single
exchange
one
bind
per
one
instance
a
show
in
the
picture
below
because
two
component_3
be
listen
on
that
exchange
there
four
bind
assign
to
that
exchange
in
total
if
you
set
group
name
for
selected
destination
technology_3
requirement_4
connector_6
will
create
a
single
bind
for
all
run
instance
of
give
component_2
the
name
of
bind
will
be
suffix
with
group
name
because
we
have
include
technology_3
requirement_4
starter
sleuth
to
the
project
connector_20
the
same
traceid
be
connector_10
between
all
the
pattern_7
connector_data_8
exchange
during
realization
of
single
connector_data_5
incoming
to
the
order
component_2

thanks
to
that
we
can
easily
correlate
all
requirement_9
use
this
use
elastic
technology_22
kibana
automate
test
you
can
easily
test
your
pattern_9
without
connector_3
to
a
connector_data_1
pattern_1
to
achieve
it
you
need
to
include
technology_3
requirement_4
connector_6
test
support
to
your
project
connector_20
it
contain
the
testsupportbinder
component_15
that

you
connector_39
with
the
bind
pattern_10
and
inspect
any
connector_data_6
connector_10
and
connector_36
by
the
component_4
connector_20


springframework
requirement_4


technology_3
requirement_4
connector_6
test
support

scope
test
scope
connector_20
in
the
test
we
need
to
declare
messagecollector
component_15
which
be
responsible
for
connector_18
connector_data_6
retain
by
testsupportbinder
here’s
my
test
from
account
component_2
use
processor
component_15
i
connector_11
test
order
to
input
pattern_10
then
messagecollector
connector_24
connector_data_1
that
be
connector_10
back
to
order
component_2
via
output
pattern_10
test
testaccepted
create
order
that
should
be
connector_13
by
account
component_2
while
testrejected
set
too
high
order
requirement_10
that
connector_data_10
in
reject
the
order
@runwith
springrunner

@springboottest
webenvironment
=
springboottest
webenvironment
random_port
orderreceivertest
{
private
final
logger
logger
=
loggerfactory
getlogger
orderreceivertest

@autowired
private
processor
processor
@autowired
private
messagecollector
messagecollector
@test
@suppresswarnings
unchecked
testaccepted
{
order
o
=
order
o
setid
1l
o
setaccountid
1l
o
setcustomerid
1l
o
setprice

o
setproductids
collection
singletonlist
2l
processor
input
connector_11
messagebuilder
withpayload
o
build
connector_data_1
connector_36
=
connector_data_1
messagecollector
forchannel
processor
output
pattern_16
logger
info
order
connector_17
connector_37
{}
connector_37
getpayload
assertnotnull
connector_37
getpayload
assertequals
orderstatus
connector_13
connector_37
getpayload
getstatus
}
@test
@suppresswarnings
unchecked
testrejected
{
order
o
=
order
o
setid
1l
o
setaccountid
1l
o
setcustomerid
1l
o
setprice

o
setproductids
collection
singletonlist
2l
processor
input
connector_11
messagebuilder
withpayload
o
build
connector_data_1
connector_36
=
connector_data_1
messagecollector
forchannel
processor
output
pattern_16
logger
info
order
connector_17
connector_37
{}
connector_37
getpayload
assertnotnull
connector_37
getpayload
assertequals
orderstatus
reject
connector_37
getpayload
getstatus
}
}
conclusion
connector_data_1
drive
pattern_2
be
a
quality_attribute_8
choice
whenever
you
don’t
need
pattern_5
connector_17
from
your
technology_23
in
this
i
have
show
sample
use
requirement_6
of
pattern_8
component_8
in
inter
component_2
connector_7
between
your
pattern_2
the
component_11
be
a
usual
quality_attribute_5
on
technology_24
technology_18
technology_25
technology_1
piomin
sample
connector_data_1
drive
pattern_2
git
for
more
interest
example
with
usage
of
technology_3
requirement_4
connector_6
technology_11
also
with
technology_9
technology_7
you
can
refer
to
chapter

in
my
book
master
technology_3
requirement_4
technology_18
www
packtpub
technology_1
component_4
development
master
technology_3
requirement_4
tag
technology_9
technology_7
connector_data_1
pattern_1
connector_data_1
pattern_2
technology_6
technology_3
technology_3
cloud7

partitioning
with
technology_9
technology_7
and
technology_26

on

2018may


by
piotr
mińkowski
technology_9
technology_7
be
a
quality_attribute_3
connector_40
component_21
it
also
act
a
a
pattern_3
component_1
in
your
architecture
traditional
connector_data_1
pattern_1
provide
two
component_22
of
connector_7
pattern_17
and
pattern_18
topic
component_23
be
use
for
point
to
point
connector_data_1
while
topic
allow
you
pattern_19
connector_data_2
to
multiple
target
component_16
technology_7
do
not
provide
pattern_17
mechanism
directly
however
it
introduce
the
component_16
group
concept
which
generalize
both
pattern_17
and
pattern_18
component_8
the
component_16
group
mechanism
guarantee
that
a
single
connector_data_1
would
be
component_9
by
the
only
one
component_16
that
belong
to
the
give
group
it
be
especially
useful
when
you
have
more
than
one
instance
of
your
component_2
which
listen
for
connector_data_6
incoming
to
the
topic
that
feature
make
your
component_24
to
behave
a
pattern_17
component_25
within
the
same
group
eclipse
technology_26
be
a
lightweight
and
fast
technology_27
for
build
reactive
component_3
on
the
technology_28
i
have
already
introduce
that
solution
be
the
some
of
my
previous

for
example
pattern_7
pattern_2
with
vert
x
technology_26
do
not
force
you
to
connector_8
a
reactive
component_4
you
create
a
technology_29
component_2
which
component_26
the
technology_18
connector_data_8
asynchronously
in
accordance
with
pattern_7
i
o
concept
the
purpose
of
this
the
purpose
of
this
be
to
show
you
the
feature
of
technology_9
technology_7
that
be
useful
when
create
component_3
connector_38
connector_data_1
the
technology_2
client’s
technology_11
choice
be
not
a
key
point
here
however
in
my
opinion
technology_26
that
be
pattern_7
high
requirement_11
technology_10
perfectly
match
to
technology_9
technology_7
it
provide
technology_26
technology_7
component_10
which
allow
you
to
connector_41
and
connector_11
connector_data_6
from
to
an
technology_7
cluster
before
we
proceed
to
the
sample
let’s
first
dive
into
the
core
abstraction
of
technology_7
technology_7
topic
i’m
assume
you
excellent
what
topic
be
and
what
be
it
role
the
every
connector_data_1
incoming
to
the
topic
go
to
every
pattern_20
what
be
the
difference
between
technology_7
and
technology_29
topic
provide
by
other
connector_data_1
pattern_1
technology_7
topic
be
component_27
each
component_27
be
an
order
immutable
sequence
of
component_28
every
component_28
can
be
uniquecly
identify
within
the
component_27
by
a
sequential
number
connector_26
the
offset
the
technology_7
cluster
retain
all
publish
component_29
accord
to
the
configure
retention
period
component_16
subscribe
to
the
whole
topic
or
only
to
the
selected
component_27
it
can
also
control
the
offset
from
where
it
start
component_9
connector_data_2
for
example
it
be
able
to
reset
offset
in
order
reprocess
connector_data_2
from
the
past
or
or
skip
ahead
to
the
most
recent
component_28
to
connector_42
only
connector_data_6
currently
connector_10
to
the
topic
here’s
the
figure
that
illustrate
a
single
component_27
connector_data_3
with
component_19
and
component_24
listen
for
the
incoming
connector_data_2
sample
architecture
me
say
some
word
about
the
sample
component_1
architecture
it
component_11
be
quality_attribute_5
on
technology_24
technology_18
technology_25
technology_1
piomin
sample
vertx
technology_7
connector_data_1
git
in
accordance
of
the
principle
that
one
picture
speak
more
than
a
thousand
word
the
diagram
illustrate
the
architecture
of
our
component_1
be
visible
below
we
have
one
topic
create
on
technology_7
component_21
that
consist
of
two
component_27
there
be
one
component_10
component_4
that
connector_9
pattern_6
component_30
allow
to
connector_11
order
into
the
component_1
and
then
connector_43
them
into
the
topic
the
target
component_27
be
calculate
base
on
the
type
of
order
we
create
order
with
type
single
and
multiple
there
be
also
some
component_3
that
connector_44
connector_data_2
from
topic
first
of
them
single
order
processor
connector_45
connector_data_2
from
component_27

the
second
multiple
order
processor
from
component_27

and
the
last
all
order
processor
do
not
choose
any
component_27
run
technology_7
to
run
technology_9
technology_7
on
the
local
component_13
we
use
it
technology_15
image
the
image
connector_28
by
spotify
also
start
technology_30
component_7
which
be
use
by
technology_7
if
you
run
technology_15
on
window
the
default
connector_30
of
it
virtual
component_13
be




technology_15
run
technology_21
name
technology_7
p


p


env
advertised_host=192



env
advertised_port=9092
spotify
technology_7
however
that
option
assume
the
topic
would
be
automatically
create
during
component_4
startup
i’ve
connector_19
some
problem
with
it
while
create
multi
component_27
topic
there
be
also
another
image
ches
technology_7
which
require
start
technology_30
separately
but
provide
technology_7
component_10

technology_15
run
technology_21
name
technology_30
p


technology_30
technology_15
run
technology_21
name
technology_7
p


p


requirement_12
technology_7
net
env
kafka_advertised_host_name=192



env
zookeeper_ip=192



ches
technology_7
finally
we
can
run
ches
technology_7
container
in
component_10
mode
and
then
create
topic
order
out
with
two
component_27
technology_15
run
rm
requirement_12
technology_7
net
ches
technology_7
technology_7
topic
sh
create
topic
order
out
pattern_21
factor

component_27

technology_30





create
topic
order
out
build
component_19
component_4
first
we
need
to
include
technology_31
connector_20
to
enable
technology_26
technology_10
for
the
component_4
if
the
component_4
connector_9
pattern_22
technology_18
component_30
you
should
include
vertx
web
technology_11
vertx
technology_7
component_10
have
to
be
include
to
all
the
sample

to
start
technology_26
a
technology_2
component_4
we
have
to
create
verticle
by
extend
abstractverticle
then
the
verticle
need
to
be
quality_attribute_1
in
the
use
vertx
connector_data_7
for
more
detail
about
technology_26
and
verticles
concept
you
refer
to
one
of
my
previous
mention
in
the
preface
orderverticle
extend
abstractverticle
{


args
{
vertx
vertx
=
vertx
vertx
vertx
deployverticle

orderverticle
}
}
the
next
step
be
to
define
component_19
use
kafkaproducer

we
have
to
provide
connector_46
setting
and
serializer
implementation

you
can
choose
between
various
build
in
serializer
implemementations
the
most
suitable
for
me
be
jsonobjectserializer
which
require
a
an
input
parameter
property
config
=
property
config
put
producerconfig
bootstrap_servers_config





config
put
producerconfig
key_serializer_class_config
stringserializer

config
put
producerconfig
value_serializer_class_config
jsonobjectserializer

config
put
producerconfig
acks_config

kafkaproducer
component_19
=
kafkaproducer
create
vertx
config
the
procuder
be
invoke
inside
connector_47
definition
it

an
pattern_7
connector_17
with
a
status
after
connector_21
connector_data_1
to
the
topic
the
connector_data_1
be
create
use
kafkaproducerrecord

it
take
topic’s
name
connector_data_5
connector_data_7
and
component_27
number
a
the
parameter
a
you
see
in
the
fragment
of
below
component_27
number
be
calculate
on
the
basis
order
type
o
gettype
ordinal
pattern_13
pattern_13
=
pattern_13
pattern_13
vertx
pattern_13
connector_47
order
*
pattern_23
responsecontenttypehandler
create
pattern_13
connector_47
httpmethod

order
pattern_23
bodyhandler
create
pattern_13

order
produce
component_4
technology_32
pattern_23
rc
{
order
o
=
technology_32
decodevalue
rc
getbodyasstring
order

kafkaproducerrecord
component_28
=
kafkaproducerrecord
create
order

rc
getbodyasjson
o
gettype
ordinal
component_19
connector_4
component_28
do
{
if
do
succeed
{
recordmetadata
recordmetadata
=
do
connector_data_9
logger
info
component_28
connector_11
msg={}
destination={}
partition={}
offset={}
component_28
requirement_3
recordmetadata
gettopic
recordmetadata
getpartition
recordmetadata
getoffset
o
setid
recordmetadata
getoffset
o
setstatus
orderstatus
component_9
}
else
{
throwable
t
=
do
cause
logger
error
error
connector_10
to
topic
{}
t
getmessage
o
setstatus
orderstatus
reject
}
rc
connector_17
end
technology_32
encodeprettily
o
}
}
vertx
createhttpserver
requesthandler
pattern_13
connector_13
listen

build
component_16
component_3
the
component_16
configuration
be
very
similar
to
that
for
component_19
we
also
have
to
set
connector_46
setting
and
use
for
deserialization
there
be
one
interest
set
which
have
be
define
for
the
component_16
in
the
fragment
of
visible
below
it
be
auto
offset
reset
consumerconfig
auto_offset_reset_config
it
set
the
initial
offset
in
technology_7
for
the
requirement_13
during
initialization
if
you
would
to
connector_41
all
component_29
from
the
begin
of
connector_6
use
requirement_3
early
if
you
would
to
component_26
only
the

component_29
connector_37
after
component_4
startup
set
that
property
to
late
because
in
our
requirement_6
technology_7
act
a
a
connector_data_1
pattern_1
it
be
set
to
late
property
config
=
property
config
put
consumerconfig
bootstrap_servers_config





config
put
consumerconfig
key_deserializer_class_config
stringdeserializer

config
put
consumerconfig
value_deserializer_class_config
stringdeserializer

config
put
consumerconfig
auto_offset_reset_config
late
config
put
consumerconfig
enable_auto_commit_config
false
kafkaconsumer
component_16
=
kafkaconsumer
create
vertx
config
a
you
probably
remember
we
have
three
different
component_4
that
subscribe
to
the
topic
the
first
of
them
connector_8
under
the
all
order
processor
connector_44
all
the
incoming
to
the
the
topic
this
implemementation
be
relatively
the
quality_attribute_2
we
only
need
to
invoke
subscribe
and
pass
the
name
of
topic
a
a
parameter
then
every
incoming
connector_data_1
be
component_9
by
pattern_23

component_16
subscribe
order
out
ar
{
if
ar
succeed
{
logger
info
subscribe
}
else
{
logger
error
could
not
subscribe
err={}
ar
cause
getmessage
}
}
component_16
pattern_23
component_28
{
logger
info
component_9
key={}
value={}
partition={}
offset={}
component_28
key
component_28
requirement_3
component_28
component_27
component_28
offset
order
order
=
technology_32
decodevalue
component_28
requirement_3
order

order
setstatus
orderstatus
do
logger
info
order
component_9
id={}
price={}
order
getid
order
getprice
}
the
implementation
of
connector_38
for
the
other
component_3
be
a
little
more
complicate
besides
define
target
topic
every
component_16
can
ask
for
a
specific
component_27
the
component_4
multiple
order
processor
subscribe
to
component_27

while
multiple
order
processor
to
component_27

topicpartition
tp
=
topicpartition
setpartition

settopic
order
out
component_16
assign
tp
ar
{
if
ar
succeed
{
logger
info
subscribe
component_16
assignment
done1
{
if
done1
succeed
{
for
topicpartition
topicpartition
done1
connector_data_9
{
logger
info
component_27
topic={}
number={}
topicpartition
gettopic
topicpartition
getpartition
}
}
else
{
logger
error
could
not
assign
component_27
err={}
done1
cause
getmessage
}
}
}
else
{
logger
error
could
not
subscribe
err={}
ar
cause
getmessage
}
}
the
implamentation
of
handle
inside
multiple
order
processor
be
pretty
interest
if
it
connector_24
order
with
non
empty
relatedorderid
it
try
to
find
it
in
the
historical
component_29
component_6
in
topic
it
achieve
by
connector_12
seek
on
kafkaconsumer
component_16
pattern_23
component_28
{
logger
info
component_9
key={}
value={}
partition={}
offset={}
component_28
key
component_28
requirement_3
component_28
component_27
component_28
offset
order
order
=
technology_32
decodevalue
component_28
requirement_3
order

if
orderswaiting
containskey
component_28
offset
{
logger
info
relate
order
find
id={}
price={}
order
getid
order
getprice
logger
info
current
requirement_10
price={}
order
getprice
+
orderswaiting
connector_19
component_28
offset
getprice
component_16
seektoend
tp
}
if
order
getrelatedorderid
=
&&
orderswaiting
containskey
order
getrelatedorderid
{
orderswaiting
put
order
getrelatedorderid
order
component_16
seek
tp
order
getrelatedorderid
}
}
test
now
it
be
time
to
launch
our
component_4
you
run
the
from
your
ide
or
build
the
whole
project
use
mvn
clean
install
command
and
then
run
it
with
technology_2
jar
also
run
two
instance
of
all
order
processor
in
order
to
connector_48
out
how
a
component_16
group
mechanism
work
in
practice
let’s
connector_11
some
test
connector_data_8
to
the
order
component_2
in
the
follow
sequence
curl
h
content
type
component_4
technology_32
x
technology_21
{
type
single
status

requirement_10
200}
technology_18
localhost

order
{


type
single
status
component_9
requirement_10
200}
curl
h
content
type
component_4
technology_32
x
technology_21
{
type
single
status

requirement_10
300}
technology_18
localhost

order
{


type
single
status
component_9
requirement_10
300}
curl
h
content
type
component_4
technology_32
x
technology_21
{
type
multiple
status

requirement_10
400}
technology_18
localhost

order
{


type
multiple
status
component_9
requirement_10
400}
curl
h
content
type
component_4
technology_32
x
technology_21
{
type
multiple
status

requirement_10

relatedorderid
0}
technology_18
localhost

order
{


type
multiple
status
component_9
requirement_10
500}
here’s
requirement_9
from
component_19
component_4






info
component_28
connector_11
msg={
type
single
status

requirement_10
200}
destination=orders
out
partition=0
offset=0






info
component_28
connector_11
msg={
type
single
status

requirement_10
300}
destination=orders
out
partition=0
offset=1






info
component_28
connector_11
msg={
type
multiple
status

requirement_10
400}
destination=orders
out
partition=1
offset=0






info
component_28
connector_11
msg={
type
multiple
status

requirement_10

relatedorderid
0}
destination=orders
out
partition=1
offset=1
here’s
requirement_9
from
single
order
processor
it
have
component_9
only
connector_data_6
from
component_27







info
component_9
key=null
value={
type
single
status

requirement_10
200}
partition=0
offset=0






info
component_9
key=null
value={
type
single
status

requirement_10
300}
partition=0
offset=1
here’s
requirement_9
from
multiple
order
processor
it
have
component_9
only
connector_data_6
from
component_27







info
component_9
key=null
value={
type
multiple
status

requirement_10
400}
partition=1
offset=0






info
component_9
key=null
value={
type
multiple
status

requirement_10

relatedorderid
0}
partition=1
offset=1
here’s
requirement_9
from
first
instance
of
all
order
processor






info
component_9
key=null
value={
type
single
status

requirement_10
200}
partition=0
offset=0






info
component_9
key=null
value={
type
single
status

requirement_10
300}
partition=0
offset=1
here’s
requirement_9
from
second
instance
of
all
order
processor
it
be
a
little
bit
surprise
for
you
but
if
you
run
two
instance
of
component_16
which
listen
for
the
whole
topic
each
instance
would
component_9
connector_data_1
from
the
single
component_27






info
component_9
key=null
value={
type
multiple
status

requirement_10
400}
partition=1
offset=0






info
component_9
key=null
value={
type
multiple
status

requirement_10

relatedorderid
0}
partition=1
offset=1
summary
in
this
i
be
try
to
give
you
a
little
bit
of
pattern_3
with
technology_9
technology_7
such
concept
component_16
group
or
partitioning
be
something
what
make
it
different
from
traditional
pattern_3
solution
technology_7
be
widely
adopt
technology_17
which
can
act
a
storage
pattern_3
component_1
or
connector_6
processor
together
with
popular
technology_28
base
technology_27
technology_26
it
be
really
powerful
fast
and
lightweight
solution
for
your
component_3
that
exchange
connector_data_6
between
each
other
the
key
concept
introduce
by
technology_7
have
be
adopt
by
technology_3
requirement_4
connector_6
which
make
them
a
an
opinionated
choice
for
create
pattern_3
pattern_2
tag
technology_9
technology_7
technology_2
connector_data_1
publish
subscribe
vert
x1
technology_6
in
cluster

on

2017may


by
piotr
mińkowski
technology_6
grow
into
the
most
popular
connector_data_1
pattern_1

it
be
connector_4
in
technology_33
and
connector_49
advance
connector_data_1
component_20
technology_34
technology_35
it
be
easy
to
use
and
configure
even
if
we
be
talk
about
such
mechanism
a
cluster
or
high
availibility
in
this
i’m
go
to
show
you
how
to
run
some
instance
of
technology_6
provide
in
technology_15
container
in
the
cluster
with
highly
quality_attribute_5
ha
component_20
base
on
the
sample
technology_2
component_4
we’ll
see
how
to
connector_11
and
connector_37
connector_data_6
from
the
technology_6
cluster
and
connector_48
how
this
connector_data_1
pattern_1
handle
a
large
number
of
incoming
connector_data_1
sample
technology_3
component_4
be
quality_attribute_5
on
technology_25
here
be
picture
ilustrating
architecture
of
the
present
solution
we
use
technology_15
official
pattern_15
of
technology_6
here
be
command
for
run
three
technology_6
technology_14
first
technology_14
be
the
master
of
cluster
–
two
other
technology_14
will
join
him
we
use
container
requirement_5
to
enable
an
ui
administration
console
for
each
technology_14
every
technology_14
have
default
connector_46
and
ui
requirement_5
port
connector_50
important
thing
be
to
connector_51
rabbit2
and
rabbit3
constainers
to
rabbit1
which
be
necessary
while
join
to
cluster
master
by
rabbit1
technology_15
run
technology_21
hostname
rabbit1
name
rabbit1
e
rabbitmq_erlang_cookie=
rabbitcluster
p


p


technology_6
requirement_5
technology_15
run
technology_21
hostname
rabbit2
name
rabbit2
connector_51
rabbit1
rabbit1
e
rabbitmq_erlang_cookie=
rabbitcluster
p


p


technology_6
requirement_5
technology_15
run
technology_21
hostname
rabbit3
name
rabbit3
connector_51
rabbit1
rabbit1
e
rabbitmq_erlang_cookie=
rabbitcluster
p


p


technology_6
requirement_5
ok
now
there
be
three
technology_6
run
instance
we
can
go
to
the
ui
requirement_5
console
for
all
of
those
instance
quality_attribute_5
a
technology_15
container
for
example
technology_18





technology_6
each
instance
be
quality_attribute_5
on
it
independent
cluster
we
see
in
the
picture
below
we
would
to
make
all
instance
work
in
same
cluster
rabbit@rabbit1
here’s
set
of
command
run
on
rabbit2
instance
for
join
cluster
rabbit@rabbit1
the
same
set
should
be
run
on
rabbit3
technology_14
in
the
begin
we
have
to
connector_3
to
technology_15
container
and
run
bash
command
before
run
technology_6
join_cluster
command
we
have
to
stop
pattern_1
technology_15
exec
i
t
rabbit2
\bash
root@rabbit2
#
rabbitmqctl
stop_app
stop
technology_14
rabbit@rabbit2
root@rabbit2
#
rabbitmqctl
join_cluster
rabbit@rabbit1
cluster
technology_14
rabbit@rabbit2
with
rabbit@rabbit1
root@rabbit2
#
rabbitmqctl
start_app
start
technology_14
rabbit@rabbit2
if
everything
be
successful
we
should
see
cluster
name
rabbit@rabbit1
in
upper
right
corner
of
rabbit2
requirement_5
console
you
should
also
see
connector_data_4
of
run
technology_14
in
the
technology_14
section
you
can
also
connector_48
cluster
status
by
run
on
every
technology_14
command
rabbitmqctl
cluster_status
which
should
also
display
connector_data_4
of
all
cluster
technology_14
after
start
all
technology_14
go
to
ui
managent
console
on
one
of
technology_14
now
we
be
go
to
configure
high
availibility
for
selected
component_20
it
be
not
important
which
technology_14
you
choose
because
they
be
in
one
cluster
in
the
component_23
tab
create
component_20
with
name
q
example
then
go
to
admin
tab
and
select
requirement_14
section
and
create
requirement_14
in
the
picture
below
you
can
see
requirement_14
i
have
create
i
selected
ha
mode=all
which
mean
that
be
mirror
across
all
technology_14
in
the
cluster
and
when
technology_14
be

to
the
cluster
the
component_20
will
be
mirror
to
that
technology_14
there
be
also
quality_attribute_5
exactly
technology_14
mode
–
more
about
technology_6
high
availibility
you
can
find
here
in
pattern_11
enter
your
component_20
name
and
in
apply
to
select
component_20
if
everything
be
succeded
you
should
see
ha
all
feature
in
component_20
row
one
of
the
greatest
advantage
of
technology_6
be
pattern_24
you
can
see
many
statistic
memory
disk
usage
i
o
statistic
detail
connector_data_1
rat
graph
etc
some
of
them
you
could
see
below
technology_6
have
a
great
support
in
technology_3
technology_10
there
many
project
in
which
use
technology_6
implementation
by
default
for
example
technology_3
requirement_4
connector_6
technology_3
requirement_4
sleuth
i’m
go
to
show
you
sample
technology_3
component_4
that
connector_25
connector_data_6
to
technology_6
cluster
and
connector_24
them
from
ha
component_20
component_4
component_11
be
quality_attribute_5
on
technology_25
here’s
of
component_4
we
enable
technology_6
component_17
by
declare
@enablerabbit
on
and
@rabbitlistener
on
connector_18

we
also
have
to
declare
listen
component_20
pattern_1
connector_46
factory
and
component_17
container
factory
to
allow
component_17
pattern_25
inside
cachingconnectionfactory
we
set
all
three
connector_30
of
technology_6
cluster
instance















@springbootapplication
@enablerabbit
component_17
{
private
logger
logger
=
logger
getlogger
component_17


args
{
springapplication
run
component_17

args
}
@rabbitlistener
component_20
=
q
example
onmessage
order
order
{
logger
info
order
tostring
}
@bean
connectionfactory
connectionfactory
{
cachingconnectionfactory
connectionfactory
=
cachingconnectionfactory
connectionfactory
setusername
guest
connectionfactory
setpassword
guest
connectionfactory
setaddresses















connectionfactory
setchannelcachesize

connectionfactory
}
@bean
simplerabbitlistenercontainerfactory
rabbitlistenercontainerfactory
{
simplerabbitlistenercontainerfactory
factory
=
simplerabbitlistenercontainerfactory
factory
setconnectionfactory
connectionfactory
factory
setconcurrentconsumers

factory
setmaxconcurrentconsumers

factory
}
@bean
component_20
component_20
{
component_20
q
example
}
}
conclusion
cluster
and
high
availibility
configuration
with
technology_6
be
pretty
quality_attribute_2
i
rabbit_mq
for
support
in
the
cluster
pattern_26
component_9
with
ui
requirement_5
console
in
my
opinion
it
be
component_31
friendly
and
intuitive
in
the
sample
component_4
i
connector_11
100k
connector_data_6
into
the
sample
component_20
use

concurrent
component_24
they
be
component_9

second
~80
s
per
component_16
component_32
and
memory
usage
at
it
peak
be
about
400mb
on
each
technology_14
of
cource
our
component_4
be
connector_18
connector_data_7
connector_data_1
and
requirement_9
it
in
console
tag
technology_35
cluster
high
availibility
technology_6
technology_3
amqp4

drive
pattern_2
use
technology_3
requirement_4
connector_6
and
technology_6

on

2017may


by
piotr
mińkowski
before
we
start
let’s
look
at
technology_3
requirement_4
quick
start
there
be
a
connector_data_4
of
technology_3
requirement_4
release
quality_attribute_5
group
a
release
train
we
use
the

release
camden
sr5
with



release
technology_3
and
brooklyn
sr2
technology_3
requirement_4
connector_6
version
parent


springframework
boot


technology_3
boot
starter
parent

version



release
version
parent
dependencymanagement
connector_20
connector_20


springframework
requirement_4


technology_3
requirement_4
connector_20

version
camden
sr5
version
type
pom
type
scope

scope
connector_20
connector_20
dependencymanagement
here’s
our
architecture
visualization
order
component_2
connector_25
connector_data_1
to
technology_6
topic
exchange
technology_17
and
shipment
component_18
listen
on
that
topic
for
incoming
order
connector_data_6
and
then
component_9
them
after
component_9
they
connector_11
connector_data_11
connector_data_1
to
the
topic
on
which
payment
component_2
listen
to
payment
component_2
connector_52
incoming
connector_data_6
aggregate
connector_data_11
from
technology_17
and
shipment
component_2
then
count
requirement_10
and
connector_25
final
connector_17
each
component_2
have
the
follow
connector_20
we
have
sample
common
where
connector_data_7
for
connector_data_6
connector_10
to
topic
be
component_6
they’re
connector_28
between
all
component_2
we’re
also
use
technology_3
requirement_4
sleuth
for
quality_attribute_3
trace
with
one
connector_data_5
between
all
pattern_2
connector_20
connector_20


springframework
requirement_4


technology_3
requirement_4
connector_6

connector_20
connector_20


springframework
requirement_4


technology_3
requirement_4
connector_6
binder
rabbit

connector_20
connector_20


springframework
requirement_4


technology_3
requirement_4
starter
sleuth

connector_20
connector_20

pl
piomin
component_2


sample
common

version
${project
version}
version
connector_20
connector_20
me
start
with
a
few
word
on
the
theoretical
aspect
of
technology_3
requirement_4
connector_6
here’s
short
reference
of
that
technology_10
technology_3
requirement_4
connector_6
reference
guide
it’s
base
on
technology_3
requirement_2
it
provide
three
predefined
out
of
the
component_33
component_11
–
can
be
use
for
an
component_4
which
have
a
single
outbound
pattern_10
connector_23
–
can
be
use
for
an
component_4
which
have
a
single
inbound
pattern_10
processor
–
can
be
use
for
an
component_4
which
have
both
an
inbound
pattern_10
and
an
outbound
pattern_10
i’m
go
to
show
you
sample
usage
of
all
of
these

in
order
component_2
we’re
use
component_11

use
@inboundchanneladapter
and
@poller
annotation
we’are
connector_21
order
connector_data_1
to
output
once
per

second
@springbootapplication
@enablebinding
component_11

component_4
{
protect
logger
logger
=
logger
getlogger
component_4

getname
private
index
=



args
{
springapplication
run
component_4

args
}
@bean
@inboundchanneladapter
requirement_3
=
component_11
output
poller
=
@poller
fixeddelay
=

maxmessagesperpoll
=

messagesource
order
ordersource
{
{
order
o
=
order
index++
ordertype
purchase
localdatetime
now
orderstatus

technology_17
shipment
logger
info
connector_11
order
+
o
genericmessage
o
}
}
@bean
alwayssampler
defaultsampler
{
alwayssampler
}
}
here’s
output
configuration
in
component_4
yml

technology_3
requirement_4
connector_6
bind
input
destination
ex
connector_6
in
binder
rabbit1
output
destination
ex
connector_6
out
binder
rabbit1
binder
rabbit1
type
rabbit
environment
technology_3
technology_6
component_12




port

username
guest
password
guest
technology_17
and
shipment
component_18
use
processor

they
listen
on
connector_6
input
and
after
component_9
connector_11
connector_data_1
to
their
output
@springbootapplication
@enablebinding
processor

component_4
{
@autowired
private
productservice
productservice
protect
logger
logger
=
logger
getlogger
component_4

getname


args
{
springapplication
run
component_4

args
}
@streamlistener
processor
input
@sendto
processor
output
order
processorder
order
order
{
logger
info
component_9
order
+
order
productservice
processorder
order
}
@bean
alwayssampler
defaultsampler
{
alwayssampler
}
}
here’s
component_2
configuration
it
listen
on
order
component_2
output
exchange
and
also
define
it
group
name
technology_17
that
group
name
will
be
use
for
automatic
component_20
creation
and
exchange
bind
on
technology_6
there
be
also
output
exchange
define
technology_3
requirement_4
connector_6
bind
input
destination
ex
connector_6
out
group
technology_17
binder
rabbit1
output
destination
ex
connector_6
out2
binder
rabbit1
binder
rabbit1
type
rabbit
environment
technology_3
technology_6
component_12




port

username
guest
password
guest
we
use
technology_15
container
for
run
technology_6
instance
technology_15
run
technology_21
name
rabbit1
p


p


technology_6
requirement_5
let’s
look
at
the
requirement_5
console
it’s
quality_attribute_5
on
technology_18





here’s
ex
connector_6
out
topic
exchange
configuration
below
we
see
the
connector_data_4
of
declare
component_20
here’s
component_4
from
payment
component_2
we
use
connector_23
for
listen
on
incoming
connector_data_1
input
order
be
component_9
and
we
final
requirement_10
of
order
connector_10
by
order
component_2
sample
component_4
component_11
be
quality_attribute_5
on
technology_25
@springbootapplication
@enablebinding
connector_23

component_4
{
@autowired
private
paymentservice
paymentservice
protect
logger
logger
=
logger
getlogger
component_4

getname


args
{
springapplication
run
component_4

args
}
@streamlistener
connector_23
input
processorder
order
order
{
logger
info
component_9
order
+
order
order
o
=
paymentservice
processorder
order
if
o
=

logger
info
final
connector_17
+
o
getproduct
getprice
+
o
getshipment
getprice
}
@bean
alwayssampler
defaultsampler
{
alwayssampler
}
}
by
use
@bean
alwayssampler
in
every
of
our
pattern_2
we
propagate
one
trace
and
span
between
all
connector_data_12
of
single
order
here’s
fragment
from
our
pattern_2
requirement_9
console
and
also
i
connector_19
the
follow
warn
connector_data_1
which
be
not
quality_attribute_9
for
me
‘deprecated
trace

detected
please
upgrade
sleuth
to


or
start
connector_21

present
in
the
tracemessageheaders
class’
version



release
of
technology_3
requirement_4
sleuth
be
not
applicable
camden
sr5
release


how
to
ship
requirement_9
with
logstash
elasticsearch
and
technology_6

on

2017may


by
piotr
mińkowski
here’s
quality_attribute_2
picture
of
our
solution
we’ll
start
from
sample
technology_3
component_4
ship
requirement_9
to
technology_6
exchange
then
use
technology_15
we’ll
configure
environment
contain
technology_6
logstash
elasticsearch
and
kibana
–
each
run
on
separate
technology_15
container
my
sample
technology_2
component_4
be
quality_attribute_5
on
technology_18
technology_25
technology_1
piomin
sample
technology_35
requirement_9
git
there
be
only
two
technology_3
connector_20
need
inside
pom
technology_19
first
for
pattern_6
pattern_14
and
second
for
technology_36
connector_20
connector_20
connector_20


springframework
boot


technology_3
boot
starter
connector_data_2
rest

connector_20
connector_20


springframework
boot


technology_3
boot
starter
technology_35

connector_20
connector_20
here’s
quality_attribute_2
pattern_14
with
one
requirement_9
connector_data_1

slf4j
logger

slf4j
loggerfactory

springframework
web
bind
annotation
pathvariable

springframework
web
bind
annotation
requestmapping

springframework
web
bind
annotation
restcontroller
@restcontroller
pattern_14
{
protect
logger
logger
=
loggerfactory
getlogger
pattern_14

getname
@requestmapping
hello
{param}
hello
@pathvariable
param
param
{
logger
info
pattern_14
hello
+
param
+
hello
}
}
i
use
technology_37
a
logger
implementation
and
technology_3
technology_36
appender
for
connector_21
requirement_9
to
technology_6
over
technology_36
technology_34
appender
name=
technology_35
class=

springframework
technology_35
rabbit
technology_37
amqpappender
layout
pattern_11
{
time
%date{iso8601}
component_32
%thread
level
%level

%logger{36}
connector_data_1
%message
}
pattern_11
layout
technology_6
connector_46
component_12




component_12
port

port
username
guest
username
password
guest
password
applicationid
technology_23
component_2

applicationid
routingkeypattern
technology_23
component_2

routingkeypattern
declareexchange
true
declareexchange
exchangetype
direct
exchangetype
exchangename
ex_logstash
exchangename
generateid
true
generateid
charset
utf

charset
quality_attribute_10
true
quality_attribute_10
deliverymode
persistent
deliverymode
appender
i
run
technology_6
component_7
use
technology_15
image
technology_18
hub
technology_15
technology_1
_
technology_6
here’s
technology_15
command
for
it
i
choose
technology_6
requirement_5
technology_15
image
to
enable
connector_50
of
technology_6
ui
requirement_5
console
on
port

after
run
this
command
we
can
go
to
requirement_5
console
quality_attribute_5
on





there
we
have
to
create
component_20
name
q_logstash
and
direct
exchange
name
ex_logstach
have
connector_53
set
to
q_logstash
component_20
technology_15
run
technology_21
it
name
rabbit
hostname
rabbit
p


p


technology_6
requirement_5
technology_6
requirement_5
console
with
exchange
and
component_20
bind
then
we
run
elasticsearch
and
kibana
technology_15
image
kibana
container
need
to
be
connector_51
to
elasticsearch
technology_15
run
technology_21
it
name
e
p


p


elasticsearch
technology_15
run
technology_21
it
name
kibana
connector_51
e
elasticsearch
p


kibana
finally
we
can
run
logstash
technology_15
image
which
connector_19
technology_6
component_20
a
input
and
set
elasticsearch
technology_23
a
output
we
have
to
connector_54
component_12
to
technology_15
component_13
default
connector_30
and
port
configure
when
run
technology_6
container
also
we
have
quality_attribute_10
component_20
so
it
have
to
be
connector_54
because
default
requirement_3
for
that
be
false
follow
this
reference
technology_18
www
elastic
co
guide
en
logstash
current
plugins
input
technology_6
technology_38
technology_15
run
technology_21
it
name
logstash
logstash
e
input
{
technology_6
{
component_12
=




port
=

quality_attribute_10
=
true
}
}
output
{
elasticsearch
{
component_34
=




}
}
after
run
all
technology_15
container
for
technology_6
logstash
elasticsearch
and
kibana
we
can
run
our
sample
technology_3
component_4
and
see
requirement_9
on
kibana
quality_attribute_5
on
technology_18





tag
technology_15
elk
technology_2
kibana
pattern_2
technology_6
technology_3
technology_3
bootleave
a
follow
me
twitter
technology_24
linkedin
youtube
follow
via
enter
your
connector_30
to
follow
this
and
connector_37
connector_data_13
of

by

connector_30
follow
tagsconsul
continuous
delivery
continuous
requirement_2
technology_15
eureka
istio
technology_2
technology_39
jib
technology_40
technology_4
technology_31
micronaut
pattern_2
minikube
technology_41
requirement_11
pattern_27
quality_attribute_11
component_2
discovery
skaffold
technology_3
technology_3
technology_3
actuator
technology_3
requirement_4
technology_3
requirement_4
gateway
technology_3
requirement_4
technology_4
technology_3
connector_data_2
testcontainers
test
search
for
top

&
pagesintroduction
to
blockchain
with
technology_2
use
ethereum
web3j
and
technology_3
bootreactive
elasticsearch
with
technology_3
bootsecure
discovery
with
technology_3
requirement_4
netflix
eurekaadvanced
pattern_2
with
technology_9
camelmicroservices
component_30
documentation
with
swagger2part

create
pattern_9
use
technology_3
requirement_4
eureka
and
zuulquick
guide
to
pattern_2
with
micronaut
frameworklogging
with
technology_3
and
elastic
stackhow
to
ship
requirement_9
with
logstash
elasticsearch
and
rabbitmqbest
practice
for
pattern_2
on
kubernetescategories
container

continuous
requirement_2

connector_data_2
grid

technology_42

technology_40

connector_data_1
pattern_1

micronaut

pattern_2

other

requirement_11

quality_attribute_11

serverless

technology_3

technology_3
requirement_4

archive









































twitter
google+
technology_24
technology_43
technology_1
follow
piotr
s
techblog
on
technology_43
technology_1
recent

quick
guide
to
pattern_2
with
quarkus
on
technology_41


guide
to
quarkus
on
technology_4


guide
to
quarkus
with
technology_40


intro
to
technology_41
component_2
mesh


development
on
technology_4
choose
a
component_21


at
technology_43
technology_1
follow
follow
piotr
s
techblog
join


other
follower
sign
me
up
already
have
a
technology_43
technology_1
account
requirement_9
in
now
piotr
s
techblog
customize
follow
follow
sign
up
requirement_9
in
report
this
content
pattern_28
in
reader
manage
subscription
collapse
this
bar
loading

connector_4
a

require
name
require
