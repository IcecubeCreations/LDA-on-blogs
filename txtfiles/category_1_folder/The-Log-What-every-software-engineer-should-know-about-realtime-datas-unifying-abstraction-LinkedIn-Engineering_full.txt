the
requirement_1
what
every
engineer
should
about
real
time
connector_data_1
s
unify
abstraction
|
linkedin
engineering
linkedin
engineering
engineering
home
connector_data_1
open_source
trust
infrastructure
search
clear
search
input
cancel
dismiss
this
connector_data_2
the
requirement_1
what
every
engineer
should
about
real
time
connector_data_1
s
unify
abstraction
jay
kreps


connector_1
i
join
linkedin
about
six
year
ago
at
a
particularly
interest
time
we
be
begin
to
run
up
against
the
limit
of
our
monolithic
centralized
component_1
and
need
to
start
the
transition
to
a
portfolio
of
specialize
quality_attribute_1
component_2
this
have
be
an
interest
experience
we
build
quality_attribute_2
and
run
to
this
day
a
quality_attribute_1
graph
component_1
a
quality_attribute_1
search
backend
a
technology_1
installation
and
a
first
and
second
generation
key
requirement_2
component_3
one
of
the
most
useful
thing
i

in
all
this
be
that
many
of
the
thing
we
be
build
have
a
very
quality_attribute_3
concept
at
their
heart
the
requirement_1
sometimes
connector_2
connector_3
ahead
requirement_1
or
connector_4
requirement_1
or
transaction
requirement_1
requirement_1
have
be
around
almost
a
long
a
component_4
and
be
at
the
heart
of
many
quality_attribute_1
connector_data_1
component_5
and
real
time
component_6
architecture
you
can
t
fully
understand
component_1
technology_2
component_3
key
requirement_2
component_3
pattern_1
paxos
technology_1
version
control
or
almost
any
component_2
without
understand
requirement_1
and
yet
most
engineer
be
not
familiar
with
them
i
technology_3
to
connector_5
that
in
this

i
ll
walk
you
through
everything
you
need
to
about
requirement_1
include
what
be
requirement_1
and
how
to
use
requirement_1
for
connector_data_1
requirement_3
real
time
component_7
and
component_2
build
part
one
what
be
a
requirement_1
a
requirement_1
be
perhaps
the
quality_attribute_3
possible
storage
abstraction
it
be
an
append
only
totally
order
sequence
of
component_8
order
by
time
it
look
this
component_8
be
append
to
the
end
of
the
requirement_1
and
connector_6
proceed
leave
to
right
each
entry
be
assign
a
unique
sequential
requirement_1
entry
number
the
order
of
component_8
define
a
notion
of
time
since
entry
to
the
leave
be
define
to
be
old
then
entry
to
the
right
the
requirement_1
entry
number
can
be
think
of
a
the
pattern_2
of
the
entry
describe
this
order
a
a
notion
of
time
seem
a
bit
odd
at
first
but
it
have
the
convenient
property
that
it
be
decouple
from
any
particular
physical
clock
this
property
will
turn
out
to
be
essential
a
we
connector_7
to
quality_attribute_1
component_2
the
content
and
technology_4
of
the
component_8
aren
t
important
for
the
purpose
of
this
discussion
also
we
can
t
keep

component_8
to
the
requirement_1
a
we
ll
eventually
run
out
of
space
i
ll
come
back
to
this
in
a
bit
so
a
requirement_1
be
not
all
that
different
from
a
or
a
component_9
a
be
an
of
byte
a
component_9
be
an
of
component_10
and
a
requirement_1
be
really
a
kind
of
component_9
or
where
the
component_8
be
sort
by
time
at
this
point
you
might
be
wonder
why
it
be
worth
talk
about
something
so
quality_attribute_3
how
be
a
append
only
sequence
of
component_8
in
any
way
relate
to
connector_data_1
component_2
the
answer
be
that
requirement_1
have
a
specific
purpose
they
component_10
what
happen
and
when
for
quality_attribute_1
connector_data_1
component_5
this
be
in
many
way
the
very
heart
of
the
problem
but
before
we
connector_7
too
far
me
clarify
something
that
be
a
bit
confuse
every
programmer
be
familiar
with
another
definition
of
logging—the
pattern_3
error
connector_data_3
or
trace
info
an
component_6
might
connector_3
out
to
a
local
use
syslog
or
technology_5
for
clarity
i
will
connector_data_4
this
component_6
requirement_1
the
component_6
requirement_1
be
a
degenerative
form
of
the
requirement_1
concept
i
be
describe
the
big
difference
be
that
text
requirement_1
be
mean
to
be
primarily
for
human
to
connector_8
and
the
journal
or
connector_data_1
requirement_1
i
m
describe
be
build
for
programmatic
connector_9
actually
if
you
think
about
it
the
idea
of
human
connector_10
through
requirement_1
on
individual
component_11
be
something
of
an
anachronism
this
approach
quickly
become
an
unmanageable
strategy
when
many
component_12
and
component_13
be
involve
and
the
purpose
of
requirement_1
quickly
become
a
an
input
to
connector_11
and
graph
to
understand
behavior
across
many
machines—something
for
which
english
text
in
be
not
nearly
a
appropriate
a
the
kind
pattern_4
requirement_1
describe
here
requirement_1
in
component_1
i

t
where
the
requirement_1
concept
originated—probably
it
be
one
of
those
thing
binary
search
that
be
too
quality_attribute_3
for
the
inventor
to
realize
it
be
an
invention
it
be
present
a
early
a

s
component_2
r
the
usage
in
component_1
have
to
do
with
keep
in
pattern_5
the
variety
of
connector_data_1
connector_data_5
and
index
in
the
presence
of
crash
to
make
this
atomic
and
quality_attribute_4
a
component_1
us
a
requirement_1
to
connector_3
out
connector_data_6
about
the
component_8
they
will
be
modify
before
apply
the
connector_12
to
all
the
various
connector_data_1
connector_data_5
it
maintain
the
requirement_1
be
the
component_10
of
what
happen
and
each
component_9
or
index
be
a
projection
of
this
history
into
some
useful
connector_data_1
connector_data_7
or
index
since
the
requirement_1
be
immediately
persist
it
be
use
a
the
authoritative
component_14
in
restore
all
other
persistent
connector_data_5
in
the
of
a
crash
over
time
the
usage
of
the
requirement_1
grow
from
an
implementation
detail
of
acid
to
a
for
replicate
connector_data_1
between
component_1
it
turn
out
that
the
sequence
of
connector_12
that
happen
on
the
component_1
be
exactly
what
be
need
to
keep
a
remote
replica
component_1
in
pattern_5
technology_6
technology_7
and
technology_8
include
requirement_1
ship
technology_9
to
connector_13
portion
of
requirement_1
to
replica
component_1
which
act
a
slave
technology_6
have
productized
the
requirement_1
a
a
general
connector_data_1
subscription
mechanism
for
non
technology_6
connector_data_1
pattern_6
with
their
xstreams
and
goldengate
and
similar
facility
in
technology_7
and
technology_8
be
key
component_15
of
many
connector_data_1
architecture
because
of
this
origin
the
concept
of
a
component_16
readable
requirement_1
have
largely
be
confine
to
component_1
internals
the
use
of
requirement_1
a
a
mechanism
for
connector_data_1
subscription
seem
to
have
arise
almost
by
chance
but
this
very
abstraction
be
ideal
for
support
all
kind
of
connector_data_2
connector_data_1
flow
and
real
time
connector_data_1
component_7
requirement_1
in
quality_attribute_1
component_5
the
two
problem
a
requirement_1
solves—ordering
connector_12
and
quality_attribute_1
data—are
even
more
important
in
quality_attribute_1
connector_data_1
component_2
agree
upon
an
order
for
connector_data_8
or
agree
to
disagree
and
cop
with
the
side
effect
be
among
the
core
design
problem
for
these
component_2
the
requirement_1
centric
approach
to
quality_attribute_1
component_5
arise
from
a
quality_attribute_3
observation
that
i
will
connector_data_4
the
state
component_16
pattern_1
principle
if
two
identical
deterministic
component_17
begin
in
the
same
state
and
connector_7
the
same
input
in
the
same
order
they
will
produce
the
same
output
and
end
in
the
same
state
this
seem
a
bit
obtuse
so

s
dive
in
and
understand
what
it
mean
deterministic
mean
that
the
component_7
isn
t
time
dependent
and
doesn
t
any
other
out
of
band
input
influence
it
connector_data_9
for
example
a
component_18
whose
output
be
influence
by
the
particular
order
of
connector_14
of
component_19
or
by
a
connector_data_4
to
gettimeofday
or
some
other
non
quality_attribute_5
thing
be
generally
best
consider
a
non
deterministic
the
state
of
the
component_7
be
whatever
connector_data_1
remain
on
the
component_16
either
in
memory
or
on
disk
at
the
end
of
the
component_7
the
bit
about
connector_15
the
same
input
in
the
same
order
should
ring
a
bell—that
be
where
the
requirement_1
come
in
this
be
a
very
intuitive
notion
if
you
fee
two
deterministic
piece
of
the
same
input
requirement_1
they
will
produce
the
same
output
the
component_6
to
quality_attribute_1
computing
be
pretty
obvious
you
can
reduce
the
problem
of
make
multiple
component_11
all
do
the
same
thing
to
the
problem
of
connector_16
a
quality_attribute_1
consistent
requirement_1
to
fee
these
component_17
input
the
purpose
of
the
requirement_1
here
be
to
squeeze
all
the
non
determinism
out
of
the
input
connector_17
to
ensure
that
each
replica
component_7
this
input
stay
in
pattern_5
when
you
understand
it
there
be
nothing
complicate
or
deep
about
this
principle
it
more
or
le
amount
to
say
deterministic
component_7
be
deterministic
nonetheless
i
think
it
be
one
of
the
more
general
technology_10
for
quality_attribute_1
component_5
design
one
of
the
beautiful
thing
about
this
approach
be
that
the
time
stamp
that
index
the
requirement_1
now
act
a
the
clock
for
the
state
of
the
replicas—you
can
describe
each
replica
by
a
single
number
the
pattern_2
for
the
maximum
requirement_1
entry
it
have
component_7
this
pattern_2
combine
with
the
requirement_1
uniquely
capture
the
entire
state
of
the
replica
there
be
a
multitude
of
way
of
apply
this
principle
in
component_5
quality_attribute_6
on
what
be
put
in
the
requirement_1
for
example
we
can
requirement_1
the
incoming
connector_data_10
to
a
component_20
or
the
state
connector_12
the
component_20
undergo
in
connector_18
to
connector_data_11
or
the
transformation
command
it
connector_19
theoretically
we
could
even
requirement_1
a
series
of
component_16
instruction
for
each
replica
to
connector_19
or
the
name
and
argument
to
invoke
on
each
replica
a
long
a
two
component_17
component_7
these
input
in
the
same
way
the
component_17
will
remain
consistent
across
replica
different
group
of
people
seem
to
describe
the
us
of
requirement_1
differently
component_1
people
generally
differentiate
between
physical
and
logical
requirement_1
physical
requirement_1
mean
requirement_1
the
content
of
each
row
that
be
connector_5
logical
requirement_1
mean
requirement_1
not
the
connector_5
row
but
the
technology_11
command
that
lead
to
the
row
connector_12
the
insert
update
and
delete
statement
the
quality_attribute_1
component_5
literature
commonly
distinguish
two
broad
approach
to
component_7
and
pattern_1
the
state
component_16
component_21
usually
refer
to
an
active
active
component_21
where
we
keep
a
requirement_1
of
the
incoming
connector_data_10
and
each
replica
component_17
each
connector_data_11
a
slight
modification
of
this
connector_2
the
primary
backup
component_21
be
to
elect
one
replica
a
the
leader
and
allow
this
leader
to
component_7
connector_data_10
in
the
order
they
arrive
and
requirement_1
out
the
connector_12
to
it
state
from
component_7
the
connector_data_11
the
other
replica
apply
in
order
the
state
connector_12
the
leader
make
so
that
they
will
be
in
pattern_5
and
ready
to
take
over
a
leader
should
the
leader
fail
to
understand
the
difference
between
these
two
approach

s
look
at
a
toy
problem
consider
a
replicate
arithmetic
component_20
which
maintain
a
single
number
a
it
state
initialize
to
zero
and
apply
addition
and
multiplication
to
this
requirement_2
the
active
active
approach
might
requirement_1
out
the
transformation
to
apply
say
+1
*2
etc
each
replica
would
apply
these
transformation
and
hence
go
through
the
same
set
of
requirement_2
the
active
passive
approach
would
have
a
single
master
connector_19
the
transformation
and
requirement_1
out
the
connector_data_9
say



etc
this
example
also
make
it
clear
why
order
be
key
for
ensure
consistency
between
replica
reorder
an
addition
and
multiplication
will
yield
a
different
connector_data_9
the
quality_attribute_1
requirement_1
can
be
see
a
the
connector_data_1
connector_data_7
which
component_22
the
problem
of
consensus
a
requirement_1
after
all
represent
a
series
of
decision
on
the
next
requirement_2
to
append
you
have
to
squint
a
little
to
see
a
requirement_1
in
the
paxos
family
of
algorithm
though
requirement_1
build
be
their
most
common
practical
component_6
with
paxos
this
be
usually
do
use
an
extension
of
the
technology_9
connector_2
multi
paxos
which
component_22
the
requirement_1
a
a
series
of
consensus
problem
one
for
each
slot
in
the
requirement_1
the
requirement_1
be
much
more
prominent
in
other
technology_9
such
a
zab
raft
and
viewstamped
pattern_1
which
directly
component_21
the
problem
of
maintain
a
quality_attribute_1
consistent
requirement_1
my
suspicion
be
that
our
pattern_7
of
this
be
a
little
bit
bias
by
the
path
of
history
perhaps
due
to
the
few
decade
in
which
the
theory
of
quality_attribute_1
computing
outpace
it
practical
component_6
in
reality
the
consensus
problem
be
a
bit
too
quality_attribute_3
component_23
component_5
rarely
need
to
decide
a
single
requirement_2
they
almost
always
handle
a
sequence
of
connector_data_11
so
a
requirement_1
rather
than
a
quality_attribute_3
single
requirement_2
register
be
the
more
natural
abstraction
furthermore
the
focus
on
the
algorithm
obscure
the
underlie
requirement_1
abstraction
component_5
need
i
suspect
we
will
end
up
focus
more
on
the
requirement_1
a
a
commoditized
build
block
irrespective
of
it
implementation
in
the
same
way
we
often
talk
about
a
hash
component_9
without
bother
to
connector_7
in
the
detail
of
whether
we
mean
the
murmur
hash
with
linear
probe
or
some
other
variant
the
requirement_1
will
become
something
of
a
commoditized

with
many
algorithm
and
implementation
compete
to
provide
the
best
guarantee
and
optimal
requirement_4
changelog

component_24
and
be
dual

s
come
back
to
component_1
for
a
bit
there
be
a
facinating
duality
between
a
requirement_1
of
connector_12
and
a
component_9
the
requirement_1
be
similar
to
the
connector_data_12
of
all
credit
and
debit
and
bank
component_7
a
component_9
be
all
the
current
account
balance
if
you
have
a
requirement_1
of
connector_5
you
can
apply
these
connector_12
in
order
to
create
the
component_9
capture
the
current
state
this
component_9
will
component_10
the
late
state
for
each
key
a
of
a
particular
requirement_1
time
there
be
a
sense
in
which
the
requirement_1
be
the
more
fundamental
connector_data_1
connector_data_7
in
addition
to
create
the
original
component_9
you
can
also
transform
it
to
create
all
kind
of
derive
component_9
and
yes
component_9
can
mean
key
connector_data_1
component_3
for
the
non
relational
folk
this
component_7
work
in
reverse
too
if
you
have
a
component_9
take
update
you
can
component_10
these
connector_12
and
publish
a
changelog
of
all
the
connector_data_8
to
the
state
of
the
component_9
this
changelog
be
exactly
what
you
need
to
support
near
real
time
replica
so
in
this
sense
you
can
see
component_24
and
a
dual
component_24
support
connector_data_1
at
rest
and
requirement_1
capture
connector_5
the
magic
of
the
requirement_1
be
that
if
it
be
a
complete
requirement_1
of
connector_5
it
hold
not
only
the
content
of
the
final
version
of
the
component_9
but
also
allow
recreate
all
other
version
that
might
have
exist
it
be
effectively
a
sort
of
backup
of
every
previous
state
of
the
component_9
this
might
remind
you
of
component_14
version
control
there
be
a
close
relationship
between
component_14
control
and
component_1
version
control
solve
a
very
similar
problem
to
what
quality_attribute_1
connector_data_1
component_5
have
to
solve—managing
quality_attribute_1
concurrent
connector_12
in
state
a
version
control
component_2
usually
component_22
the
sequence
of
patch
which
be
in
effect
a
requirement_1
you
connector_20
directly
with
a
connector_21
out
snapshot
of
the
current
which
be
analogous
to
the
component_9
you
will
note
that
in
version
control
component_2
a
in
other
quality_attribute_1
stateful
component_2
pattern_1
happen
via
the
requirement_1
when
you
update
you
connector_22
down
the
patch
and
apply
them
to
your
current
snapshot
some
people
have
see
some
of
these
idea
recently
from
datomic
a
requirement_5
sell
a
requirement_1
centric
component_1
this
presentation
give
a
great
overview
of
how
they
have
apply
the
idea
in
their
component_2
these
idea
be
not
unique
to
this
component_2
of

a
they
have
be
a
part
of
the
quality_attribute_1
component_5
and
component_1
literature
for
well
over
a
decade
this
all
seem
a
little
theoretical
do
not
despair
we
ll
connector_7
to
practical
stuff
pretty
quickly
what
s
next
in
the
remainder
of
this
i
will
try
to
give
a
flavor
of
what
a
requirement_1
be
quality_attribute_7
for
that
go
beyond
the
internals
of
quality_attribute_1
computing
or
abstract
quality_attribute_1
computing
component_21
this
include
connector_data_1
integration—making
all
of
an
organization
s
connector_data_1
easily
quality_attribute_8
in
all
it
storage
and
component_7
component_2
real
time
connector_data_1
processing—computing
derive
connector_data_1
connector_17
quality_attribute_1
component_2
design—how
practical
component_5
can
by
simplify
with
a
requirement_1
centric
design
these
us
all
resolve
around
the
idea
of
a
requirement_1
a
a
stand
alone
component_20
in
each
requirement_6
the
usefulness
of
the
requirement_1
come
from
quality_attribute_3
that
the
requirement_1
provide
produce
a
persistent
re
playable
component_10
of
history
surprisingly
at
the
core
of
these
problem
be
the
ability
to
have
many
component_11
playback
history
at
their
own
rate
in
a
deterministic
manner
part
two
connector_data_1
requirement_3
me
first
say
what
i
mean
by
connector_data_1
requirement_3
and
why
i
think
it
s
important
then
we
ll
see
how
it
relate
back
to
requirement_1
connector_data_1
requirement_3
be
make
all
the
connector_data_1
an
organization
have
quality_attribute_8
in
all
it
component_12
and
component_2
this
phrase
connector_data_1
requirement_3
isn
t
all
that
common
but
i

t
a
quality_attribute_7
one
the
more
recognizable
term
technology_12
usually
cover
only
a
limit
part
of
connector_data_1
integration—populating
a
relational
connector_data_1
requirement_7
but
much
of
what
i
be
describe
can
be
think
of
a
technology_12
generalize
to
cover
real
time
component_5
and
component_7
flow
you

t
hear
much
about
connector_data_1
requirement_3
in
all
the
breathless
interest
and
hype
around
the
idea
of
requirement_8
but
nonetheless
i
believe
this
mundane
problem
of
make
the
connector_data_1
quality_attribute_8
be
one
of
the
more
valuable
thing
an
organization
can
focus
on
quality_attribute_9
use
of
connector_data_1
follow
a
kind
of
maslow
s
hierarchy
of
need
the
base
of
the
pyramid
involve
capture
all
the
relevant
connector_data_1
be
able
to
put
it
together
in
an
applicable
component_7
environment
be
that
a
fancy
real
time
query
component_2
or
text
and
technology_13
script
this
connector_data_1
need
to
be
component_21
in
a
uniform
way
to
make
it
easy
to
connector_8
and
component_7
once
these
basic
need
of
capture
connector_data_1
in
a
uniform
way
be
take
care
of
it
be
reasonable
to
work
on
infrastructure
to
component_7
this
connector_data_1
in
various
ways—mapreduce
real
time
query
component_2
etc
it
s
worth
note
the
obvious
without
a
quality_attribute_10
and
complete
connector_data_1
flow
a
technology_1
cluster
be
little
more
than
a
very
expensive
and
difficult
to
assemble
space
heater
once
connector_data_1
and
component_7
be
quality_attribute_8
one
can
move
concern
on
to
more
refine
problem
of
quality_attribute_7
connector_data_1
component_22
and
consistent
well
understand
semantics
finally
concentration
can
shift
to
more
sophisticate
processing—better
visualization
report
and
algorithmic
component_7
and
prediction
in
my
experience
most
organization
have
huge
hole
in
the
base
of
this
pyramid—they
lack
quality_attribute_10
complete
connector_data_1
flow—but
want
to
jump
directly
to
advance
connector_data_1
component_21
technique
this
be
completely
backwards
so
the
question
be
how
can
we
build
quality_attribute_10
connector_data_1
flow
throughout
all
the
connector_data_1
component_5
in
an
organization
connector_data_1
requirement_3
two
complication
two
trend
make
connector_data_1
requirement_3
hard
the
connector_data_1
firehose
the
first
trend
be
the
rise
of
connector_data_1
connector_data_1
component_8
thing
that
happen
rather
than
thing
that
be
in
web
component_2
this
mean
component_25
activity
requirement_1
but
also
the
component_16
level
and
statistic
require
to
quality_attribute_11
operate
and
pattern_8
a
connector_data_1
center
s
worth
of
component_16
people
tend
to
connector_data_4
this
requirement_1
connector_data_1
since
it
be
often
connector_3
to
component_6
requirement_1
but
that
confuse
form
with

this
connector_data_1
be
at
the
heart
of
the
modern
web

s
fortune
after
all
be
generate
by
a
relevance
pipeline
build
on
click
and
impressions—that
be

and
this
stuff
isn
t
limit
to
web
requirement_5
it
s
that
web
requirement_5
be
already
fully
digital
so
they
be
easy
to
instrument
financial
connector_data_1
have
long
be

centric
rfid

this
kind
of
track
to
physical
connector_data_13
i
think
this
trend
will
continue
with
the
digitization
of
traditional
requirement_9
and
activity
this
type
of
connector_data_1
component_8
what
happen
and
tend
to
be
several
order
of
magnitude
large
than
traditional
component_1
us
this
present
significant
challenge
for
component_7
the
explosion
of
specialize
connector_data_1
component_5
the
second
trend
come
from
the
explosion
of
specialize
connector_data_1
component_5
that
have
become
popular
and
often
freely
quality_attribute_8
in
the
last
five
year
specialize
component_5
exist
for
technology_14
search
quality_attribute_3
online
storage
pattern_9
component_7
graph
analysis
and
so
on
the
combination
of
more
connector_data_1
of
more
variety
and
a
desire
to
connector_7
this
connector_data_1
into
more
component_5
lead
to
a
huge
connector_data_1
requirement_3
problem
requirement_1
pattern_4
connector_data_1
flow
the
requirement_1
be
the
natural
connector_data_1
connector_data_7
for
handle
connector_data_1
flow
between
component_2
the
recipe
be
very
quality_attribute_3
take
all
the
organization
s
connector_data_1
and
put
it
into
a
central
requirement_1
for
real
time
subscription
each
logical
connector_data_1
component_14
can
be
component_21
a
it
own
requirement_1
a
connector_data_1
component_14
could
be
an
component_6
that
requirement_1
out
say
click
or
component_26
pattern_7
or
a
component_1
component_9
that
connector_23
modification
each
subscribe
component_2
connector_6
from
this
requirement_1
a
quickly
a
it
can
apply
each
component_10
to
it
own
component_3
and
advance
it
position
in
the
requirement_1
pattern_6
could
be
any
kind
of
connector_data_1
system—a
pattern_10
technology_1
another
component_1
in
another

a
search
component_2
etc
for
example
the
requirement_1
concept
give
a
logical
clock
for
each
connector_5
against
which
all
pattern_6
can
be
measure
this
make
reason
about
the
state
of
the
different
pattern_6
component_5
with
respect
to
one
another
far
quality_attribute_3
a
each
have
a
point
in
time
they
have
connector_8
up
to
to
make
this
more
concrete
consider
a
quality_attribute_3
requirement_6
where
there
be
a
component_1
and
a
collection
of
pattern_10
component_27
the
requirement_1
provide
a
way
to
synchronize
the
connector_data_8
to
all
these
component_5
and
reason
about
the
point
of
time
of
each
of
these
component_2

s
say
we
connector_3
a
component_10
with
requirement_1
entry
x
and
then
need
to
do
a
connector_8
from
the
pattern_10
if
we
want
to
guarantee
we

t
see
stale
connector_data_1
we
need
to
ensure
we

t
connector_8
from
any
pattern_10
which
have
not
replicate
up
to
x
the
requirement_1
also
act
a
a
buffer
that
make
connector_data_1
production
pattern_11
from
connector_data_1
consumption
this
be
important
for
a
lot
of
reason
but
particularly
when
there
be
multiple
pattern_6
that
connector_24
at
different
rat
this
mean
a
subscribe
component_2
can
crash
or
go
down
for
quality_attribute_12
and
catch
up
when
it
come
back
the
pattern_6
connector_25
at
a
pace
it
control
a
pattern_9
component_2
such
a
technology_1
or
a
connector_data_1
requirement_7
connector_24
only
hourly
or
daily
whereas
a
real
time
query
component_2
need
to
be
up
to
the
second
neither
the
originate
connector_data_1
component_14
nor
the
requirement_1
have
knowledge
of
the
various
connector_data_1
destination
component_2
so
component_28
component_5
can
be

and
remove
with
no
connector_5
in
the
pipeline
each
work
connector_data_1
pipeline
be
design
a
requirement_1
each
break
connector_data_1
pipeline
be
break
in
it
own
way
—count
leo
tolstoy
translation
by
the
author
of
particular
importance
the
destination
component_2
only

about
the
requirement_1
and
not
any
detail
of
the
component_2
of
origin
the
component_28
component_2
need
not
concern
itself
with
whether
the
connector_data_1
come
from
an
technology_15
a

fangled
key
requirement_2
component_3
or
be
generate
without
a
real
time
query
component_2
of
any
kind
this
seem
a
minor
point
but
be
in
fact
critical
i
use
the
term
requirement_1
here
instead
of
connector_data_2
component_2
or
pub
sub
because
it
be
a
lot
more
specific
about
semantics
and
a
much
close
description
of
what
you
need
in
a
practical
implementation
to
support
connector_data_1
pattern_1
i
have
find
that
publish
subscribe
doesn
t
imply
much
more
than
indirect
connector_26
of
messages—if
you
compare
any
two
pattern_12
component_5
promise
publish
subscribe
you
find
that
they
guarantee
very
different
thing
and
most
component_22
be
not
useful
in
this
domain
you
can
think
of
the
requirement_1
a
act
a
a
kind
of
pattern_12
component_2
with
quality_attribute_13
guarantee
and
strong
order
semantics
in
quality_attribute_1
component_2
this
component_21
of
connector_27
sometimes
go
by
the
somewhat
terrible
name
of
atomic
pattern_13
it
s
worth
emphasize
that
the
requirement_1
be
still
the
infrastructure
that
isn
t
the
end
of
the
story
of
master
connector_data_1
flow
the
rest
of
the
story
be
around
metadata
schema
quality_attribute_14
and
all
the
detail
of
handle
connector_data_1
connector_data_7
and
evolution
but
until
there
be
a
quality_attribute_10
general
way
of
handle
the
mechanic
of
connector_data_1
flow
the
semantic
detail
be
secondary
at
linkedin
i
connector_7
to
watch
this
connector_data_1
requirement_3
problem
emerge
in
fast
connector_28
a
linkedin
move
from
a
centralized
relational
component_1
to
a
collection
of
quality_attribute_1
component_2
these
day
our
major
connector_data_1
component_5
include
search
social
graph
voldemort
key
requirement_2
component_3
espresso
document
component_3
recommendation
component_29
technology_14
query
component_29
technology_1
terradata
ingraphs
pattern_8
graph
and
metric
component_20
each
of
these
be
a
specialize
quality_attribute_1
component_2
that
provide
advance
requirement_10
in
it
area
of
specialty
this
idea
of
use
requirement_1
for
connector_data_1
flow
have
be
float
around
linkedin
since
even
before
i
connector_7
here
one
of
the
early
piece
of
infrastructure
we
develop
be
a
component_20
connector_2
databus
that
provide
a
requirement_1
pattern_10
abstraction
on
top
of
our
early
technology_6
component_24
to
quality_attribute_15
subscription
to
component_1
connector_12
so
we
could
fee
our
social
graph
and
search
index
i
ll
give
a
little
bit
of
the
history
to
provide
component_30
my
own
involvement
in
this
start
around

after
we
have
ship
our
key
requirement_2
component_3
my
next
project
be
to
try
to
connector_7
a
work
technology_1
setup
go
and
move
some
of
our
recommendation
component_17
there
have
little
experience
in
this
area
we
naturally
budget
a
few
week
for
connector_15
connector_data_1
in
and
out
and
the
rest
of
our
time
for
connector_16
fancy
prediction
algorithm
so
begin
a
long
slog
we
originally
plan
to
scrape
the
connector_data_1
out
of
our
exist
technology_6
connector_data_1
requirement_7
the
first
discovery
be
that
connector_15
connector_data_1
out
of
technology_6
quickly
be
something
of
a
dark
art
bad
the
connector_data_1
requirement_7
component_7
be
not
appropriate
for
the
production
pattern_9
component_7
we
plan
for
hadoop—much
of
the
component_7
be
non
reversable
and
specific
to
the
report
be
do
we
end
up
avoid
the
connector_data_1
requirement_7
and
go
directly
to
component_14
component_1
and
requirement_1

finally
we
connector_29
another
pipeline
to
load
connector_data_1
into
our
key
requirement_2
component_3
for
serve
connector_data_9
this
mundane
connector_data_1
copy
end
up
be
one
of
the
dominate
connector_data_14
for
the
original
development
bad
any
time
there
be
a
problem
in
any
of
the
pipeline
the
technology_1
component_2
be
largely
useless—running
fancy
algorithm
on
bad
connector_data_1
produce
more
bad
connector_data_1
although
we
have
build
thing
in
a
fairly
generic
way
each
connector_data_1
component_14
require
custom
configuration
to
set
up
it
also
prove
to
be
the
component_14
of
a
huge
number
of
error
and
failure
the
feature
we
have
connector_29
on
technology_1
become
popular
and
we
find
ourselves
with
a
long
connector_data_12
of
interest
engineer
each
component_25
have
a
connector_data_12
of
component_5
they
want
requirement_3
with
and
a
long
connector_data_12
of
connector_data_1
feed
they
want
technology_12
in
ancient
greece
not
much
have
connector_5
a
few
thing
slowly
become
clear
to
me
first
the
pipeline
we
have
build
though
a
bit
of
a
mess
be
actually
extremely
valuable
the
component_7
of
make
connector_data_1
quality_attribute_8
in
a
component_7
component_2
technology_1
unlock
a
lot
of
possibility
computation
be
possible
on
the
connector_data_1
that
would
have
be
hard
to
do
before
many
technology_16
and
analysis
come
from
put
together
multiple
piece
of
connector_data_1
that
have
previously
be
lock
up
in
specialize
component_2
second
it
be
clear
that
quality_attribute_10
connector_data_1
load
would
require
deep
support
from
the
connector_data_1
pipeline
if
we
capture
all
the
connector_data_7
we
need
we
could
make
technology_1
connector_data_1
load
fully
automatic
so
that
no
manual
effort
be
expand

connector_data_1
component_31
or
handle
schema
changes—data
would
magically
appear
in
technology_17
and
technology_18
component_24
would
automatically
be
generate
for
connector_data_1
component_31
with
the
appropriate
column
third
we
still
have
very
low
connector_data_1
coverage
that
be
if
you
look
at
the
overall
percentage
of
the
connector_data_1
linkedin
have
that
be
quality_attribute_8
in
technology_1
it
be
still
very
incomplete
and
connector_15
to
completion
be
not
go
to
be
easy
give
the
amount
of
effort
require
to
operationalize
each
connector_data_1
component_14
the
way
we
have
be
proceed
build
out
custom
connector_data_1
load
for
each
connector_data_1
component_14
and
destination
be
clearly
infeasible
we
have
dozen
of
connector_data_1
component_5
and
connector_data_1
pattern_14
connector_30
all
of
these
would
have
lead
to
build
custom
pip
between
each
pair
of
component_5
something
this
note
that
connector_data_1
often
flow
in
both
direction
a
many
component_5
component_1
technology_1
be
both
component_31
and
destination
for
connector_data_1
transfer
this
mean
we
would
end
up
build
two
pipeline
per
component_2
one
to
connector_7
connector_data_1
in
and
one
to
connector_7
connector_data_1
out
this
clearly
would
take
an
army
of
people
to
build
and
would
never
be
quality_attribute_16
a
we
approach
fully
connector_31
we
would
end
up
with
something
o
n2
pipeline
instead
we
need
something
generic
this
a
much
a
possible
we
need
to
isolate
each
component_28
from
the
component_14
of
the
connector_data_1
they
should
ideally
quality_attribute_17
with
a
single
connector_data_1
pattern_14
that
would
give
them
connector_9
to
everything
the
idea
be
that

a
connector_data_1
system—be
it
a
connector_data_1
component_14
or
a
connector_data_1
destination—should
create
requirement_3
work
only
to
connector_30
it
to
a
single
pipeline
instead
of
each
component_28
of
connector_data_1
this
experience
lead
me
to
focus
on
build
technology_19
to
combine
what
we
have
see
in
pattern_12
component_5
with
the
requirement_1
concept
popular
in
component_1
and
quality_attribute_1
component_2
internals
we
want
something
to
act
a
a
central
pipeline
first
for
all
activity
connector_data_1
and
eventually
for
many
other
us
include
connector_data_1
deployment
out
of
technology_1
pattern_15
connector_data_1
etc
for
a
long
time
technology_19
be
a
little
unique
some
would
say
odd
a
an
infrastructure
product—neither
a
component_1
nor
a
requirement_1
collection
component_2
nor
a
traditional
pattern_12
component_2
but
recently
have
offer
a
component_20
that
be
very
very
similar
to
technology_19
connector_2
kinesis
the
similarity
go
right
down
to
the
way
partitioning
be
handle
connector_data_1
be
retain
and
the
fairly
odd
split
in
the
technology_19
component_32
between
high
and
low
level
component_28
i
be
pretty
happy
about
this
a
sign
you
ve
create
a
quality_attribute_7
infrastructure
abstraction
be
that
technology_20
offer
it
a
a
component_20
their
vision
for
this
seem
to
be
exactly
similar
to
what
i
be
describe
it
be
the
pip
that
connector_32
all
their
quality_attribute_1
systems—dynamodb
technology_21
technology_22
etc
—as
well
a
the
basis
for
quality_attribute_1
connector_17
component_7
use
technology_23
relationship
to
technology_12
and
the
connector_data_1
requirement_7

s
talk
connector_data_1
warehousing
for
a
bit
the
connector_data_1
requirement_7
be
mean
to
be
a
pattern_14
of
the
clean
quality_attribute_17
connector_data_1
pattern_4
to
support
analysis
this
be
a
great
idea
for
those
not
in
the

the
connector_data_1
warehousing
methodology
involve
periodically
extract
connector_data_1
from
component_14
component_1
munging
it
into
some
kind
of
understandable
form
and
loading
it
into
a
central
connector_data_1
requirement_7
have
this
central
location
that
contain
a
clean
copy
of
all
your
connector_data_1
be
a
hugely
valuable
asset
for
connector_data_1
intensive
analysis
and
component_7
at
a
high
level
this
methodology
doesn
t
connector_5
too
much
whether
you
use
a
traditional
connector_data_1
requirement_7
technology_6
or
technology_24
or
technology_1
though
you
might
switch
up
the
order
of
loading
and
munging
a
connector_data_1
requirement_7
contain
clean
quality_attribute_17
connector_data_1
be
a
phenomenal
asset
but
the
mechanic
of
connector_15
this
be
a
bit
out
of
date
the
key
problem
for
a
connector_data_1
centric
organization
be
couple
the
clean
quality_attribute_17
connector_data_1
to
the
connector_data_1
requirement_7
a
connector_data_1
requirement_7
be
a
piece
of
pattern_9
query
infrastructure
which
be
well
suit
to
many
kind
of
report
and
hoc
analysis
particularly
when
the
connector_11
involve
quality_attribute_3
count
aggregation
and
pattern_16
but
have
a
pattern_9
component_2
be
the
only
pattern_14
of
clean
complete
connector_data_1
mean
the
connector_data_1
be
unavailable
for
component_5
require
a
real
time
feed—real
time
component_7
search
index
pattern_15
component_2
etc
in
my
pattern_7
technology_12
be
really
two
thing
first
it
be
an
extraction
and
connector_data_1
cleanup
process—essentially
liberate
connector_data_1
lock
up
in
a
variety
of
component_5
in
the
organization
and
remove
an
component_2
specific
non
sense
secondly
that
connector_data_1
be
restructure
for
connector_data_1
warehousing
connector_11
i
e
make
to
fit
the
type
component_2
of
a
relational
db
force
into
a
star
or
schema
perhaps
break
up
into
a
high
requirement_4
column
technology_4
etc
conflate
these
two
thing
be
a
problem
the
clean
quality_attribute_17
pattern_14
of
connector_data_1
should
be
quality_attribute_8
in
real
time
a
well
for
low
quality_attribute_18
component_7
a
well
a
index
in
other
real
time
storage
component_2
i
think
this
have
the

benefit
of
make
connector_data_1
warehousing
technology_12
much
more
organizationally
quality_attribute_19
the
classic
problem
of
the
connector_data_1
requirement_7
team
be
that
they
be
responsible
for
connector_33
and
clean
all
the
connector_data_1
generate
by
every
other
team
in
the
organization
the
incentive
be
not
align
connector_data_1
component_33
be
often
not
very
aware
of
the
use
of
the
connector_data_1
in
the
connector_data_1
requirement_7
and
end
up
create
connector_data_1
that
be
hard
to
extract
or
require
heavy
hard
to
quality_attribute_15
transformation
to
connector_7
into
quality_attribute_20
form
of

the
central
team
never
quite
manage
to
quality_attribute_15
to
match
the
pace
of
the
rest
of
the
organization
so
connector_data_1
coverage
be
always
spotty
connector_data_1
flow
be
fragile
and
connector_12
be
slow
a
quality_attribute_7
approach
be
to
have
a
central
pipeline
the
requirement_1
with
a
well
define
component_32
for

connector_data_1
the
responsibility
of
quality_attribute_17
with
this
pipeline
and
provide
a
clean
well
pattern_4
connector_data_1
fee
lie
with
the
component_34
of
this
connector_data_1
fee
this
mean
that
a
part
of
their
component_2
design
and
implementation
they
must
consider
the
problem
of
connector_15
connector_data_1
out
and
into
a
well
pattern_4
form
for
delivery
to
the
central
pipeline
the
addition
of
storage
component_5
be
of
no
consequence
to
the
connector_data_1
requirement_7
team
a
they
have
a
central
point
of
requirement_3
the
connector_data_1
requirement_7
team
handle
only
the
quality_attribute_3
problem
of
loading
pattern_4
feed
of
connector_data_1
from
the
central
requirement_1
and
carry
out
transformation
specific
to
their
component_2
this
point
about
organizational
quality_attribute_21
become
particularly
important
when
one
consider
adopt
additional
connector_data_1
component_5
beyond
a
traditional
connector_data_1
requirement_7
say
for
example
that
one
wish
to
provide
search
capability
over
the
complete
connector_data_1
set
of
the
organization
or
say
that
one
want
to
provide
sub
second
pattern_15
of
connector_data_1
connector_34
with
real
time
trend
graph
and
alerting
in
either
of
these
requirement_6
the
infrastructure
of
the
traditional
connector_data_1
requirement_7
or
even
a
technology_1
cluster
be
go
to
be
inappropriate
bad
the
technology_12
component_7
pipeline
build
to
support
component_1
load
be
likely
of
no
use
for
feed
these
other
component_2
make
bootstrapping
these
piece
of
infrastructure
a
large
an
undertake
a
adopt
a
connector_data_1
requirement_7
this
likely
isn
t
feasible
and
probably
help
explain
why
most
organization
do
not
have
these
capability
easily
quality_attribute_8
for
all
their
connector_data_1
by
contrast
if
the
organization
have
build
out
feed
of
uniform
well
pattern_4
connector_data_1
connector_15
any
component_2
full
connector_9
to
all
connector_data_1
require
only
a
single
bit
of
requirement_3
plumb
to
attach
to
the
pipeline
this
architecture
also
raise
a
set
of
different
option
for
where
a
particular
cleanup
or
transformation
can
reside
it
can
be
do
by
the
connector_data_1
component_34
prior
to

the
connector_data_1
to
the
requirement_5
wide
requirement_1
it
can
be
do
a
a
real
time
transformation
on
the
requirement_1
which
in
turn
produce
a

transform
requirement_1
it
can
be
do
a
part
of
the
load
component_7
into
some
destination
connector_data_1
component_2
the
best
component_21
be
to
have
cleanup
do
prior
to
publish
the
connector_data_1
to
the
requirement_1
by
the
pattern_17
of
the
connector_data_1
this
mean
ensure
the
connector_data_1
be
in
a
canonical
form
and
doesn
t
retain
any
hold
over
from
the
particular
that
produce
it
or
the
storage
component_2
in
which
it
have
be
maintain
these
detail
be
best
handle
by
the
team
that
create
the
connector_data_1
since
they
the
most
about
their
own
connector_data_1
any
component_35
apply
in
this
stage
should
be
lossless
and
reversible
any
kind
of
requirement_2

transformation
that
can
be
do
in
real
time
should
be
do
a

component_7
on
the
raw
requirement_1
fee
produce
this
would
include
thing
sessionization
of
connector_data_1
or
the
addition
of
other
derive
that
be
of
general
interest
the
original
requirement_1
be
still
quality_attribute_8
but
this
real
time
component_7
produce
a
derive
requirement_1
contain
augment
connector_data_1
finally
only
aggregation
that
be
specific
to
the
destination
component_2
should
be
perform
a
part
of
the
loading
component_7
this
might
include
transform
connector_data_1
into
a
particular
star
or
schema
for
analysis
and
report
in
a
connector_data_1
requirement_7
because
this
stage
which
most
naturally
connector_data_15
to
the
traditional
technology_12
component_7
be
now
do
on
a
far
clean
and
more
uniform
set
of
connector_17
it
should
be
much
simplify
requirement_1
and

s
talk
a
little
bit
about
a
side
benefit
of
this
architecture
it
enable
decouple
pattern_18
component_2
the
typical
approach
to
activity
connector_data_1
in
the
web
requirement_11
be
to
requirement_1
it
out
to
text
where
it
can
be
scrap
into
a
connector_data_1
requirement_7
or
into
technology_1
for
aggregation
and
query
the
problem
with
this
be
the
same
a
the
problem
with
all
pattern_9
technology_12
it
couple
the
connector_data_1
flow
to
the
connector_data_1
requirement_7
s
capability
and
component_7
schedule
at
linkedin
we
have
build
our
connector_data_1
handle
in
a
requirement_1
centric
fashion
we
be
use
technology_19
a
the
central
multi
pattern_6
requirement_1
we
have
define
several
hundred
type
each
capture
the
unique
attribute
about
a
particular
type
of
action
this
cover
everything
from
component_26
pattern_7
impression
and
search
to
component_20
invocation
and
component_6
exception
to
understand
the
advantage
of
this
imagine
a
quality_attribute_3
event—showing
a

on
the
component_26
the
component_26
should
contain
only
the
component_35
require
to
display
the

however
in
a
fairly
dynamic

this
could
easily
become
lard
up
with
additional
component_35
unrelated
to
show
the

for
example

s
say
we
need
to
quality_attribute_17
the
follow
component_2
we
need
to
connector_35
this
connector_data_1
to
technology_1
and
connector_data_1
requirement_7
for
offline
component_7
purpose
we
need
to
count
the
pattern_7
to
ensure
that
the
viewer
be
not
attempt
some
kind
of
content
scrap
we
need
to
aggregate
this
pattern_7
for
display
in
the
poster
s
requirement_12
component_26
we
need
to
component_10
the
pattern_7
to
ensure
we
properly
impression
cap
any
recommendation
for
that
component_25
we

t
want
to
show
the
same
thing
over
and
over
our
recommendation
component_2
need
to
component_10
the
pattern_7
to
correctly
track
the
popularity
of
that
etc
pretty
soon
the
quality_attribute_3
act
of
display
a
have
become
quite
complex
and
a
we
other
place
where
be
displayed—mobile
component_6
and
so
on—this
component_35
must
be
carry
over
and
the
complexity
increase
bad
the
component_5
that
we
need
to
with
be
now
somewhat
intertwined—the
person
work
on
display
need
to
about
many
other
component_5
and
feature
and
make
sure
they
be
quality_attribute_17
properly
this
be
a
toy
version
of
the
problem
any
real
component_6
would
be
more
not
le
complex
the

drive
style
provide
an
approach
to
simplify
this
the
display
component_26
now
show
a
and
component_8
the
fact
that
a
be
show
along
with
the
relevant
attribute
of
the

the
viewer
and
any
other
useful
fact
about
the
display
of
the

each
of
the
other
interest
systems—the
recommendation
component_2
the
quality_attribute_22
component_2
the
poster
requirement_12
component_2
and
the
connector_data_1
warehouse—all
subscribe
to
the
fee
and
do
their
component_7
the
display
need
not
be
aware
of
these
other
component_2
and
needn
t
be
connector_5
if
a
connector_data_1
component_28
be

build
a
quality_attribute_19
requirement_1
of

separate
pattern_17
from
pattern_6
be
nothing

but
if
you
want
to
keep
a
connector_4
requirement_1
that
act
a
a
multi
pattern_6
real
time
journal
of
everything
happen
on
a
component_28
quality_attribute_15

quality_attribute_21
will
be
a
primary
challenge
use
a
requirement_1
a
a
universal
requirement_3
mechanism
be
never
go
to
be
more
than
an
elegant
fantasy
if
we
can
t
build
a
requirement_1
that
be
fast
cheap
and
quality_attribute_19
enough
to
make
this
practical
at
quality_attribute_15
component_5
people
typically
think
of
a
quality_attribute_1
requirement_1
a
a
slow
heavy
weight
abstraction
and
usually
associate
it
only
with
the
kind
of
metadata
us
for
which
technology_25
might
be
appropriate
but
with
a
thoughtful
implementation
focus
on
journaling
large
connector_data_1
connector_17
this
need
not
be
true
at
linkedin
we
be
currently
run
over

billion
unique
connector_data_2
connector_36
through
technology_19
per
day
several
hundred
billion
if
you
count
the
connector_36
from
mirror
between
datacenters
we
use
a
few
trick
in
technology_19
to
support
this
kind
of
quality_attribute_15
partitioning
the
requirement_1
optimize
quality_attribute_23
by
pattern_9
connector_6
and
connector_36
avoid
needle
connector_data_1
copy
in
order
to
allow
horizontal
quality_attribute_24
we
chop
up
our
requirement_1
into
component_36
each
component_36
be
a
totally
order
requirement_1
but
there
be
no
global
order
between
component_36
other
than
perhaps
some
wall
clock
time
you
might
include
in
your
connector_data_2
the
assignment
of
the
connector_data_3
to
a
particular
component_36
be
controllable
by
the
writer
with
most
component_37
choose
to
component_36
by
some
kind
of
key
e
g
component_25

partitioning
allow
requirement_1
append
to
occur
without
co
ordination
between
shard
and
allow
the
quality_attribute_23
of
the
component_2
to
quality_attribute_15
linearly
with
the
technology_19
cluster
size
each
component_36
be
replicate
across
a
quality_attribute_25
number
of
replica
each
of
which
have
an
identical
copy
of
the
component_36
s
requirement_1
at
any
time
a
single
one
of
them
will
act
a
the
leader
if
the
leader
fail
one
of
the
replica
will
take
over
a
leader
lack
of
a
global
order
across
component_36
be
a
limitation
but
we
have
not
find
it
to
be
a
major
one
indeed
connector_37
with
the
requirement_1
typically
come
from
hundred
or
thousand
of
distinct
component_17
so
it
be
not
meaningful
to
talk
about
a
total
order
over
their
behavior
instead
the
guarantee
that
we
provide
be
that
each
component_36
be
order
preserve
and
technology_19
guarantee
that
append
to
a
particular
component_36
from
a
single
sender
will
be
connector_38
in
the
order
they
be
connector_35
a
requirement_1
a
filesystem
be
easy
to
optimize
for
linear
connector_8
and
connector_3
pattern_19
the
requirement_1
can
group
small
connector_6
and
connector_36
together
into
large
high
quality_attribute_23

technology_19
pursue
this
optimization
aggressively
pattern_9
occur
from
component_38
to
component_27
when
connector_39
connector_data_1
in
connector_36
to
disk
in
pattern_1
between
component_27
in
connector_data_1
transfer
to
component_28
and
in
acknowledge
connector_4
connector_data_1
finally
technology_19
us
a
quality_attribute_3
binary
technology_4
that
be
maintain
between
in
memory
requirement_1
on
disk
requirement_1
and
in
requirement_13
connector_data_1
transfer
this
allow
u
to
make
use
of
numerous
optimization
include
zero
copy
connector_data_1
transfer
the
cumulative
effect
of
these
optimization
be
that
you
can
usually
connector_3
and
connector_8
connector_data_1
at
the
rate
support
by
the
disk
or
requirement_13
even
while
maintain
connector_data_1
set
that
vastly
exceed
memory
this
connector_3
up
isn
t
mean
to
be
primarily
about
technology_19
so
i
win
t
go
into
further
detail
you
can
connector_8
a
more
detail
overview
of
linkedin
s
approach
here
and
a
thorough
overview
of
technology_19
s
design
here
part
three
requirement_1
&
real
time
connector_17
component_7
so
far
i
have
only
describe
what
amount
to
a
fancy
of
copy
connector_data_1
from
place
to
place
but
shlepping
byte
between
storage
component_5
be
not
the
end
of
the
story
it
turn
out
that
requirement_1
be
another
word
for
connector_17
and
requirement_1
be
at
the
heart
of
connector_17
component_7
but
wait
what
exactly
be
connector_17
component_7
if
you
be
a
fan
of
late
90
and
early
2000s
component_1
literature
or
semi
successful
connector_data_1
infrastructure
technology_16
you
likely
associate
connector_17
component_7
with
effort
to
build
a
technology_11
component_29
or
component_39
and
arrow
for
drive
component_7
if
you
follow
the
explosion
of
open_source
connector_data_1
component_2
you
likely
associate
connector_17
component_7
with
some
of
the
component_5
in
this
space—for
example
storm
technology_26
s4
and
samza
but
most
people
see
these
a
a
kind
of
pattern_11
connector_data_2
component_7
component_2
not
that
different
from
a
cluster
aware
pattern_20
pattern_21
and
in
fact
some
thing
in
this
space
be
exactly
that
both
these
pattern_7
be
a
little
limit
connector_17
component_7
have
nothing
to
do
with
technology_27
nor
be
it
limit
to
real
time
component_7
there
be
no
inherent
reason
you
can
t
component_7
the
connector_17
of
connector_data_1
from
yesterday
or
a
month
ago
use
a
variety
of
different
technology_28
to
express
the
computation
i
see
connector_17
component_7
a
something
much
broad
infrastructure
for
continuous
connector_data_1
component_7
i
think
the
computational
component_21
can
be
a
general
a
mapreduce
or
other
quality_attribute_1
component_7
technology_29
but
with
the
ability
to
produce
low
quality_attribute_18
connector_data_9
the
real
driver
for
the
component_7
component_21
be
the
of
connector_data_1
collection
connector_data_1
which
be
connector_40
in
pattern_9
be
naturally
component_7
in
pattern_9
when
connector_data_1
be
connector_40
continuously
it
be
naturally
component_7
continuously
the
u
census
provide
a
quality_attribute_7
example
of
pattern_9
connector_data_1
collection
the
census
periodically
kick
off
and
do
a
brute
force
discovery
and
enumeration
of
u
citizen
by
have
people
walk
around
door
to
door
this
make
a
lot
of
sense
in

when
the
census
be
first
begin
connector_data_1
collection
at
the
time
be
inherently
pattern_9
orient
it
involve
rid
around
on
horseback
and
connector_41
down
component_8
on
paper
then
transport
this
pattern_9
of
component_8
to
a
central
location
where
human

up
all
the
count
these
day
when
you
describe
the
census
component_7
one
immediately
wonder
why
we

t
keep
a
journal
of
birth
and
death
and
produce
population
count
either
continuously
or
with
whatever
granularity
be
need
this
be
an
extreme
example
but
many
connector_data_1
transfer
component_17
still
quality_attribute_6
on
take
periodic
connector_42
and
bulk
transfer
and
requirement_3
the
only
natural
way
to
component_7
a
bulk
connector_data_16
be
with
a
pattern_9
component_7
but
a
these
component_17
be
replace
with
continuous
feed
one
naturally
start
to
move
towards
continuous
component_7
to
smooth
out
the
component_7
resource
need
and
reduce
quality_attribute_18
linkedin
for
example
have
almost
no
pattern_9
connector_data_1
collection
at
all
the
majority
of
our
connector_data_1
be
either
activity
connector_data_1
or
component_1
connector_5
both
of
which
occur
continuously
in
fact
when
you
think
about
any
requirement_9
the
underlie
mechanic
be
almost
always
a
continuous
process—events
happen
in
real
time
a
jack
bauer
would
tell
u
when
connector_data_1
be
connector_40
in
pattern_9
it
be
almost
always
due
to
some
manual
step
or
lack
of
digitization
or
be
a
historical
relic
leave
over
from
the
automation
of
some
non
digital
component_7
connector_43
and
technology_30
to
connector_data_1
use
to
be
very
slow
when
the
mechanic
be
mail
and
human
do
the
component_7
a
first
pass
at
automation
always
retain
the
form
of
the
original
component_7
so
this
often
linger
for
a
long
time
production
pattern_9
component_7
that
run
daily
be
often
effectively
mimic
a
kind
of
continuous
computation
with
a
window
size
of
one
day
the
underlie
connector_data_1
be
of

always
connector_5
these
be
actually
so
common
at
linkedin
and
the
mechanic
of
make
them
work
in
technology_1
so
tricky
that
we
connector_29
a
whole
technology_29
for
manage
incremental
technology_1
workflow
see
in
this
light
it
be
easy
to
have
a
different
pattern_7
of
connector_17
component_7
it
be
component_7
which
include
a
notion
of
time
in
the
underlie
connector_data_1
be
component_7
and
do
not
require
a
snapshot
of
the
connector_data_1
so
it
can
produce
output
at
a
component_25
control
frequency
instead
of
wait
for
the
end
of
the
connector_data_1
set
to
be
reach
in
this
sense
connector_17
component_7
be
a
generalization
of
pattern_9
component_7
and
give
the
prevalence
of
real
time
connector_data_1
a
very
important
generalization
so
why
have
the
traditional
pattern_7
of
connector_17
component_7
be
a
a
niche
component_6
i
think
the
big
reason
be
that
a
lack
of
real
time
connector_data_1
collection
make
continuous
component_7
something
of
an
academic
concern
i
think
the
lack
of
real
time
connector_data_1
collection
be
likely
what
doom
the
commercial
pattern_22
component_2
their
requirement_14
be
still
do

orient
daily
pattern_9
component_7
for
technology_12
and
connector_data_1
requirement_3
requirement_5
build
connector_17
component_7
component_5
focus
on
provide
component_7
component_40
to
attach
to
real
time
connector_data_1
connector_17
but
it
turn
out
that
at
the
time
very
few
people
actually
have
real
time
connector_data_1
connector_17
actually
very
early
at
my
career
at
linkedin
a
requirement_5
try
to
sell
u
a
very
cool
connector_17
component_7
component_2
but
since
all
our
connector_data_1
be
connector_40
in
hourly
at
that
time
the
best
component_6
we
could
come
up
with
be
to
pattern_23
the
hourly
into
the
connector_17
component_2
at
the
end
of
the
hour
they
note
that
this
be
a
fairly
common
problem
the
exception
actually
prove
the
rule
here
finance
the
one
domain
where
connector_17
component_7
have
meet
with
some
success
be
exactly
the
area
where
real
time
connector_data_1
connector_34
be
already
the
norm
and
component_7
have
become
the
bottleneck
even
in
the
presence
of
a
healthy
pattern_9
component_7
ecosystem
i
think
the
actual
applicability
of
connector_17
component_7
a
an
infrastructure
style
be
quite
broad
i
think
it
cover
the
gap
in
infrastructure
between
real
time
connector_data_11
connector_18
component_12
and
offline
pattern_9
component_7
for
modern
internet
requirement_5
i
think
around
25%
of
their
fall
into
this
category
it
turn
out
that
the
requirement_1
solve
some
of
the
most
critical
technical
problem
in
connector_17
component_7
which
i
ll
describe
but
the
big
problem
that
it
solve
be
make
connector_data_1
quality_attribute_8
in
real
time
multi
pattern_6
connector_data_1
feed
for
those
interest
in
more
detail
we
have
open
component_14
samza
a
connector_17
component_7
component_2
explicitly
build
on
many
of
these
idea
we
describe
a
lot
of
these
component_41
in
more
detail
in
the
documentation
here
connector_data_1
flow
graph
the
most
interest
aspect
of
connector_17
component_7
have
nothing
to
do
with
the
internals
of
a
connector_17
component_7
component_2
but
instead
have
to
do
with
how
it
extend
our
idea
of
what
a
connector_data_1
fee
be
from
the
early
connector_data_1
requirement_3
discussion
we
discuss
primarily
feed
or
requirement_1
of
primary
data—the
and
row
of
connector_data_1
produce
in
the
connector_14
of
various
component_6
but
connector_17
component_7
allow
u
to
also
include
feed
compute
off
other
feed
these
derive
feed
look
no
different
to
component_42
then
the
feed
of
primary
connector_data_1
from
which
they
be
compute
these
derive
feed
can
pattern_24
arbitrary
complexity

s
dive
into
this
a
bit
a
connector_17
component_7

for
our
purpose
will
be
anything
that
connector_6
from
requirement_1
and
connector_36
output
to
requirement_1
or
other
component_2
the
requirement_1
they
use
for
input
and
output
join
these
component_17
into
a
graph
of
component_7
stage
indeed
use
a
centralized
requirement_1
in
this
fashion
you
can
pattern_7
all
the
organization
s
connector_data_1
capture
transformation
and
flow
a
a
series
of
requirement_1
and
component_17
that
connector_3
to
them
a
connector_17
processor
need
not
have
a
fancy
technology_29
at
all
it
can
be
any
component_7
or
set
of
component_17
that
connector_8
and
connector_3
from
requirement_1
but
additional
infrastructure
and
support
can
be
provide
for
help
manage
component_7

the
purpose
of
the
requirement_1
in
the
requirement_3
be
two
fold
first
it
make
each
dataset
multi
pattern_6
and
order
recall
our
state
pattern_1
principle
to
remember
the
importance
of
order
to
make
this
more
concrete
consider
a
connector_17
of
connector_data_8
from
a
database—if
we
re
order
two
connector_data_8
to
the
same
component_10
in
our
component_7
we
produce
the
wrong
final
output
this
order
be
more
permanent
than
what
be
provide
by
something
technology_31
a
it
be
not
limit
to
a
single
point
to
point
connector_44
and
survive
beyond
component_7
failure
and
reconnections
second
the
requirement_1
provide
buffer
to
the
component_7
this
be
very
fundamental
if
component_7
proceed
in
an
unsynchronized
fashion
it
be
likely
to
happen
that
an
upstream
connector_data_1
produce
will
produce
connector_data_1
more
quickly
than
another
downstream
can
connector_24
it
when
this
occur
component_7
must
block
buffer
or
drop
connector_data_1
drop
connector_data_1
be
likely
not
an
option
pattern_25
cause
the
entire
component_7
graph
to
grind
to
a
halt
the
requirement_1
act
a
a
very
very
large
buffer
that
allow
component_7
to
be
restart
or
fail
without
slow
down
other
part
of
the
component_7
graph
this
isolation
be
particularly
important
when
extend
this
connector_data_1
flow
to
a
large
organization
where
component_7
be
happen
by
make
by
many
different
team
we
cannot
have
one
faulty
cause
back
pressure
that
stop
the
entire
component_7
flow
both
technology_32
and
samza
be
build
in
this
fashion
and
can
use
technology_19
or
other
similar
component_5
a
their
requirement_1
stateful
real
time
component_7
some
real
time
connector_17
component_7
be
stateless
component_10
at
a
time
transformation
but
many
of
the
us
be
more
sophisticate
count
aggregation
or
join
over
window
in
the
connector_17
one
might
for
example
want
to
enrich
an
connector_17
say
a
connector_17
of
click
with
connector_data_6
about
the
component_25
do
the
click—in
effect
join
the
click
connector_17
to
the
component_25
account
component_1
invariably
this
kind
of
component_7
end
up
require
some
kind
of
state
to
be
maintain
by
the
processor
for
example
when
computing
a
count
you
have
the
count
so
far
to
maintain
how
can
this
kind
of
state
be
maintain
correctly
if
the
processor
themselves
can
fail
the
quality_attribute_3
alternative
would
be
to
keep
state
in
memory
however
if
the
component_7
crash
it
would
lose
it
intermediate
state
if
state
be
only
maintain
over
a
window
the
component_7
could
fall
back
to
the
point
in
the
requirement_1
where
the
window
begin
however
if
one
be
do
a
count
over
an
hour
this
not
be
feasible
an
alternative
be
to
simply
component_3
all
state
in
a
remote
storage
component_2
and
join
over
the
requirement_13
to
that
component_3
the
problem
with
this
be
that
there
be
no
locality
of
connector_data_1
and
lot
of
requirement_13
round
trip
how
can
we
support
something
a
component_9
that
be
component_36
up
with
our
component_7
well
recall
the
discussion
of
the
duality
of
component_24
and
requirement_1
this
give
u
exactly
the
technology_10
to
be
able
to
convert
connector_34
to
component_24
co
locate
with
our
component_7
a
well
a
a
mechanism
for
handle
fault
tolerance
for
these
component_9
a
connector_17
processor
can
keep
it
s
state
in
a
local
component_9
or
index
—a
bdb
leveldb
or
even
something
more
unusual
such
a
a
technology_33
or
fastbit
index
the
content
of
this
this
component_3
be
feed
from
it
input
connector_34
after
first
perhaps
apply
arbitrary
transformation
it
can
journal
out
a
changelog
for
this
local
index
it
keep
to
allow
it
to
restore
it
state
in
the
of
a
crash
and
restart
this
mechanism
allow
a
generic
mechanism
for
keep
co
component_36
state
in
arbitrary
index
type
local
with
the
incoming
connector_17
connector_data_1
when
the
component_7
fail
it
restore
it
index
from
the
changelog
the
requirement_1
be
the
transformation
of
the
local
state
into
a
sort
of
incremental
component_10
at
a
time
backup
this
approach
to
state
requirement_15
have
the
elegant
property
that
the
state
of
the
processor
be
also
maintain
a
a
requirement_1
we
can
think
of
this
requirement_1
we
would
the
requirement_1
of
connector_12
to
a
component_1
component_9
in
fact
the
processor
have
something
very
a
co
component_36
component_9
maintain
along
with
them
since
this
state
be
itself
a
requirement_1
other
processor
can
subscribe
to
it
this
can
actually
be
quite
useful
in
requirement_6
when
the
goal
of
the
component_7
be
to
update
a
final
state
and
this
state
be
the
natural
output
of
the
component_7
when
combine
with
the
requirement_1
come
out
of
component_1
for
connector_data_1
requirement_3
purpose
the
power
of
the
requirement_1
component_9
duality
become
clear
a
connector_5
requirement_1
be
extract
from
a
component_1
and
index
in
different
form
by
various
connector_17
processor
to
join
against
connector_17
we
give
more
detail
on
this
style
of
manage
stateful
component_7
in
samza
and
a
lot
more
practical
example
here
requirement_1
compaction
of

we
can
t
hope
to
keep
a
complete
requirement_1
for
all
state
connector_12
for
all
time
unless
one
want
to
use
infinite
space
somehow
the
requirement_1
must
be
clean
up
i
ll
talk
a
little
about
the
implementation
of
this
in
technology_19
to
make
it
more
concrete
in
technology_19
cleanup
have
two
option
quality_attribute_6
on
whether
the
connector_data_1
contain
key
connector_data_8
or
connector_data_1
for
connector_data_1
technology_19
support
retain
a
window
of
connector_data_1
usually
this
be
configure
to
a
few
day
but
the
window
can
be
define
in
term
of
time
or
space
for
key
connector_data_1
though
a
nice
property
of
the
complete
requirement_1
be
that
you
can
replay
it
to
recreate
the
state
of
the
component_14
component_2
potentially
recreate
it
in
another
component_2
however
retain
the
complete
requirement_1
will
use
more
and
more
space
a
time
go
by
and
the
replay
will
take
long
and
long
hence
in
technology_19
we
support
a
different
type
of
retention
instead
of
simply
throw
away
the
old
requirement_1
we
remove
obsolete
records—i
e
component_8
whose
primary
key
have
a
more
recent
update
by
do
this
we
still
guarantee
that
the
requirement_1
contain
a
complete
backup
of
the
component_14
component_2
but
now
we
can
no
long
recreate
all
previous
state
of
the
component_14
component_2
only
the
more
recent
one
we
connector_data_4
this
feature
requirement_1
compaction
part
four
component_2
build
the
final
topic
i
want
to
discus
be
the
role
of
the
requirement_1
in
connector_data_1
component_2
design
for
online
connector_data_1
component_2
there
be
an
analogy
here
between
the
role
a
requirement_1
serve
for
connector_data_1
flow
inside
a
quality_attribute_1
component_1
and
the
role
it
serve
for
connector_data_1
requirement_3
in
a
large
organization
in
both
requirement_6
it
be
responsible
for
connector_data_1
flow
consistency
and
recovery
what
after
all
be
an
organization
if
not
a
very
complicate
quality_attribute_1
connector_data_1
component_2
unbundling
so
maybe
if
you
squint
a
bit
you
can
see
the
whole
of
your
organization
s
component_5
and
connector_data_1
flow
a
a
single
quality_attribute_1
component_1
you
can
pattern_7
all
the
individual
query
orient
component_5
technology_34
technology_35
technology_18
component_9
and
so
on
a
particular
index
on
your
connector_data_1
you
can
pattern_7
the
connector_17
component_7
component_5
technology_32
or
samza
a
a
very
well
develop
connector_45
and
pattern_7
materialization
mechanism
classical
component_1
people
i
have
notice
this
pattern_7
very
much
because
it
finally
explain
to
them
what
on
earth
people
be
do
with
all
these
different
connector_data_1
systems—they
be
different
index
type
there
be
undeniably
now
an
explosion
of
type
of
connector_data_1
component_2
but
in
reality
this
complexity
have
always
exist
even
in
the
heyday
of
the
relational
component_1
organization
have
lot
and
lot
of
relational
component_1
so
perhaps
real
requirement_3
hasn
t
exist
since
the
component_43
when
all
the
connector_data_1
really
be
in
one
place
there
be
many
motivation
for
segregate
connector_data_1
into
multiple
component_2
quality_attribute_15
geography
quality_attribute_22
and
requirement_4
isolation
be
the
most
common
but
these
issue
can
be
connector_26
by
a
quality_attribute_7
component_2
it
be
possible
for
an
organization
to
have
a
single
technology_1
cluster
for
example
that
contain
all
the
connector_data_1
and
serve
a
large
and
diverse
constituency
so
there
be
already
one
possible
simplification
in
the
handle
of
connector_data_1
that
have
become
possible
in
the
move
to
quality_attribute_1
component_2
coalesce
lot
of
little
instance
of
each
component_2
into
a
few
big
cluster
many
component_5
aren
t
quality_attribute_7
enough
to
allow
this
yet
they

t
have
quality_attribute_22
or
can
t
guarantee
requirement_4
isolation
or

t
quality_attribute_15
well
enough
but
each
of
these
problem
be
solvable
my
take
be
that
the
explosion
of
different
component_5
be
cause
by
the
difficulty
of
build
quality_attribute_1
connector_data_1
component_2
by
cut
back
to
a
single
query
type
or
use
requirement_6
each
component_2
be
able
to
bring
it
scope
down
into
the
set
of
thing
that
be
feasible
to
build
but
run
all
these
component_5
yield
too
much
complexity
i
see
three
possible
direction
this
could
follow
in
the
future
the
first
possibility
be
a
continuation
of
the
status
quo
the
separation
of
component_5
remain
more
or
le
a
it
be
for
a
quality_attribute_7
deal
long
this
could
happen
either
because
the
difficulty
of
distribution
be
too
hard
to
overcome
or
because
this
specialization
allow
level
of
convenience
and
power
for
each
component_2
a
long
a
this
remain
true
the
connector_data_1
requirement_3
problem
will
remain
one
of
the
most
centrally
important
thing
for
the
successful
use
of
connector_data_1
in
this
requirement_6
an
external
requirement_1
that
quality_attribute_17
connector_data_1
will
be
very
important
the
second
possibility
be
that
there
could
be
a
re
consolidation
in
which
a
single
component_2
with
enough
generality
start
to
merge
back
in
all
the
different
into
a
single
uber
component_2
this
uber
component_2
could
be
the
relational
component_1
superficially
but
it
s
use
in
an
organization
would
be
far
different
a
you
would
need
only
one
big
one
instead
of
umpteen
little
one
in
this
world
there
be
no
real
connector_data_1
requirement_3
problem
except
what
be
solve
inside
this
component_2
i
think
the
practical
difficulty
of
build
such
a
component_2
make
this
unlikely
there
be
another
possible
outcome
though
which
i
actually
find
appeal
a
an
engineer
one
interest
facet
of
the
generation
of
connector_data_1
component_5
be
that
they
be
virtually
all
open_source
open_source
allow
another
possibility
connector_data_1
infrastructure
could
be
unbundled
into
a
collection
of
component_12
and
component_6
face
component_2
apis
you
already
see
this
happen
to
a
certain
extent
in
the
technology_36
technology_37
technology_25
handle
much
of
the
component_2
co
ordination
perhaps
with
a
bit
of
help
from
high
level
abstraction
helix
or
curator
mesos
and
technology_38
do
component_7
virtualization
and
resource
requirement_15
embed
technology_39
technology_33
and
leveldb
do
index
technology_40
technology_41
and
high
level
wrapper
technology_42
and
technology_43
handle
remote
connector_27
avro
technology_9
buffer
technology_44
and
umpteen
zillion
other
technology_39
handle
serialization
technology_19
and
bookeeper
provide
a
back
requirement_1
if
you
technology_37
these
thing
in
a
pile
and
squint
a
bit
it
start
to
look
a
bit
a
lego
version
of
quality_attribute_1
connector_data_1
component_2
engineering
you
can
piece
these
ingredient
together
to
create
a
vast
of
possible
component_2
this
be
clearly
not
a
story
relevant
to
end
component_25
who
presumably
care
primarily
more
about
the
component_32
then
how
it
be
connector_29
but
it
might
be
a
path
towards
connector_15
the
quality_attribute_26
of
the
single
component_2
in
a
more
diverse
and
modular
world
that
continue
to
quality_attribute_27
if
the
implementation
time
for
a
quality_attribute_1
component_2
go
from
year
to
week
because
quality_attribute_10
quality_attribute_28
build
block
emerge
then
the
pressure
to
coalesce
into
a
single
monolithic
component_2
disappear
the
place
of
the
requirement_1
in
component_2
architecture
a
component_2
that
assume
an
external
requirement_1
be
present
allow
the
individual
component_5
to
relinquish
a
lot
of
their
own
complexity
and
rely
on
the
connector_46
requirement_1
here
be
the
thing
i
think
a
requirement_1
can
do
handle
connector_data_1
consistency
whether
eventual
or
immediate
by
sequence
concurrent
connector_data_8
to
technology_45
provide
connector_data_1
pattern_1
between
technology_45
provide
connector_4
semantics
to
the
writer
i
e
acknowledge
only
when
your
connector_3
guarantee
not
to
be
lose
provide
the
external
connector_data_1
subscription
fee
from
the
component_2
provide
the
capability
to
restore
fail
replica
that
lose
their
connector_data_1
or
bootstrap
replica
handle
rebalancing
of
connector_data_1
between
technology_45
this
be
actually
a
substantial
portion
of
what
a
quality_attribute_1
connector_data_1
component_2
do
in
fact
the
majority
of
what
be
leave
over
be
relate
to
the
final
component_38
face
query
component_32
and
index
strategy
this
be
exactly
the
part
that
should
vary
from
component_2
to
component_2
for
example
a
full
text
search
query
need
to
query
all
component_36
whereas
a
query
by
primary
key
only
need
to
query
a
single
technology_45
responsible
for
that
key
s
connector_data_1
here
be
how
this
work
the
component_2
be
divide
into
two
logical
piece
the
requirement_1
and
the
serve
pattern_21
the
requirement_1
capture
the
state
connector_12
in
sequential
order
the
serve
technology_45
component_3
whatever
index
be
require
to
serve
connector_11
for
example
a
key
requirement_2
component_3
might
have
something
a
btree
or
sstable
a
search
component_2
would
have
an
invert
index
connector_36
either
go
directly
to
the
requirement_1
though
they
be
proxied
by
the
serve
pattern_21
connector_41
to
the
requirement_1
yield
a
logical
pattern_2
say
the
index
in
the
requirement_1
if
the
component_2
be
component_36
and
i
assume
it
be
then
the
requirement_1
and
the
serve
technology_45
will
have
the
same
number
of
component_36
though
they
have
very
different
number
of
component_16
the
serve
technology_45
subscribe
to
the
requirement_1
and
apply
connector_36
a
quickly
a
possible
to
it
local
index
in
the
order
the
requirement_1
have
component_3
them
the
component_38
can
connector_7
connector_8
your
connector_3
semantics
from
any
technology_45
by
provide
the
pattern_2
of
a
connector_3
a
part
of
it
query—a
serve
technology_45
connector_47
such
a
query
will
compare
the
desire
pattern_2
to
it
own
index
point
and
if
necessary
delay
the
connector_data_11
until
it
have
index
up
to
at
least
that
time
to
avoid
serve
stale
connector_data_1
the
serve
technology_45
or
not
need
to
have
any
notion
of
mastership
or
leader
election
for
many
quality_attribute_3
use
requirement_6
the
serve
technology_45
can
be
completely
without
leader
since
the
requirement_1
be
the
component_14
of
truth
one
of
the
trickier
thing
a
quality_attribute_1
component_2
must
do
be
handle
restore
fail
technology_45
or
move
component_36
from
technology_45
to
technology_45
a
typical
approach
would
have
the
requirement_1
retain
only
a
fix
window
of
connector_data_1
and
combine
this
with
a
snapshot
of
the
connector_data_1
component_3
in
the
component_36
it
be
equally
possible
for
the
requirement_1
to
retain
a
complete
copy
of
connector_data_1
and
garbage
connector_40
the
requirement_1
itself
this
move
a
significant
amount
of
complexity
out
of
the
serve
pattern_21
which
be
component_2
specific
and
into
the
requirement_1
which
can
be
general
purpose
by
have
this
requirement_1
component_2
you
connector_7
a
fully
develop
subscription
component_32
for
the
content
of
the
connector_data_1
component_3
which
feed
technology_12
into
other
component_2
in
fact
many
component_5
can
connector_1
the
same
the
requirement_1
while
provide
different
index
this
note
how
such
a
requirement_1
centric
component_2
be
itself
immediately
a
technology_46
of
connector_data_1
connector_34
for
component_7
and
loading
in
other
component_2
likewise
a
connector_17
processor
can
connector_24
multiple
input
connector_34
and
then
serve
them
via
another
component_2
that
index
that
output
i
find
this
pattern_7
of
component_5
a
factor
into
a
requirement_1
and
query
technology_47
to
very
reveal
a
it

you
separate
the
query
characteristic
from
the
quality_attribute_29
and
consistency
aspect
of
the
component_2
i
actually
think
this
be
even
a
useful
way
to
mentally
factor
a
component_2
that
isn
t
build
this
way
to
quality_attribute_7
understand
it
it
s
worth
note
that
although
technology_19
and
bookeeper
be
consistent
requirement_1
this
be
not
a
requirement
you
could
a
easily
factor
a
dynamo

component_1
into
an
eventually
consistent
ap
requirement_1
and
a
key
requirement_2
serve
pattern_21
such
a
requirement_1
be
a
bit
tricky
to
work
with
a
it
will
redeliver
old
connector_data_3
and
quality_attribute_6
on
the
pattern_6
to
handle
this
much
dynamo
itself
the
idea
of
have
a
separate
copy
of
connector_data_1
in
the
requirement_1
especially
if
it
be
a
complete
copy
strike
many
people
a
wasteful
in
reality
though
there
be
a
few
factor
that
make
this
le
of
an
issue
first
the
requirement_1
can
be
a
particularly
quality_attribute_30
storage
mechanism
we
component_3
over
75tb
per
datacenter
on
our
production
technology_19
component_27
meanwhile
many
serve
component_5
require
much
more
memory
to
serve
connector_data_1
efficiently
text
search
for
example
be
often
all
in
memory
the
serve
component_2
also
use
optimize
hardware
for
example
most
our
live
connector_data_1
component_5
either
serve
out
of
memory
or
else
use
ssds
in
contrast
the
requirement_1
component_2
do
only
linear
connector_6
and
connector_3
so
it
be
quite
happy
use
large
multi
tb
hard
drive
finally
a
in
the
picture
above
in
the
requirement_6
where
the
connector_data_1
be
serve
by
multiple
component_2
the
cost
of
the
requirement_1
be
amortize
over
multiple
index
this
combination
make
the
expense
of
an
external
requirement_1
pretty
minimal
this
be
exactly
the
pattern_19
that
linkedin
have
use
to
build
out
many
of
it
own
real
time
query
component_2
these
component_5
fee
off
a
component_1
use
databus
a
a
requirement_1
abstraction
or
off
a
dedicate
requirement_1
from
technology_19
and
provide
a
particular
partitioning
index
and
query
capability
on
top
of
that
connector_data_1
connector_17
this
be
the
way
we
have
connector_29
our
search
social
graph
and
technology_14
query
component_2
in
fact
it
be
quite
common
to
have
a
single
connector_data_1
fee
whether
a
live
fee
or
a
derive
fee
come
from
technology_1
replicate
into
multiple
serve
component_5
for
live
serve
this
have
prove
to
be
an
enormous
simplify
assumption
none
of
these
component_5
need
to
have
an
externally
quality_attribute_31
connector_3
technology_47
at
all
technology_19
and
component_1
be
use
a
the
component_2
of
component_10
and
connector_12
flow
to
the
appropriate
query
component_5
through
that
requirement_1
connector_36
be
handle
locally
by
the
technology_45
component_44
a
particular
component_36
these
technology_45
blindly
transcribe
the
fee
provide
by
the
requirement_1
to
their
own
component_3
a
fail
technology_45
can
be
restore
by
replay
the
upstream
requirement_1
the
degree
to
which
these
component_5
rely
on
the
requirement_1
vary
a
fully
reliant
component_2
could
make
use
of
the
requirement_1
for
connector_data_1
partitioning
technology_45
restore
rebalancing
and
all
aspect
of
consistency
and
connector_data_1
propagation
in
this
setup
the
actual
serve
tier
be
actually
nothing
le
than
a
sort
of
pattern_10
pattern_4
to
enable
a
particular
type
of
component_7
with
connector_36
go
directly
to
the
requirement_1
the
end
if
you
make
it
this
far
you
most
of
what
i
about
requirement_1
here
be
a
few
interest
reference
you
want
to
connector_21
out
everyone
seem
to
us
different
term
for
the
same
thing
so
it
be
a
bit
of
a
puzzle
to
connector_30
the
component_1
literature
to
the
quality_attribute_1
component_5
stuff
to
the
various
requirement_16
camp
to
the
open_source
world
nonetheless
here
be
a
few
pointer
in
the
general
direction
academic
paper
component_2
talk
and

a
quality_attribute_7
overview
of
state
component_16
and
primary
backup
pattern_1
pacifica
be
a
generic
technology_29
for
connector_16
requirement_1
base
quality_attribute_1
storage
component_5
at

spanner—not
everyone
love
logical
time
for
their
requirement_1

s
component_1
try
to
use
physical
time
and
component_22
the
uncertainty
of
clock
drift
directly
by
treat
the
pattern_2
a
a
range
datanomic
deconstruct
the
component_1
be
a
great
presentation
by
rich
hickey
the
creator
of
technology_48
on
his
startup
s
component_1
technology_16
a
survey
of
rollback
recovery
technology_9
in
connector_data_2
pass
component_2
i
find
this
to
be
a
very
helpful
introduction
to
fault
tolerance
and
the
practical
component_6
of
requirement_1
to
recovery
outside
component_1
reactive
manifesto—i
m
actually
not
quite
sure
what
be
mean
by
reactive
programming
but
i
think
it
mean
the
same
thing
a

drive
this
connector_44
doesn
t
have
much
info
but
this
by
martin
odersky
of
technology_49
fame
look
facinating
paxos
original
paper
be
here
leslie
lamport
have
an
interest
history
of
how
the
algorithm
be
create
in
the
1980s
but
not
publish
until

because
the
reviewer
didn
t
the
greek
parable
in
the
paper
and
he
didn
t
want
to
connector_5
it
even
once
the
original
paper
be
publish
it
wasn
t
well
understand
lamport
try
again
and
this
time
even
include
a
few
of
the
uninteresting
detail
of
how
to
put
it
to
use
use
these

fangled
automatic
component_23
it
be
still
not
widely
understand
fred
schneider
and
butler
lampson
each
give
more
detail
overview
of
apply
paxos
in
real
component_2
a
few
engineer
summarize
their
experience
connector_16
paxos
in
chubby
i
actually
find
all
the
paxos
paper
pretty
painful
to
understand
but
dutifully
struggle
through
but
you

t
need
to
because
this
video
by
john
ousterhout
of
requirement_1
pattern_4
filesystem
fame
will
make
it
all
very
quality_attribute_3
somehow
these
consensus
algorithm
be
much
quality_attribute_7
present
by
draw
them
a
the
connector_27
round
unfold
rather
than
in
a
presentation
in
a
paper
ironically
this
video
be
create
in
an
attempt
to
show
that
paxos
be
hard
to
understand
use
paxos
to
build
a
quality_attribute_19
consistent
connector_data_1
component_3
this
be
a
cool
paper
on
use
a
requirement_1
to
build
a
connector_data_1
component_3
by
jun
one
of
the
co
author
be
also
one
of
the
early
engineer
on
technology_19
paxos
have
competitor
actually
each
of
these
connector_data_17
a
lot
more
closely
to
the
implementation
of
a
requirement_1
and
be
probably
more
suitable
for
practical
implementation
viewstamped
pattern_1
by
barbara
liskov
be
an
early
algorithm
to
directly
component_21
requirement_1
pattern_1
zab
be
the
algorithm
use
by
technology_25
raft
be
an
attempt
at
a
more
understandable
consensus
algorithm
the
video
presentation
also
by
john
ousterhout
be
great
too
you
can
see
the
role
of
the
requirement_1
in
action
in
different
real
quality_attribute_1
component_1
pnuts
be
a
component_2
which
attempt
to
apply
to
requirement_1
centric
design
of
traditional
quality_attribute_1
component_1
at
large
quality_attribute_15
technology_50
and
bigtable
both
give
another
example
of
requirement_1
in
modern
component_1
linkedin
s
own
quality_attribute_1
component_1
espresso
pnuts
us
a
requirement_1
for
pattern_1
but
take
a
slightly
different
approach
use
the
underlie
component_9
itself
a
the
component_14
of
the
requirement_1
if
you
find
yourself
comparison
shop
for
a
pattern_1
algorithm
this
paper
help
you
out
pattern_1
theory
and
practice
be
a
great
book
that
connector_48
a
bunch
of
summary
paper
on
pattern_1
in
quality_attribute_1
component_2
many
of
the
chapter
be
online
e
g






connector_17
component_7
this
be
a
bit
too
broad
to
summarize
but
here
be
a
few
thing
i

component_22
and
issue
in
connector_data_1
connector_17
component_2
probably
the
best
overview
of
the
early
research
in
this
area
high
quality_attribute_29
algorithm
for
quality_attribute_1
connector_17
component_7
a
couple
of
random
component_5
paper
telegraphcq
aurora
niagaracq
discretized
connector_17
this
paper
discus
technology_51
s
connector_49
component_2
millwheel
be
one
of

s
connector_17
component_7
component_2
naiad
a
timely
dataflow
component_2
requirement_16
have
all
the
same
problem
but
with
different
name
a
small
quality_attribute_15
and
technology_52
ha
ha
kid
kind
of
sourcing—as
far
a
i
can
tell
this
be
basically
the
requirement_16
engineer
s
way
of
say
state
component_16
pattern_1
it
s
interest
that
the
same
idea
would
be
invent
again
in
such
a
different
component_30
component_14
seem
to
focus
on
small
in
memory
use
requirement_6
this
approach
to
component_6
development
seem
to
combine
the
connector_17
component_7
that
occur
on
the
requirement_1
of
with
the
component_6
since
this
become
pretty
non
trivial
when
the
component_7
be
large
enough
to
require
connector_data_1
partitioning
for
quality_attribute_15
i
focus
on
connector_17
component_7
a
a
separate
infrastructure
primitive
connector_5
connector_data_1
capture—there
be
a
small
requirement_11
around
connector_15
connector_data_1
out
of
component_1
and
this
be
the
most
requirement_1
friendly
style
of
connector_data_1
extraction
requirement_16
component_6
requirement_3
seem
to
be
about
solve
the
connector_data_1
requirement_3
problem
when
what
you
have
be
a
collection
of
off
the
shelf
requirement_16
crm
or
supply
chain
requirement_15

complex
component_7
cep
fairly
certain
nobody

what
this
mean
or
how
it
actually
differ
from
connector_17
component_7
the
difference
seem
to
be
that
the
focus
be
on
unordered
connector_34
and
on
pattern_16
and
detection
rather
than
aggregation
but
this
in
my
opinion
be
a
distinction
without
a
difference
i
think
any
component_2
that
be
quality_attribute_7
at
one
should
be
quality_attribute_7
at
another
requirement_16
component_20
bus—i
think
the
requirement_16
component_20
bus
concept
be
very
similar
to
some
of
the
idea
i
have
describe
around
connector_data_1
requirement_3
this
idea
seem
to
have
be
moderately
successful
in
requirement_16

and
be
mostly
unknown
among
web
folk
or
the
quality_attribute_1
connector_data_1
infrastructure
crowd
interest
open_source
stuff
technology_19
be
the
requirement_1
a
a
component_20
project
that
be
the
basis
for
much
of
this

bookeeper
and
hedwig
comprise
another
open_source
requirement_1
a
a
component_20
they
seem
to
be
more
target
at
connector_data_1
component_2
internals
then
at
connector_data_1
databus
be
a
component_2
that
provide
a
requirement_1

overlay
for
component_1
component_9
technology_26
be
an
actor
technology_29
for
technology_53
it
have
an
on
eventsourced
that
provide
persistence
and
journaling
samza
be
a
connector_17
component_7
technology_29
we
be
work
on
at
linkedin
it
us
a
lot
of
the
idea
in
this
a
well
a
quality_attribute_17
with
technology_19
a
the
underlie
requirement_1
technology_32
be
popular
connector_17
component_7
technology_29
that
quality_attribute_17
well
with
technology_19
technology_51
connector_49
be
a
connector_17
component_7
technology_29
that
be
part
of
technology_51
summingbird
be
a
pattern_21
on
top
of
technology_32
or
technology_1
that
provide
a
convenient
computing
abstraction
i
try
to
keep
up
on
this
area
so
if
you
of
some
thing
i
ve
leave
out
me

i
leave
you
with
this
connector_data_2
topic
requirement_1
connector_17
component_7
technology_1
connector_data_1
quality_attribute_1
component_2
technology_12
technology_19
relate
storylessons
from


year
of
our
requirement_17
&
connector_data_1
connector_10
grouprelated
storyevolution
of
technology_54
at
linkedin
back
to
toplinkedin
technology_55
connector_data_1
open_source
trust
infrastructure
linkedin
corporation
©

about
requirement_18
privacy
requirement_18
component_25
agreement
quality_attribute_32
linkedin
twitter
youtube
technology_56
dismiss
