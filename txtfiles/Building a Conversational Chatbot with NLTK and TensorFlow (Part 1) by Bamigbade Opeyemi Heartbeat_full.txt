Building a Conversational Chatbot with NLTK and TensorFlow (Part 1) | by Bamigbade Opeyemi | HeartbeatOpen in appHomeNotificationsListsStoriesWritePublished inHeartbeatBamigbade OpeyemiFollowJul 31, 2020·9 min readSaveBuilding a Conversational Chatbot with NLTK and TensorFlow (Part 1)A Tennis Chatbot built with Convolutional Neural NetworksPhoto by Darko Nesic on UnsplashThe integration of conversational chatbot in business platforms or websites now feels inevitable, as companies try to ensure customers have access to the right information—anytime, anywhere, any day.A conversational chatbot is an intelligent piece of AI-powered software that makes machines capable of understanding, processing, and responding to human language based on sophisticated deep learning and natural language understanding (NLU).What you will learn in this seriesTypes of ChatbotsWorking with a DatasetText Pre-ProcessingModel ArchitectureTraining and EvaluationDeployment Environment Setup (part 2)Demo with Streamlit (part 2)At the end of this short series, you should be confident in your ability to build a version of the chatbot web application demo shown below.Bot demo with streamlitTypes of chatbotsCategorizing chatbots is becoming an increasingly difficult task due to the fast rate at which developer tools and methodologies are changing. On a high level, we can categorize bots into:1. Retrieval-based Chatbots: These are chatbots that use some type of heuristic approach to select the appropriate response from sets of predefined responses.2. Generative-based Chatbots: These are deep neural network-based chatbots that use a large amount of data to train models that provide a more easy translation of user input to output.With these 2 categories in mind, chatbots can further be classified in the following manner:Scripted/Quick Reply Bots: A type of chatbot in which interaction with the end-user happens through a predefined knowledge base and technical capabilities that can quickly respond only to specific instructions.NLP Chatbots: A type of chatbot that uses natural language processing (NLP) to map user input to an intent, with the aim of classifying the message for an appropriate predefined possible responseAction/Service Chatbots: Helps users complete their requests by asking for relevant information.Social Messaging Chatbots: Integrated into social media platforms such as Whatsapp, Messenger, Twitter, etc.Context-Enabled Chatbots: These have the capability to utilize machine learning and AI to learn from their experience with users and better understand the context with time, so as to better be customized to the user. Examples of this type include Siri, Alexa, and Google Assistant.Voice-Enabled Chatbots: They accept user input through voice and use the request to query possible responses based on the personalized experience.For this project, we will be building an NLP Generative-based Chatbot on a tennis-related corpus.Working with a DatasetA conversational chatbot can be multidisciplinary or specific. The scope of the chatbot is partly dependent on the volume of data used to train it. The dataset used for the project was scraped from a few sites that specifically include tennis-related information.Why tennis ? It’s a sport that require angle evaluation, geometry, and physics to get the best result at every point play, and it makes me a better problem solver off the court. It's a sport that improves fast optimal decision making, good enough for me to relax on weekends.The data is structured into tags, patterns, responses, and context.Tags: Possible classes of user intention for asking a question.Patterns: The ways in which users usually ask questions relating to a particular tag.Responses: Predefined responses for each tag in the dataset from which the model can choose to respond to a particular question.Context: Contextual words relating to a tag for easy and better classification of what the user intends with their request.With the four data heads explained above in an intent.json file, we can train a chatbot to suit our particular use case.datasetA newsletter for machine learners — by machine learners. Sign up to receive our weekly dive into all things ML, curated by our experts in the field.Text Pre-ProcessingWe cannot go straight from raw text to fitting a machine learning or deep learning model. First, we need to prepare the data for modeling in a few ways—by splitting words, handling punctuation and cases, and more. Cleaning up text data in NLP is task-specific. For this conversational chatbot we’re building, we can do the following:TokenizationLemmatizationRemoving stop wordsVocabulary buildingEncoding and decodingData splittingText pre-processing can be really challenging, but in order to avoid writing all functions from scratch, we can frame the JSON file with a Pandas DataFrame with the function below:Convert the .json into Pandas DataFrameLibraries used for the projectTokenizationTokenization is the act of splitting a text corpus into constitute words—i.e splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units is called a token. Tokenization can be done manually by splitting based on white space or by using dedicated tools in libraries such as NLTK. The function below was used to tokenize our corpus:tokenizer functionLemmatizationLemmatization is a common normalization technique in text pre-processing. In lemmatization, words are replaced by their root form or words with similar context. Another text normalization technique similar to this is called stemming. This is often done alongside manual tokenization so as to yield useful tokens.source: kdnuggets.comRemoving stop wordsIt’s also good practice to remove stop words from tokens so as to avoid misleading the model. Stop words are words that do not contribute to the deeper meaning of the phrase—definite and indefinite articles, pronouns, and conjunctions to mention a few.With the NLTK library, filtering out stop words is easy, and you can also add words that you feel should be a stop word into the predefined set of words in the library. The code snippet below will print out the stop words using the NLTK library:from nltk.corpus import stopwordsstop_words = stopwords.words('english')print(stop_words)remove stop words and save the token with joblib libraryVocabulary buildingOnce we remove the stop words, the text is becoming cleaner, and at least halfway ready for modeling. Our next step is to build a vocabulary, which is a set of words in a given dataset after the removal of stop words. This will come in very handy during data encoding.function to create words vocabularyEncoding and decodingNow that we have a vocabulary of words in the dataset, each of the patterns can be encoded into numerical features for modeling, using any of the common text encoding techniques—count vectorizer, term frequency-inverse document frequency (TF-IDF), hashing, etc.Using TensorFlow.Keras text_to_sequence, we can encode each pattern corpus to vectorize a text corpus by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count which is based on TF-IDF. The resulting vectors will be post-padded with zeros so as to equal the length of the vectors.encoding functionData splittingWith the data encoded, we can now split it into training and testing sets. The training set will be used to train the model while the testing set will be used for evaluating its performance on unseen data. This can be done using a stratified approach, whereby of the patterns in the tags are well represented in the testing set.train = df_encoded.loc[train_index]test = df_encoded.loc[test_index]X_train = train.drop(columns=['labels'],axis=1)y_train = train.labelsX_test = test.drop(columns=['labels'],axis=1)y_test = test.labelsModel architectureThe most common approach to building a model on sequence input is to use a reinforcement learning model—we’ll use a long-short-term-memory (LSTM) architecture due to its state-of-the-art performance. The choice for this project is a convolutional neural network (CNN) + an embedding layer + fully connected layer.source: Deep Learning for Natural Language Processing by Jason BrownleeA word embedding is a method of representing text in which each word in the vocabulary is represented by a real-valued vector in a high-dimensional space. These vectors are learned in such a way that words that have similar meanings will have similar representations in the vector space.This has been proven as a better representation for text than normal classical methods like bag-of-words, where relationships between words or tokens are ignored, or forced in bigram and trigram approaches. These vectors are learned and updated during the model training.modelling functionAs seen in the code snippet above, the vector output of the embedding layer is 300 and followed by a 1-dimensional convolution layer using 64 filters with kernel size of 4, each using a ReLU activation function.Then, a 1-dimensional max pooling layer with a pool size of 8. These vectors are fattened (transposed to a single row of vectors) for the fully connected layer before compiling it with an Adam optimizer. These parameters are a result of several iterative processes, with the aim being to get the best model architecture for the dataset at hand. The model summary is shown belowmodel summaryWith epochs set to 500 for training, the best model was found at 165th epoch:history = model.fit(X_train, y_train, epochs=500, verbose=1,validation_data=(X_test,y_test),callbacks=callbacks)Model History and Evaluationacc = history.history['acc']val_acc = history.history['val_acc']loss=history.history['loss']val_loss=history.history['val_loss']plt.figure(figsize=(16,8))plt.subplot(1, 2, 1)plt.plot(acc, label='Training Accuracy')plt.plot(val_acc, label='Validation Accuracy')plt.legend(loc='lower right')plt.title('Training and Validation Accuracy')plt.subplot(1, 2, 2)plt.plot(loss, label='Training Loss')plt.plot(val_loss, label='Validation Loss')plt.legend(loc='upper right')plt.title('Training and Validation Loss')plt.show()With the visualization code snippet above, we can view the accuracy and losses recorded during the training, which can be found below:Training HistoryEvaluating the model on the test set, we can see the model performance was just over 80% accuracy.model evaluationWhat’s Next?With the trained model and artefacts saved, the next step is to set up the deployment environment, a bot response script, and modules that we’ll be testing using Streamlit. This will be done in part 2 of this article series.Congratulations for reading this far. Do find time check out my other articles and further readings in the reference section. Kindly remember to follow me so as to get notified of my publications.Connect with me on Twitter and LinkedInCheck out the Project GitHub Repository and remember to star it in the link below:opeyemibami/NLP-Tennis-BotA Conversational chatbot built with NLTK and Tensorflow on tennis corpus Dismiss GitHub is home to over 50 million…github.comCheers!ReferencesRonan Collobert, et al. in their paper Natural Language Processing (almost) from Scratch, 2011.Lemmatization in Natural Language Processing (NLP) and Machine Learning by Sunny SrinidhiTopic Modeling Open Source ToolA tool built with python and streamlit for topic modellingmedium.com6 Types of Chatbots: Know Which One Works Best for your BusinessThe introduction of chatbots revolutionized customer and brand interaction. With the ability to mimic conversations and…insights.daffodilsw.comDeploying Machine Learning Models on Google Cloud Platform (GCP)Train on Kaggle; deploy on Google Cloudheartbeat.comet.mlDeployment of Machine learning Model Demystified (Part 1)What if probability measures can best be used in loan default algorithm?towardsdatascience.comDeployment of Machine learning Models Demystified (Part 2)Loan acceptance status prediction with risk-free loanable amountmedium.comEditor’s Note: Heartbeat is a contributor-driven online publication and community dedicated to providing premier educational resources for data science, machine learning, and deep learning practitioners. We’re committed to supporting and inspiring developers and engineers from all walks of life.Editorially independent, Heartbeat is sponsored and published by Comet, an MLOps platform that enables data scientists & ML teams to track, compare, explain, & optimize their experiments. We pay our contributors, and we don’t sell ads.If you’d like to contribute, head on over to our call for contributors. You can also sign up to receive our weekly newsletters (Deep Learning Weekly and the Comet Newsletter), join us on Slack, and follow Comet on Twitter and LinkedIn for resources, events, and much more that will help you build better ML models, faster.--1----1More from HeartbeatFollowHelping data scientists, ML engineers, and deep learning engineers build better models fasterRead more from HeartbeatRecommended from MediumAustin KodrainHeartbeatBest of Machine Learning: Reddit EditionDaniel AngelovFree tools to visualize your computational graphPaul SiodorosBoston vs Seattle Airbnb RatesMahnoor JavedinTowards Data ScienceThe best Machine Learning algorithm for Email ClassificationJia Ning HuGating Mechanism 門控機制Felipe MeloinTowards Data ScienceA Tale of Two ArchitecturesKirill PanarininTowards Data ScienceGradient Descent with Free MonadsKinder ChenBasics of Convolutional Neural Networks — Stride and PoolingAboutHelpTermsPrivacyGet the Medium appGet startedBamigbade Opeyemi313 FollowersData Scientist |ML-Engineer at Data Science Nigeria. Open to consulting and new opportunities. https://opeyemibami.github.io/yhemmy/FollowMore from MediumPrateek ChhikarainLevel Up CodingPerformance Analysis of Text-Summary Generation Models Using ROUGE ScoreSerigne DIAWComponent of Natural Language Processing (NLP)La Javaness R&DDetection and Normalization of Temporal Expressions in French Text (2) — Label Format and…Prakhar GurawaCreating an E-Commerce Product Category Classifier using Deep Learning — Part 2HelpStatusWritersBlogCareersPrivacyTermsAboutKnowable






































