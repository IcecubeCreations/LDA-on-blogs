(93) Understanding the basics of any IoT data pipeline — from ingress capture to processing to generation of value (Part 1) | by Ravi Kumar. | Products, Platforms, Business & Innovation in Industry 4.0/IIoT | MediumGet unlimited accessOpen in appHomeNotificationsListsStoriesWritePublished inProducts, Platforms, Business & Innovation in Industry 4.0/IIoTRavi Kumar.FollowAug 1, 2019·8 min read(93) Understanding the basics of any IoT data pipeline — from ingress capture to processing to generation of value (Part 1)I will be writing a series of articles on standard big data and cloud technologies relevant to Industrial IoT, I will begin with IoT data pipeline to simplify my own understanding. The sheer breadth of technology stack in the entire data journey from data collection to data visualization is overwhelming and hence this is an attempt to simplify.A brief comparison between the old and the new world:*ETL stands for Extract, Transform and LoadData pipeline has changed profoundly since its very beginning. Earlier ETL tools were used to move data from relational databases/data sources into enterprise data warehouse and analytics was done using BI tools. The challenge of previous pipeline was that the data was structured, that is it could be neatly represented in rows and columns, but it could not process unstructured data like audio and images. The other challenge was the speed and scalability of processing.With big data today, the world has changed profoundly. The variety of data sources has multiplied from apps, to devices, to logfiles to streams and message queues. Today the data stores have multiplied too — from Hadoop, to Casandra etc to address the needs of big data.What is an IoT data pipeline?A data pipeline is any set of processes designed for two things:To define what data to collect, where and how;To extract, transform, combine, validate, and load the data for further analysis and visualization.There are a few categories in which the data pipeline is divided into:Aiven likes to divide it into these categories:IngestionTransportStorage and ManagementProcessing and VisualizingThe pipeline architecture could change depending upon whether the data is streaming or batch. For this post, I focus on streaming data.Here’s how an IoT data pipeline architecture looks like:The best way to understand the data journey is to split it into various layers, where each layer performs a specific function.Picture credit:XenonstackThe architecture consists of six basic layers:* Data Ingestion Layer* Data collection layer* Data Processing Layer* Data storage layer*Data query layer*Data visualization layerThe other way to look at the stages according to Laurenz Da Lus, Solutions Architect at Cloudera are these five elements:Acquire — Acquire raw data from source systemsParse — Convert raw data into a common data model and formatEnrich — Add additional context to the dataProfile — Analyse entity behavior across time (sessionisation)Store — Store data to support access and visualizationAiven clearly distinguishes between Data Ingestion and Data Transport.Data Ingestion LayerData ingestion involves procuring events from sources (applications, IoT devices, web and server logs, and even data file uploads) and transporting them into a data store for further processing. It is about moving unstructured data — from where it is originated, into a system where it can be stored and analyzed. Data ingestion is the first step in building the data pipeline. At this stage, data comes from multiple sources at variable speeds in different formats. Hence, it is very important to get the data ingestion right in any IoT pipeline.Data can be streamed in realtime or ingested in batches. When data is ingested in real time then, as soon as data arrives it is ingested immediately. When data is ingested in batches, data items are ingested in some chunks at a periodic interval of time. Ingestion is the process of bringing data into Data Processing system.Data ingestion can be continuous, asynchronous, batched, in real time, or some combination thereof. There are many data ingestion technologies that can take raw data from disparate sources and upload them to a single source of truth.Popular data ingestion tools:* Apache Flume*Apache Kafka*Apache Nifi*Google pub/subData Transport/Collection LayerData transport overlaps somewhat with data ingestion, but “ingestion” revolves around getting data extracted from one system and into another, while “transport” concerns getting data from any location to any other.Message brokers are a key component in data transport; their purpose is to translate a message from a sender’s protocol to that of a receiver, and possibly transform messages prior to moving them.Apache Kafka is a high-throughput distributed messaging system for consistent, fault-tolerant and durable message collection and delivery. Kafka producers publish streams of records or topics to which consumers subscribe. These streams of records are stored and processed as they occur.Kafka is typically used for a few broad classes of applications:Real-time streaming data pipelines between systems or applications;Real-time streaming applications that transform streams of data;Real-time streaming applications that react to streams of data.Compared to earlier, simpler messaging systems like ZeroMQ or RabbitMQ, Kafka generally has better throughput, integrated partitioning, and fault tolerance, making it excellent for large-scale message handling.Kafka’s use has expanded to include everything from commit logs, to website activity tracking, to stream processing.Amazon’s equivalent is Amazon Kinesis, a real-time data processing platform offered on Amazon Web Services. As a fully managed solution, it can handle widely varying amounts of ingest data (without worrying about scaling); it ingests, buffers, and processes streaming data in real time.Data Processing LayerIn this layer, our task is to do magic with data, as now data is ready we only have to route the data to different destinations.In this main layer, the focus is to specialize Data Pipeline processing system or we can say the data we have collected by the last layer in this next layer we have to do processing on that data.The type of Data processing differs between batch and stream data types.* Batch:*Stream:There are three steps in real-time stream processing for Analytics:TransformationData enrichmentStoring of dataTransformation — It includes the conversion of the data which is collected from the IoT device. After this conversion, the resulting data is transferred for further analytics.Data Enrichment — Data enrichment process is the operation in which the sensor collected raw data is combined with the other data-set to get the results.Storing Data — This task includes storing the data at the required storage location.Big Data Storage/Management LayerNo one talks about Big Data or its ecosystem without including Apache Hadoop and Apache Spark. Hadoop is a framework that can process large data sets across clusters; Spark is “a unified analytics engine for large scale data processing.”Both are widely adopted, often used together, and have strong community support with open-source and commercial versions available. However, as both are early evolutionary steps in big data, they come with their unique problems.And Spark, though it can be much faster than Hadoop (with in-memory processing), and supports SQL queries (taking the Hadoop/Spark stack comfortably out of the data engineer’s domain into that of analysts, data scientists and even managers)Nonetheless, pipelines have emerged with other data stores and management methods; some established, some new. Here are some of them:PostgreSQLPostgreSQL is an open-source object-relational database management system emphasizing extensibility and standards compliance that has been around so long, it’s become a standby for companies ranging from manufacturing to IoT.RedisRedis is a super fast variant of the NoSQL database known as a key-value store. As such, it’s an extremely simple database that stores only key-value pairs and serves search results by retrieving the value associated with a known key.Redis’s speed and simplicity make it well-suited for embedded databases, session caches, or queues. In fact, it’s often used in conjunction with message brokers, or as a message broker itself. The Aiven Redis service can be found here.CassandraIf you’re working with large, active data sets, and need to tweak the trade-off between consistency, availability and partition tolerance, then Apache Cassandra may be your solution. Because data is distributed across nodes, when one node — or even an entire data center — goes down, the data remains preserved in other nodes (depending on the consistency level setting).As a wide column store, Cassandra is schema-agnostic and stores data in column families resulting in a multi-dimensional key-value store. Technically schema-free and “NoSQL”, Cassandra uses a SQL variant called CQL for data definition and manipulation, making administration easy for RDBMS experts.InfluxDBThe rapid instrumentation of the physical world due to IoT and data-collecting applications has led to an explosion of time-stamped data. Time series databases serve this evolving niche, and among them, InfluxDB is emerging as a major player. InfluxDB, like others, can handle complex logic or business rules atop massive — and fast-growing — data sets, and InfluxDB adds the advantage of a range of ingestion methods, as well as the ability to append tags to different data points.Data Visualization LayerWhen you want to develop insights and reach conclusions to support your hypotheses, you’re in the domain of data scientists. Data visualization tools and dashboards also support managers, marketers, and even end consumers, but there are simply too many such tools, with too many areas of specialty, to possibly cover in this article.When time-series data needs to be plotted to a graph and visualized — to monitor system performance, say, or how a particular variable or group of variables has performed over time, then a solution like Grafana might be just the ticket. Although originally built for performance and system monitoring, it now directly supports more than 40 data sources and 16 apps.Google Cloud Platform PipelineGoogle Cloud Platform presents the ingestion to analysis stages in this form:Microsoft Azire IoT Pipeline— — — — — — — — — — — — —References:What is ETL?: https://www.youtube.com/watch?v=r4Hrp1fFpMs&list=PLlgLmuG_KgbasW0lpInSAIxYd2vqAEPit&index=2The future of data pipelines: https://dzone.com/articles/the-future-of-data-pipelines--1----1More from Products, Platforms, Business & Innovation in Industry 4.0/IIoTI write on product management, business models and innovations in Industry 4.0/Industrial Internet of ThingsRead more from Products, Platforms, Business & Innovation in Industry 4.0/IIoTRecommended from MediumVishugoyalDocker from very Noobranderson112358inGeek CultureWrite A Simple Python GameEJ HummelCatch calculation errors fasterAlkesh GhorpadeinGeek CultureLeetCode — Convert Sorted Array to Binary Search TreePedro GalvaoEnd to End tests using Apache AirflowDiego VelasquezFlutter : Widget Size and PositionMichael Corleonhttp://ift.tt/2oiyyiI…Trilogy Education ServicesinTrilogy Education ServicesThe Teacher Becomes the Student (Then the Master): How a Math Teacher Found Her Calling in CodingAboutHelpTermsPrivacyGet the Medium appGet startedRavi Kumar.1.2K FollowersBuilding nextgen real estate platform at PriceHubble & podcaster at productlessons.com. I blog about products, business around products, and growth strategies.FollowMore from MediumNagendra BhatBuilding Modern Data Platform — 1Don DiegoWhy Data Management matters (Part I)Irene YuinThe Skiplevel ProgramThe Data Lifecycle Lowdown: Data Processing & Data Processing EnginesBowen LData Asset Companies — Build or Buy Your Data Platform and Data InfrastructureHelpStatusWritersBlogCareersPrivacyTermsAboutKnowable






































