Some detailed differences between Kafka and ActiveMQ | by Victor Alekseev | Dev GeniusOpen in appHomeNotificationsListsStoriesWritePublished inDev GeniusVictor AlekseevFollowMay 14, 2020·22 min readSaveSome detailed differences between Kafka and ActiveMQThe reason that inspired me to write this article is trivial — curiosity and a desire to clarify the difference primarily for myself.The idea of moving to distributed architectures, based on asynchronous interaction of loosely coupled services, has strongly captured the masses. MQ middleware (ActiveMQ for example) are well known to developers since ancient times. A brief description of the common functionality provided by Kafka allows to consider it as only a modern advanced version of the traditional messaging system.So it’s no surprise that many teams, influenced by hype and really impressive examples of Kafka’s applying in well-known projects, without much thought, make the not quite informed decision to use it as a common project infrastructure. After that, they face a lot of difficulties caused by the difference in philosophies between the two systems.I would like to admit right away that there is very little new in this article apart from my own highly subjective accents and conclusions based on more than 10 years of experience with JMS middleware (mainly Apache ActiveMQ and IBM WebSphere MQ) primarily in banks related but not only projects.For those who want to deeply understand the subject, I insistently recommend reading the next two books:Neha Narkhede, Gwen Shapira, Todd Palino “Kafka: The Definitive Guide” (very fundamental, but unfortunately already slightly outdated)Jakub Korab “Understanding Message Brokers” (good, but requires to have some previous experience)If you don’t want to waste your time, I can offer a brief extract from these books and my own humble experience.Storage of messagesPerhaps the most fundamental difference between these two systems is the next: Kafka is NOT a transport, but streaming STORAGE of data with limited addressing capability (abstract offset in the stream or timestamp which can be roughly converted into offset).Data is not only transferred but also permanently stored for multiple attempts of consumptions (in case of connection of a new application, error correction, addition of new processing functionality) by many applications.By defining KSQL it is possible to use this distributed storage for the execution of queries. Unlike classical SQL queries, they are not executed each time “by demand” but work as stateful filters that update their internal state based on the data streams that pass through them. They can be considered as some analogs of the materialized view from the classical relation database.At the same time, MQ is not initially designed as a long-term data storage: its slogan is “as far as consume — delete”:constantly crowded (and, of course, growing) queues are usually a symptom of some problemsDurability and Persistence significantly slow down the processing and usually have to be sacrificed when achieving high throughput and low latencyLogical structureActiveMQJMS API implements a fundamental semantic difference between a queue (peer-to-peer) and topic(publish-subscribe) destinations. ActiveMQ provides optional durability and persistence for each of these types.All messages are coming in one flow and are round-robin distributed between a potentially unlimited number of consumers, each of which can potentially receive any message.In the end, messages are deleted from server storage as soon as they are delivered to one/all subscribers. It is possible to configure the message “obsolescence” by time, in this case, they can be not delivered at all. Consumers fundamentally can not read already acknowledged messages again.The storage functioning is a more or less automatic process, which does not require advanced administration and monitoringKafkaIt supports only topics with mandatory durability and persistence, there is no possibility to implement a classic queue with P2P semantic. You can play with the storage capacity and reliability, but data will always be persisted and available for some time.Messages are reproducibly distributed over a configured set of partitions, evenly or under the control of the producers. Each consumer within a single group (implementation of some functionality) can read several partitions of the topic at the same time and receive messages from each one in the strict order they were sent by producers. At any time, each partition can only be processed only by one consumer of the group. So, the maximum number of active consumers in the group is limited to the count of partitions.Kafka messages are deleted not when they already have been delivered (received and acknowledged by a consumer), but according to the storage policy (maximum configured storage time/volume) and even if they were not delivered to anywhere at all. Any consumer at any time can read already previously received messages again, from the beginning of the storage or using some offset/timeout. It is principally not possible to “delete” the message so that it would be no longer available to consumers.A unique feature of Kafka is the “log compacted” topics, which stores only the last value for each key (standard optional property of the message).Physical storageActiveMQIt is not mandatory to physically store messages on disk at all — we can work completely only with memory. Up to some data volumes, it allows (if memory is enough and consumers are receiving messages fast) to reach a low delay and high throughput.Messages of all the topics are stored in one log, located on one logical device. To write data into this log the threads are lined up. There is only one broker working with the storage at any given time.The broker optionally provides a reliable data storage strategy by forcing all caches to be flushed for each message (a group of messages in one transaction), which slows down significantly.The bandwidth of logical storage is limited by the bandwidth of one disk (you can raid or a special storage plugin for several disks but with some limitations, especially concerning transaction). So, this option is not well suitable for a cloud where all file systems are actually connected via a network and therefore slow enough.There are various plug-in strategies of permanent data storage (per multiple disks, with JDBC and so on)KafkaKafka messages are always permanently saved, there are no other options. Of, course, we can make the maximum time/size of storage minimal, but we can’t make it zero. Thus we also have an additional feature — a backup data storage for free. In the case of right architecture and configuration, all other data storage potentially can be restored from the initial streams of messages.The messages from each topic are distributed among several partitions, each of them is clustered for reading/write/storage load balancing purposes. Accordingly, only write (the latest versions of Kafka allow to use for reading the nearest replica of the partition) operations to one replica of one partition of one topic are lined up. The rest are executed in parallel on one / different cluster nodesInstead of forcefully flushing all caches for each message, the Kafka broker relies on flexibly configurable replication of messages to other nodes in the cluster. This significantly increases bandwidth (sometimes by a thousand times) because the messages are committed with the memory update rate and the broker consumes more file system cache rather than application memory. Heap is used minimally and brokers do not suffer from a typical JVM application problem as an STW pause during garbage collection. Also, this architecture matches up well with cloud deployments.The traffic of messages is more or less evenly distributed across the entire cluster by splitting each topic into multiple sections distributed across the clusterThere is the only strategy to store data — in a segmented log. Data is always written in large bulks to the end of the file containing an active segment and retrieved by pages through the system file cache. The absence of random access to file fragments provides significant performance gains in a file I/O.It requires thorough configuration and continuous monitoring to maintain an equal distribution of data and loading across cluster nodes.General ArchitectureThe fundamental difference is “smart broker vs smart client”The MQ model is focused on a highly functional brokerIt is responsible for the centralized logic of receiving and distributing messages, but clients should only worry about sending, receiving, and acknowledging individual messages. Accordingly, it slows down significantly as the number of connected clients growsIt usually supports many protocols and more complex routing/application integration templates. For example temporary queues, synchronous request-response, automated re-processing individual broken messages by another consumer + dead-letter queue, delaying messages, and so on.Apart from the payload, messages can contain arbitrary headers. Some of them can carry the standard for JMS (or for a particular implementation) system metadata, telling the broker how to process the message. For example, the obsolescence time, where to send the response, is it necessary to store it reliable and so on.It is possible to subscribe with a condition on the content of headers. Due to this and the hierarchical names system of topics/queues, it is possible to implement many Enterprise Integration Patterns used for complex data routing between applications/services in SOA architectureKafkaIt is oriented on “smart” clients who are ( sometimes together) in charge of many functions of a traditional broker: definition of a partition for the delivering message, distribution, and re-distribution of partitions between consumers, processing of exceptions, tracing of the already consumed messages, filtering of already messages by conditions and so on.Instead of “smart” one, there is a simple, fast, and scalable broker that slows down only slightly as the number of connected clients and data processing volume grows.Kafka can be more often found in micro-service architecture and as a platform for distributed processing data flows.There is an extremely limited set of standard metadata describing messages. Not any standard headers are controlling the broker/reader behavior at all. For example, it is not able to consume only messages with some specific values of headers. It is possible to process or throw away messages only after consuming them.Protocols of interactionActiveMQBesides native OpenWire it also supports AMQP, STOMP, MQTT, XMPP.Some protocols can be both TCP and WebSocket based, it allows calling directly from, for example, the Java Script based UI.The message size is potentially unlimited or splitting/gathering is performing automatically.Broker implements a “push “ approach — pushes out messages from storage to clients. Also pulling approach can be configured too.KafkaAs a matter of principle, it does not support anything but owns a binary protocol for the sake of keeping full control over implementations. As it is an “always-on” system, it is critical to maintaining protocol compatibility as the software evolves and the cluster nodes are gradually updated.It also allows writing received messages on disk “as-is” without transformation and passing them by request of the consumer directly from the disk cache page to network socket (zero-copy) with saving a lot of resources.So, due to the long-term storage, the newest version of the software can receive a message in the oldest format via replica.The default message size is 1MB and it is highly discouraged to increase it. The “Claim Check” integration template can help.Broker implements a “pull” approach — clients are pulling messages from the broker, backpressure is not needed at all.Sending messagesActiveMQIt is usually (there are options, but implemented only as configuration and not presented in the API) performing by synchronous blocking call from the supplier till saving the data into the memory or the broker’s disk storage.When physically stored, changes are immediately available to customersKafkaApart from the previous case, it is a more complex and flexible process:successfully sending a message to a broker doesn’t mean that it already been physically stored somewhere. The supplier can be released immediately after the network call. Normally broker saves received message only in the file system cache without flushing it.Not only the leading broker of the partition is involved in the saving procedure, but also the brokers of its following replicas.Consumers will not see the changes until they are distributed among the cluster’s nodes. It takes some time which depends on the cluster node’s loading.The sending process is by default asynchronous, which allows automatically fixing retryable problems — the inaccessibility of the broker, temporary absence of a leader for the partition, etc. But at the same time, we sacrifice a fixed order of sending messages. It is also possible to make this process synchronous, applying the configuration, but it results in low throughput.So, as we can see there is a wide range of trade-offs “throughput” vs “reliability of data storage” — from “sent and forget” to waiting until all brokers of the cluster confirm receiving the message.Message ConsumptionActiveMQThe centralized distribution of messages to the connecting consumers is carried out by the broker, that it is heavily loadedEach message in a normal situation can only be received by the consumer once. After successful acknowledgment of a specified message by the consumer, it is fundamentally impossible to receive the message a second time. But if, for example, acknowledgment is lost, the message will be delivered again. So, real semantic is “as a minimum once”.Consumers can connect/disconnect to a broker at any time without any pauses in the message processing. In this case, we are dealing with a very elastic set of consumers which is good for round-robin load balancing.The broker controls the flow of messages and theoretically can overload the consumerUsing JMS selectors, consumers can filter the receiving messages on the broker’s side. Some useful use cases of this quite important in the MQ architecture feature are the next:the execution of synchronous requests-responses between two components through a pair of queues.ahead fetching from a queue of aged messages for which QoS is already close to critical — some kind of double-ended queue, which allows you to redirect messages from overloaded or dead handlers to healthy or just started ones to fix the traffic jam.The biggest disadvantage of this situation is that if several applications are connected to the same queue (which is normal, for example, for Event Bus pattern), each of them will receive (and perhaps at worst even deserialize) all messages. Correspondingly, we have an excess of network bandwidth and CPU.KafkaConsumers reach agreement among themselves who will receive messages from which partitions. Also, they acknowledge messages by periodically recording their current flow position and sharing this information with “colleagues” to be able to apply it in the case of repartitioning. Acknowledge information can be saved in Kafka (in ZooKeeper for old clients, in a special service topic for new clients) or their storage (for example, together with the result of message processing in the database, which provides some kind of transactionality).They can receive the last message as well as messages already read again by passing a previously saved offset or timestamp.Consumers do not acknowledge individual received messages as in the case of JMS but save their offset of the last received message. Thus the fixing of message X (writing offset X in Kafka or another storage) automatically means that the messages X-1, X-2, and so on are also safely received. If a package of received messages is passed to the worker’s pool for acceleration purposes, it can be a problem.Connections/disconnection of consumers initiates the procedure of repartitioning — redistribution of all partitions belong to all of the affected topics between all their involved consumers, which results in a pause in the consumption of data and may cause duplication of messages. Thus, their number should be as stable as possible.The intensity of messages receiving is fully controlled by the consumer. Moreover, he must do it fast enough not to violate the configured timeout, after which his partitions will be passed to someone else.Consumers receive all messages from assigned partitions. There is no possibility to filter them on the broker’s side. It makes, for example, the already mentioned “synchronous request-response” pattern hard to implement.Distribution of messagesKafkaMandatory distribution of messages to potential consumers is performed by the producer at the moment of sending the message in the partition of the topic.Within each partition, messages are stored in an orderly manner and come to the consumer in the same order. This greatly simplifies the implementation of many business operations but is well suited for a stable number of consumers, as changing their number causes a pause in the redistribution of partitions between consumers, during which no one reads anything.The relative order of messages in different partitions of the same/different topics is not defined, even if they were consecutively sent to the broker.Also, the fixed number of partitions (and consequently consumers) and the complexity of their extension limit the degree of parallelism in message processing. So, for example, Kafka is not very good for elastic load balancing between handlers.ActiveMQIt distributes messages as a round-robin between all registered consumers at the moment of consumption.Partitioning is a quite rarely used feature. if necessary, it can be implemented by adding special metadata to messages but it is not a part of the JMS standard.Some frequent functional requirementsTransaction supportKafkaAs a matter of principle, it does not participate in shared transactions with other systemsSince version 0.11 it is possible to transactionally send several messages in several topics with simultaneous acknowledgment of already received messages.As a matter of principle, there is no automatic rollback of one message if there is a problem with its processing. Depending on when we acknowledge the received messages “before” or “after” processing, the crash of the consumer leads to the loss or repeated receiving of a whole set of messages.There’s no built-in “dead letter queue” analogActiveMQIt allows not only to organize a transaction between several broker calls but also comprehensively participate as a resource in local / XA transactions.An exception when processing one message automatically results in only its repeated receivingYou can limit the number of such attempts and organize a “dead letter queue” for undelivered messagesMessage orderingActiveMQThe orderliness of delivered messages is not guaranteed due to the distributing nature of the synchronous process, which deals with a parallel set of threads, passing messages to consumers. This means that the relative order of processing each several neighboring messages is not defined, but this uncertainty window itself is rather small.But in most cases, the strict orderliness of messages is not important at all:Some events (e.g., a stream of system metrics or logs) are not sensitive to the order in which individual messages are received because each event is time-stamped and the processing of each one is independent.If entity processing is based on the state machine and the execution of each operation can be inited only by the event sent by the previous state, then there is only one message addressed to the entity simultaneously in the flow. The relative order of the messages addressed to different entities is not so important in this case.In some cases, the time “distance” between two messages that should not be interchanged is so relatively long that the probability of collision is negligible.From my experience with systems that generate hundreds of millions of messages a day, I can say that the order of the messages is sensitive almost only when we transfer a large portion of data divided into separate pieces. But it is not a good practice and the EIP pattern “Claim Check” is more preferable.KafkaIt guarantees that all messages within the same partition will be delivered in the order they were added. The price for this is the ability to consume messages from a single partition only by a single thread. If we, to speed up data processing, pass received messages for handling to the thread pool, the execution sequence is lost again. We also have problems with accurate confirmation of received messages.No guarantee is given in respect of messages from different partitions. Due to the distributed nature of storage, the data throughput may vary quite a lot between different partitions. Therefore, if the business key for partitioning is not selected or incorrectly selected, the actual order in which the messages are processed by all consumers may differ very much from the order in which they were initially generated.It means that individual data consumers within the same application can live in very different logical times. And this difference between times depends on the unbalanced distribution of data across the cluster, which usually increases as it is operated and requires manual intervention for elimination.Thus, a thoughtless desire to reduce the weak randomness of close messages when using ActiveMQ can result in a significantly disordered receipt of whole data fragments.Exactly-once deliveryKafkaImplementation of exactly-once delivery is only possible based on the Kafka Streams framework. It is not a standard transactional feature of the API method call, but the integral effect of the architecture and the collaborative event processing by platform and individual tasks. Thus, the application code appears to be quite closely linked to the architecture and functionality of the framework.Strictly speaking, we have not an exactly-once delivery of the message, but the ability to perform the operation “read-process-write” only once for each message, provided that the result of processing is also stored through Kafka (shared transactions are not supported).This is the source of another problem: reliable storage of the processing state is performed by replicating all its changes to the dedicated topic. If the storage is large, its recovery in case of a handler fall can take quite a long time.More advanced but complex for implementation manual approach is bloom filter + local storage (or RockDB as a combination both) + partitioning (to reduce the size of local storage) + periodical backup of storage file. In the case of the restart, we can use the destination topic as a source of information about already processed messages and update restored from backup outdated storage before continuing the work.ActiveMQThere are not ready to use API or patterns of exactly-once messages delivery.It can be easily implemented on the base of a unique message key, generated by the supplier, and one of the following methods:Implementing a “process” step as an idempotent operation. For example by saving the key together with the result of message processing and at the same time checking for data uniqueness. Easy, but not always possible. It also requires saving the result of each message individually, which puts more load on the storage.Saving the key with TTL in a separate fast (usually NoSQL, Redis for example) storage and checking before processing each message. The drawbacks are obvious: transactions are not available, additional remote calls, continuous cleaning up TTL based keys require additional resources, clustered storage is one more element of infrastructure for installation, administration, monitoring, and so on.The third possible opportunity is to move the queue implementation inside the database, which will be further discussed.PartitioningAs was already mentioned, ActiveMQ does not support partitioning, but for Kafka, it is the only and default Modus Operandi. For some business scenarios, especially related to the processing of infinite data sequences, it is an actually necessary feature.In the case of ActiveMQ it can be achieved by generation partition key (in the simplest way it is “hash(<business key>) mod <count of consumers>” ) on the provider’s side and two common approaches:by JMS selector consumers subscribe for receiving the only message from one’s partition. It requires each consumer to have a unique index.by JMS JMXGroupID header broker organize some analog of HTTP stiky session with each consumer, based on the value of this header, which is initialized with partition key or business keyAt the same time, many scenarios do not require full partitioning at all. Due to the independence of individual messages among themselves, it is enough that at every moment any business entity is updated in an exclusive mode.It can be implemented base on the “select for update” query, which is applied to the entity before processing. It is a quite simple, direct, and transparent approach, but it also has some disadvantages:you have to take into account the risk of deadlocks. For example, for transferring data between banking accounts, it is always necessary to acquire locks of them in the same order (for example, alphabetical).By holding the lockout it is extremely undesirable to make remote calls, as this increases the average transaction time, which should normally be minimal. This situation can be improved by introducing intermediate states for business entities ( READY_FOR_UPDATE => LOCKED_FOR_UPDATE ), but this significantly complicates the implementation of the solution. For example, it requires the introduction of timeouts and a rescue manager that picks up the entities that are long in an intermediate state due to the fall of the handler.Thus, a fairly simple at first glance, the task of an exclusive update of business entities can easily become much more complicated as the business logic develops consistently.Consistent backup of the systemKafkaThe task of the consistent backup of a distributed system with multiple data storage is complex in itself. In the case of Kafka, the situation becomes even more complicated as we get additional distributed storage, where individual brokers cannot be backed up consistently.Luckily, the Kafka cluster itself is quite stable and is itself a backup. It is a normal practice also to organize a special disaster recovery cluster and replicate all changes to it with Mirror Maker. Switching to a backup cluster is not so simple, nowadays it is fundamentally impossible to switch to a backup cluster without losing data and/or duplicating events, as well as artifacts of data inconsistency. A good alternative is “stretch cluster” when one cluster is installed simultaneously in several data centers and switched application does not suffer from inconsistency. But this choice requires a good channel between two regions.Unfortunately, this does not solve the problem of the system being able to roll back to a previous state. Once sent into Kafka data remains forever cannot be updated or deleted.ActiveMQIn the case of ActiveMQ, the situation is more simple — the broker’s storage can be consistently stored as backup without data loss. So, the only we have to do is to backup it after other storages and implement one-time handling “orphan” messages, which are born by data, not existed yet in the other data storage. Yes, it may be a complex business task, which can’t be performed automatically, but it is more simple.Possible AlternativeThe only simple general way to be able to perform a consistent backup of all application data is having only one storage, based on the classic relational database. In this case, the queue can be implemented based on the dedicated table with messages and receiving procedures implemented as “select for update skip locked” query.It is quite simple, flexible, and reliable enough for throughput of about 1000 operations per second provided that the duration of each operation does not exceed 10 milliseconds (long transactions kill database performance).If the amount of messages in the queue at the same time is not large, the database server actually keeps them all in memory, working almost a real broker. It is therefore recommended that the minimum amount of data required is transmitted through the queue.The major advantage (apart from subj):we can implement very flexible message prioritization policies based not only on timestamp but also on some business-related propertiesdata processing is multithreaded on the one hand and strictly transactional on the other handautomatic retry in case of any exceptionwe also get the strong persistency, guaranties for task delivery and at-least-once delivery semanticThe disadvantages of this approach are obvious:every message must be processed very fast. It means the inability to make remote calls inside a transaction or complex processing logic with intermediate states and lock’s timeoutsconsumers who constantly re-requests data even if the message queue is empty.it’s not very scalable in terms of throughput and the number of participants. The situation can be improved with the tuning and partitioning of the table by vendor-specific options. For example, it is possible to split the single table into several ones, located into different physical servers and logically joined by something like “dblink” for Oracle or “foreign data wrapper” for PostgreSQLThere is a ready implementation of this approach — “db-queue”, which is the kernel of the payment system from Yandex.Scalability and fault toleranceKafkaIt is originally designed for a cluster with redundant data replication and distribution of user’s requests among nodes. This allows almost unlimited increasing capacity and throughput.The degree of parallelism of access to the topic’s data is limited by the number of partitions (maximum one reader of the group per partition) and should preferably be determined in advance during design. Increasing the number of partitions is complex, causes a pause in the reading of the whole topic, and affects only new data. Data can only be fairly redistributed among the expanded count of partitions by creating the new topic and copying all the existing data into it.Scalability and fault tolerance can be reached horizontally based on ordinary hardware which is rebuilt/removed/added to the cluster as needed without stopping (always-on system).Problems of one topic are more or less isolated and lead to problems only of some brokers. Due to this, the system is more robust but the difference between the presence and absence of problems for the Kafka cluster is more blurred.If individual brokers of the cluster fail periodically, the unbalanced distribution of load on the cluster increases and can only be fixed by a manual operation.ActiveMQIt allows only Master-Slaves configuration based on one shared storage. Thus, scalability is limited and almost only vertical. As the traffic grows, it needs to deploy separate servers with separate stores and manually distribute application’s destinations between them.The degree of parallelism is limited by the broker’s resources. Theoretically, it supports an infinite number of readers per each queue / topical without the need to redistribute data in order to increase the degree of parallelism.Scaling “on the fly” is difficult and fundamentally limited. It is required a pause while maintenance. Increasing capacity can be achieved only vertically and requires more and more powerful hardware.Problems of one queue/topic are usually caused by a lack of disk space and immediately become problems of the broker as a whole. It makes the infrastructure more fragile but simpler and encourages administrators to be more attentive.Operating CostsKafkaLow-level API is unique and complex enough. The behavior of the consumer/producer/top/broker can be configured by about 30 configuration parameters for each type, many of these parameters fundamentally affect its behavior, not only nonfunctional characteristics, such as throughput/latency balance for example.The implementation of reliable solutions requires a good understanding of system architecture. Therefore, it is almost necessary to base on Streams / Connectors / KSQL frameworks, which increases the already not low cost of entry. Applying Kafka requires much more experience from the developers and architects.It is difficult enough to install and administrate for production usage. Usually, an advanced DevOps culture is required for successful maintenance.Lack of ready for use out of the box management and monitoring tools, which are necessary to operate the system and meet QOS requirements.The critical obstacle for the usual enterprise applications — inability to participate in distributed transactions. It requires developers to move to BASE architecture and to have painful discussions with business analysts.ActiveMQIt is simple enough to install. It can be executed even in the embedded mode, which is quite good for component testing.Most of the time It does not require any complex management and continuous monitoring.JMS API is classic, compact, simple enough, well known, and predictable.There are no problems with distributed transactions, and therefore enterprise developers can safely follow a comfortable for them ACID paradigm.SummaryGenerally speaking, Kafka can be coarsely described as a “NoSQL style” solution in the ancient world of MQ:it is a modern, wonderful, lightweight, quite flexible tool, but it also provides a great opportunity to get a reach experience by shooting yourself in the leg.However, with the right architecture and deep understanding of how it works, it is possible to achieve the impressive (in terms of QoS) results with the optimal loading of your resources.Unavoidable paying for it is the complexity of your application but Connectors and Kafka Streams parts of the framework can simplify significantly the implementation (of course, due to the next layer of abstraction, which can make the system behavior even more mysterious than before).KafkaIt is quite good when we need to organize reliable storage of very large amounts of data with very high (10–100 times difference) throughput and relatively simple delivery logic to a fairly constant number of customers.The price of entry, development, and maintenance of the ready system is relatively higher.Well suited for high volume service projects, especially in the public Internet and IoT domains, with cloud-based deploymentActiveMQIt is quite good as a smart transport for implementation of complex message routing flowsIt allows effective balancing requests among a dynamic number of concurrent consumersThe potential throughput is not so high and degrades with the increasing amount of stored data and the number of connected clients.Well can be integrated with traditional enterprise landscape, especially in financial and advanced document workflow domains, with on-premises deploymentIn general, if you’re not a Facebook or Google developer and you don’t critically need for big data storage, endless data flow processing or such advanced features like real partitioning, strict message order or exactly-once delivery, then Kafka is likely to be too much for you.Thanks toАлексей Власов--1----1More from Dev GeniusFollowCoding, Tutorials, News, UX, UI and much more related to developmentRead more from Dev GeniusRecommended from MediumSammy-Jo WymerinREWRITE TECH by diconiumGetting friendly with the terminal: a super-friendly beginner’s guideHasintha IndrajeeSAML Bearer Grant Type with WSO2 Identity Server 5.4.0Jonathan AirdUpstate: Behind the ScenesThe Pragmatic ProgrammersinThe Pragmatic ProgrammersUsing Multiple FixturesSHR Tips and tricks from a beginner.Carlos H.Hello to the WorldJama SoftwareFour Fundamentals of Requirements ManagementDavid H. DeansinTechnology | Media | TelecomLow-Code Software Tools Accelerate Digital InnovationAboutHelpTermsPrivacyGet the Medium appGet startedVictor Alekseev61 FollowersSoftware Architect and Lead Developer with 20 years experience in Java EE, Spring and Oracle. More at https://www.linkedin.com/in/victoralekseev/FollowMore from MediumKyle CarterinDev GeniusDiving Into Kafka Partitioning By Building a Custom Partition AssignorSumit KumarinTowards DevConsume AVRO Messages from Kafka without schemaAll About CodeThe 5 Major APIs in Apache KafkaJeyakeerthananExplore Prototype Design PatternHelpStatusWritersBlogCareersPrivacyTermsAboutKnowable






































