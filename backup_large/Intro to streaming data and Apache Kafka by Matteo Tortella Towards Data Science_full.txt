Intro to streaming data and Apache Kafka | by Matteo Tortella | Towards Data ScienceOpen in appHomeNotificationsListsStoriesWritePublished inTowards Data ScienceMatteo TortellaFollowMay 20, 2020·6 min readSaveIntro to streaming data and Apache KafkaOverview of streaming data architectures and why Apache Kafka has become so popularThe term ‘Big Data’ contains more than just its reference to quantity and volume. Living in the era of readily available information and instantaneous communication, it is not surprising that data architectures have been shifting to be stream-oriented. Real value for companies doesn’t just come from sitting on a gargantuan amount of collected data points, but also from their ability to extract actionable insights as quickly as possible (even in real-time). Processing data at faster rates allows a company to react to changing business conditions in real-time.It goes without saying that over the last decade, there has been a constant growth for applications (aka message-broker software) capable of capturing, retain and process this overwhelmingly rapid flow of information. As of 2020, Apache Kafka is one of the most widely adopted message-broker software (used by the likes of Netflix, Uber, Airbnb and LinkedIn) to accomplish these tasks. This blog will give a very brief overview of the concept of stream-processing, streaming data architecture and why Apache Kafka has gained so much momentum.Image Credit: GiphyWhat is stream-processing?Stream-processing is best visualised as a river. In the same way that water flows through a river, so do packages of information in the endless flow of stream-processing.According to AWS, the general definition of streaming data would be “data that is generated continuously by thousands of data sources, which typically send in the data records simultaneously, and in small sizes (order of Kilobytes)”.Data streaming works particularly well in time-series in order to find underlying patterns over time. It also really shines in the IoT space where different data signals can be constantly collected. Other common uses are found on web interactions, e-commerce, transaction logs, geolocation points and much much more.There is a subtle difference between real-time processing and stream-processing:Real-time processing implies a hard deadline in terms of data processing. In other words, event time is very relevant and latencies in the order of a second are unacceptable.Stream-processing refers more to a method of computation in a continuous flow of data. An application printing out of all the Facebook posts created in the last day doesn’t really have constraints in terms of time. However, the long term output rate should be faster (or at least equal) to the long term input rate otherwise system storage requirements would have to be indefinitely large.The Streaming Data ArchitectureThe canonic streaming data architecture is composed of 3 fundamental blocks:Message Broker. It collects streaming data from so-called producers. Producers can track any event ranging from metrics, clickstreams, searches and etc. All these actions are then stored into the atomic building blocks of the Apache Kafka architecture, namely topics and partitions. One of the key strength of Kafka is its ability to parallelise partitions in order to increase the overall throughput. The information is then made available to consumers which will be using this output in different ways.Image Credit: Wikipedia / Kafka Message BrokerTwo very famous new-generations message-broker software are Apache Kafka and Amazon Kinesis as opposed to older types such as RabbitMQ and Active MQ.2. Real time ETL-ToolsThe raw data is then transformed, cleaned and aggregated so that it can be then used by SQL-based systems in order to be analysed later on.Some of the most used platform to perform these tasks would be stream processors such as Spark Streaming, Kafka Streaming and others. The outcome of this stage could be an API call, an action, an alert etc.3. Data Storage and AnalysisAfter the data is prepared by our data stream processors we have to store it analyse it in order to provide valuable insights. Such a vast amount of data can be either stored in cheap data-lakes like Amazon S3 at the expense of latency and SQL operations. Or in data warehouses like Cassandra and Amazon Redshift which are harder to manage compared to data-lakes but also more structured. Alternatively, they can also be stored on the memory of the message-broker itself (for example Kafka) but this is 10x more expensive than the data-lakes counterpart.Why is Kafka getting so much attention?To understand better how stream-processing came to be let’s have a brief look at how the data processing looked like before Apache Kafka. All data was previously stored in a database or a distributed file system which would be periodically (1 hour, 1 day etc) be analysed and operated on.Even though this model has worked for a long time and it still has many applications, it has also a fundamental flaw. It simply cannot analyse data on the fly in a world where data value quickly decreases over time.Stream processing is all about ingesting and analysing a flow of events and, as we saw earlier, the backbone responsible for the fetching is made up of our message-broker software which are able to take in all these data signals and made them (almost) immediately available. However, creating a super-efficient system many-to-many producer to consumer was no easy task and it was exactly what Linkedin had to face in 2008 when they started working on Kafka. One of the main issues was how the producer should pass the information directly to each consumer in private sessions and how could consumers read this data while it was being written by producers.Linkedin’s project (which will become later on Apache Kafka) solved for this issue and provided a distributed platform that was able to process stream records as they occurred.It basically became the backbone of modern stream processing and it was so successful that other companies created their counterpart such as Amazon Kinesis and Azure Event Hubs.The heart of its popularity came from the introduction of a system that it’s log-oriented as opposed to the batch structures previously adopted. The log is time-ordered and append-only series of events from which consumers can extract information at any time. Example of data events in the logs coming from a shopping cart could be add item, delete item, checkout etc. In Kafka, messages are written on topics which maintains its log and from which consumers can extract the data.Image Credit: Official Apache Kafka Website / Append-only Log structureEven though message-broker software such as Rabbit MQ and ActiveMQ have been around for years even before Kafka, the log-append structure has allowed for the processing of an insane amount of transactions. In Kafka, each consumer is responsible to keep track of their activity whereas in software like RabbitMQ, they had to behave like a queue and the message-broker itself keeps track of the consumer activity.It has to be said that Kafka did not make previous systems obsolete and they are still applicable to other architectures where they are simply a better fit. Kafka provides a much higher throughput which currently enables a company like Netflix to process a mind-boggling amount of data in the order of 8 million events per second at peak times.------More from Towards Data ScienceFollowYour home for data science. A Medium publication sharing concepts, ideas and codes.Read more from Towards Data ScienceRecommended from MediumPhilip WilkinsoninTowards Data ScienceA Practical Introduction to Hierarchical clustering from scikit-learnGarrett KeyesinAnalytics VidhyaGeoSpatial Data and its Role in Data ScienceNikolaos KonstantinouinThe Data Value FactoryThis Week in Data Preparation (June 8, 2020)JosiahZomato Data Analytics and Visualization PT2Brett Vintchinbag of wordsDiscovering hidden patterns in high dimensional time seriesDarío WeitzinAnalytics VidhyaTreemaps & Sunburst Charts with PlotlyGraham GillerinAdventures in Data ScienceRandom is difficultIMPEER of the NAS of UkraineA mathematical model and forecast for the coronavirus disease COVID-19 in Ukraine (М2)AboutHelpTermsPrivacyGet the Medium appGet startedMatteo Tortella17 FollowersJunior Data Scientist | Programmatic Digital Analyst | Passionate about breaking down complex technical concepts into simpler terms.FollowMore from MediumLingeshwaran KanniappaninTowards Data ScienceBeginners Guide for choosing the correct Spark API: RDDs, DataFrames & DatasetsKsheerja SethPySpark Project: Predicting Churn For A Music AppSharmo SarkarinTowards DevHow to write PySpark One Hot Encoding results to an interpretable CSV fileRichardChurn Prediction with PysparkHelpStatusWritersBlogCareersPrivacyTermsAboutKnowable






































